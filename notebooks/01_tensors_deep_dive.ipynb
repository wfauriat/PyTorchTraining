{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.1: Tensors Deep Dive\n",
    "\n",
    "This notebook provides a comprehensive exploration of PyTorch tensors - the fundamental data structure underlying all of deep learning. We'll go beyond basic operations to understand memory layout, performance implications, and common pitfalls.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand tensor creation patterns and when to use each\n",
    "- Master dtypes and device management (CPU/GPU)\n",
    "- Comprehend memory layout: strides, contiguity, and their performance implications\n",
    "- Distinguish between views and copies\n",
    "- Apply broadcasting rules correctly\n",
    "- Use in-place operations safely\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA version: 12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check PyTorch version and CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Tensor Creation\n",
    "\n",
    "PyTorch provides multiple ways to create tensors. Understanding when to use each method is crucial for writing efficient code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 From Python Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From list: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Shape: torch.Size([2, 3]), Dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# From Python list - creates a copy\n",
    "data = [[1, 2, 3], [4, 5, 6]]\n",
    "t1 = torch.tensor(data)\n",
    "print(f\"From list: {t1}\")\n",
    "print(f\"Shape: {t1.shape}, Dtype: {t1.dtype}\")\n",
    "\n",
    "# Note: torch.tensor() ALWAYS copies data\n",
    "# This is different from torch.as_tensor() which may share memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original NumPy array: [[1. 2.]\n",
      " [3. 4.]]\n",
      "Tensor from NumPy: tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n",
      "\n",
      "After modifying NumPy array:\n",
      "NumPy: [[999.   2.]\n",
      " [  3.   4.]]\n",
      "Tensor: tensor([[999.,   2.],\n",
      "        [  3.,   4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# From NumPy - torch.from_numpy() shares memory!\n",
    "np_array = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "t2 = torch.from_numpy(np_array)\n",
    "\n",
    "print(f\"Original NumPy array: {np_array}\")\n",
    "print(f\"Tensor from NumPy: {t2}\")\n",
    "\n",
    "# Modify the numpy array\n",
    "np_array[0, 0] = 999\n",
    "print(f\"\\nAfter modifying NumPy array:\")\n",
    "print(f\"NumPy: {np_array}\")\n",
    "print(f\"Tensor: {t2}\")  # Also changed!\n",
    "\n",
    "# Key insight: from_numpy() creates a VIEW, not a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as_tensor (shares memory): tensor([100.,   2.,   3.], dtype=torch.float64)\n",
      "tensor (copied): tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# torch.as_tensor() - smart conversion (shares memory when possible)\n",
    "np_array2 = np.array([1.0, 2.0, 3.0])\n",
    "t3 = torch.as_tensor(np_array2)\n",
    "\n",
    "# vs torch.tensor() - always copies\n",
    "t4 = torch.tensor(np_array2)\n",
    "\n",
    "np_array2[0] = 100\n",
    "print(f\"as_tensor (shares memory): {t3}\")  # Changed\n",
    "print(f\"tensor (copied): {t4}\")            # Unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Factory Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty (uninitialized):\n",
      "tensor([[3.4163e-20, 0.0000e+00, 0.0000e+00, 1.4013e-45],\n",
      "        [0.0000e+00, 4.3028e-41, 9.1084e-44, 0.0000e+00],\n",
      "        [3.4002e-20, 0.0000e+00, 2.6358e-36, 4.3031e-41]])\n",
      "\n",
      "Zeros:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Ones:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Uninitialized tensor - contains garbage values (fast but dangerous)\n",
    "t_empty = torch.empty(3, 4)\n",
    "print(f\"Empty (uninitialized):\\n{t_empty}\\n\")\n",
    "\n",
    "# Zeros and ones\n",
    "t_zeros = torch.zeros(2, 3)\n",
    "t_ones = torch.ones(2, 3)\n",
    "print(f\"Zeros:\\n{t_zeros}\\n\")\n",
    "print(f\"Ones:\\n{t_ones}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform [0,1):\n",
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009]])\n",
      "\n",
      "Standard normal:\n",
      "tensor([[ 1.1561,  0.3965, -2.4661],\n",
      "        [ 0.3623,  0.3765, -0.1808]])\n",
      "\n",
      "Random integers [0, 10):\n",
      "tensor([[7, 6, 9],\n",
      "        [6, 3, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Random tensors - essential for weight initialization\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "\n",
    "# Uniform distribution [0, 1)\n",
    "t_rand = torch.rand(2, 3)\n",
    "print(f\"Uniform [0,1):\\n{t_rand}\\n\")\n",
    "\n",
    "# Standard normal distribution (mean=0, std=1)\n",
    "t_randn = torch.randn(2, 3)\n",
    "print(f\"Standard normal:\\n{t_randn}\\n\")\n",
    "\n",
    "# Random integers\n",
    "t_randint = torch.randint(low=0, high=10, size=(2, 3))\n",
    "print(f\"Random integers [0, 10):\\n{t_randint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arange(0, 10, 2): tensor([0, 2, 4, 6, 8])\n",
      "linspace(0, 1, 5): tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Ranges and linear spaces\n",
    "t_arange = torch.arange(0, 10, 2)  # start, end (exclusive), step\n",
    "print(f\"arange(0, 10, 2): {t_arange}\")\n",
    "\n",
    "t_linspace = torch.linspace(0, 1, 5)  # start, end (inclusive), num_points\n",
    "print(f\"linspace(0, 1, 5): {t_linspace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference shape: torch.Size([3, 4]), dtype: torch.float32\n",
      "zeros_like matches: shape=torch.Size([3, 4]), dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "# \"_like\" functions - create tensor with same shape/dtype/device\n",
    "reference = torch.randn(3, 4, device='cpu', dtype=torch.float32)\n",
    "\n",
    "t_zeros_like = torch.zeros_like(reference)\n",
    "t_ones_like = torch.ones_like(reference)\n",
    "t_rand_like = torch.rand_like(reference)\n",
    "\n",
    "print(f\"Reference shape: {reference.shape}, dtype: {reference.dtype}\")\n",
    "print(f\"zeros_like matches: shape={t_zeros_like.shape}, dtype={t_zeros_like.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Types (dtypes)\n",
    "\n",
    "Choosing the right dtype affects memory usage, computation speed, and numerical precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (default for floats): 4 bytes per element\n",
      "float64 (double precision): 8 bytes per element\n",
      "float16 (half precision - GPU training): 2 bytes per element\n",
      "bfloat16 (brain float - better range than fp16): 2 bytes per element\n",
      "int32: 4 bytes per element\n",
      "int64 (default for integers): 8 bytes per element\n",
      "bool: 1 bytes per element\n"
     ]
    }
   ],
   "source": [
    "# Common dtypes\n",
    "dtypes_info = [\n",
    "    (torch.float32, \"float32 (default for floats)\"),\n",
    "    (torch.float64, \"float64 (double precision)\"),\n",
    "    (torch.float16, \"float16 (half precision - GPU training)\"),\n",
    "    (torch.bfloat16, \"bfloat16 (brain float - better range than fp16)\"),\n",
    "    (torch.int32, \"int32\"),\n",
    "    (torch.int64, \"int64 (default for integers)\"),\n",
    "    (torch.bool, \"bool\"),\n",
    "]\n",
    "\n",
    "for dtype, desc in dtypes_info:\n",
    "    t = torch.tensor([1.0], dtype=dtype) if 'float' in str(dtype) or 'bfloat' in str(dtype) else torch.tensor([1], dtype=dtype)\n",
    "    print(f\"{desc}: {t.element_size()} bytes per element\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integers -> torch.int64\n",
      "Floats -> torch.float32\n",
      "Mixed -> torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Type inference rules\n",
    "t_int = torch.tensor([1, 2, 3])\n",
    "t_float = torch.tensor([1.0, 2.0, 3.0])\n",
    "t_mixed = torch.tensor([1, 2.0, 3])  # Promotes to float\n",
    "\n",
    "print(f\"Integers -> {t_int.dtype}\")\n",
    "print(f\"Floats -> {t_float.dtype}\")\n",
    "print(f\"Mixed -> {t_mixed.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to(int32): tensor([1, 2, 3], dtype=torch.int32)\n",
      ".long(): tensor([1, 2, 3])\n",
      ".half(): tensor([1.7002, 2.3008, 3.9004], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Type casting\n",
    "t = torch.tensor([1.7, 2.3, 3.9])\n",
    "\n",
    "# Method 1: .to(dtype)\n",
    "t_int = t.to(torch.int32)\n",
    "print(f\"to(int32): {t_int}\")  # Truncates, doesn't round!\n",
    "\n",
    "# Method 2: Convenience methods\n",
    "t_long = t.long()    # int64\n",
    "t_float = t.float()  # float32\n",
    "t_half = t.half()    # float16\n",
    "\n",
    "print(f\".long(): {t_long}\")\n",
    "print(f\".half(): {t_half}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Impact of dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32        -> 4.00 MB\n",
      "torch.float16        -> 2.00 MB\n",
      "torch.bfloat16       -> 2.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Memory comparison for a typical neural network layer\n",
    "size = (1024, 1024)  # 1M parameters\n",
    "\n",
    "for dtype in [torch.float32, torch.float16, torch.bfloat16]:\n",
    "    t = torch.randn(size, dtype=dtype)\n",
    "    memory_mb = t.element_size() * t.numel() / (1024 * 1024)\n",
    "    print(f\"{str(dtype):20} -> {memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Devices: CPU vs GPU\n",
    "\n",
    "PyTorch tensors can live on CPU or GPU. Operations require tensors to be on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU device: cpu\n",
      "GPU device: cuda:0\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "# Check available devices\n",
    "cpu_device = torch.device('cpu')\n",
    "print(f\"CPU device: {cpu_device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_device = torch.device('cuda:0')  # First GPU\n",
    "    print(f\"GPU device: {gpu_device}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU tensor device: cpu\n",
      "GPU tensor device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors on specific devices\n",
    "t_cpu = torch.randn(3, 3, device='cpu')\n",
    "print(f\"CPU tensor device: {t_cpu.device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    t_gpu = torch.randn(3, 3, device='cuda')\n",
    "    print(f\"GPU tensor device: {t_gpu.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: cpu\n",
      "After .to('cuda'): cuda:0\n",
      "After .cpu(): cpu\n"
     ]
    }
   ],
   "source": [
    "# Moving tensors between devices\n",
    "t = torch.randn(1000, 1000)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Method 1: .to(device)\n",
    "    t_gpu = t.to('cuda')\n",
    "    \n",
    "    # Method 2: .cuda() / .cpu()\n",
    "    t_gpu2 = t.cuda()\n",
    "    t_back = t_gpu.cpu()\n",
    "    \n",
    "    print(f\"Original: {t.device}\")\n",
    "    print(f\"After .to('cuda'): {t_gpu.device}\")\n",
    "    print(f\"After .cpu(): {t_back.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device-agnostic code pattern\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create tensors on the appropriate device\n",
    "x = torch.randn(100, 100, device=device)\n",
    "y = torch.randn(100, 100, device=device)\n",
    "z = x @ y  # Matrix multiplication on GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 32.94 ms per 2048x2048 matmul\n",
      "GPU: 2.25 ms per 2048x2048 matmul\n",
      "Speedup: 14.7x\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison: CPU vs GPU\n",
    "import time\n",
    "\n",
    "def benchmark_matmul(size, device, num_iters=100):\n",
    "    a = torch.randn(size, size, device=device)\n",
    "    b = torch.randn(size, size, device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        c = a @ b\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_iters):\n",
    "        c = a @ b\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    return elapsed / num_iters * 1000  # ms per operation\n",
    "\n",
    "size = 2048\n",
    "cpu_time = benchmark_matmul(size, torch.device('cpu'))\n",
    "print(f\"CPU: {cpu_time:.2f} ms per {size}x{size} matmul\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_time = benchmark_matmul(size, torch.device('cuda'))\n",
    "    print(f\"GPU: {gpu_time:.2f} ms per {size}x{size} matmul\")\n",
    "    print(f\"Speedup: {cpu_time / gpu_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Memory Layout: Strides and Contiguity\n",
    "\n",
    "Understanding how tensors are stored in memory is crucial for performance optimization and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 What are Strides?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "Shape: torch.Size([3, 4])\n",
      "Strides: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Strides tell you how many elements to skip in memory to move one position in each dimension\n",
    "t = torch.arange(12).reshape(3, 4)\n",
    "print(f\"Tensor:\\n{t}\\n\")\n",
    "print(f\"Shape: {t.shape}\")\n",
    "print(f\"Strides: {t.stride()}\")\n",
    "\n",
    "# Stride (4, 1) means:\n",
    "# - Move 4 elements in memory to go to the next row\n",
    "# - Move 1 element in memory to go to the next column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory layout (row-major / C-contiguous):\n",
      "Flat memory: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "\n",
      "To access t[1, 2]:\n",
      "  Memory offset = 1 * stride[0] + 2 * stride[1] = 1 * 4 + 2 * 1 = 6\n",
      "  Value at offset 6: 6 = t[1,2] = 6\n"
     ]
    }
   ],
   "source": [
    "# Visualizing memory layout\n",
    "print(\"Memory layout (row-major / C-contiguous):\")\n",
    "print(f\"Flat memory: {t.flatten().tolist()}\")\n",
    "print(\"\")\n",
    "print(\"To access t[1, 2]:\")\n",
    "print(f\"  Memory offset = 1 * stride[0] + 2 * stride[1] = 1 * 4 + 2 * 1 = 6\")\n",
    "print(f\"  Value at offset 6: {t.flatten()[6].item()} = t[1,2] = {t[1,2].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Contiguous vs Non-Contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: contiguous=True, strides=(4, 1)\n",
      "Transposed: contiguous=False, strides=(1, 4)\n"
     ]
    }
   ],
   "source": [
    "# Original tensor is contiguous\n",
    "t = torch.arange(12).reshape(3, 4)\n",
    "print(f\"Original: contiguous={t.is_contiguous()}, strides={t.stride()}\")\n",
    "\n",
    "# Transpose creates a non-contiguous view\n",
    "t_T = t.T\n",
    "print(f\"Transposed: contiguous={t_T.is_contiguous()}, strides={t_T.stride()}\")\n",
    "\n",
    "# The transposed tensor has reversed strides - it's viewing the same memory differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "After .contiguous(): True\n"
     ]
    }
   ],
   "source": [
    "# Why does contiguity matter?\n",
    "t = torch.randn(1000, 1000)\n",
    "t_T = t.T  # Non-contiguous\n",
    "\n",
    "# Some operations require contiguous tensors\n",
    "try:\n",
    "    # view() requires contiguous input\n",
    "    t_T.view(-1)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "# Solution: use .contiguous() or .reshape()\n",
    "t_T_contig = t_T.contiguous()  # Creates a copy with contiguous memory\n",
    "flat = t_T_contig.view(-1)  # Now works\n",
    "print(f\"After .contiguous(): {t_T_contig.is_contiguous()}\")\n",
    "\n",
    "# Or use reshape(), which handles non-contiguous tensors automatically\n",
    "flat2 = t_T.reshape(-1)  # Works without explicit .contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: contiguous=True, strides=(12, 4, 1)\n",
      "transpose(0,1): contiguous=False, strides=(4, 12, 1)\n",
      "permute(2,0,1): contiguous=False, strides=(1, 12, 4)\n",
      "slice [::2]: contiguous=False, strides=(12, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "# Operations that create non-contiguous tensors\n",
    "t = torch.arange(24).reshape(2, 3, 4)\n",
    "print(f\"Original: contiguous={t.is_contiguous()}, strides={t.stride()}\")\n",
    "\n",
    "# Transpose\n",
    "t1 = t.transpose(0, 1)\n",
    "print(f\"transpose(0,1): contiguous={t1.is_contiguous()}, strides={t1.stride()}\")\n",
    "\n",
    "# Permute\n",
    "t2 = t.permute(2, 0, 1)\n",
    "print(f\"permute(2,0,1): contiguous={t2.is_contiguous()}, strides={t2.stride()}\")\n",
    "\n",
    "# Slicing with step\n",
    "t3 = t[:, ::2, :]\n",
    "print(f\"slice [::2]: contiguous={t3.is_contiguous()}, strides={t3.stride()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Performance Implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (contiguous): 3.670 ms\n",
      "Transposed (non-contiguous): 2.902 ms\n",
      "Transposed + contiguous(): 3.013 ms\n"
     ]
    }
   ],
   "source": [
    "# Non-contiguous memory access is slower due to cache misses\n",
    "import time\n",
    "\n",
    "size = 4096\n",
    "t = torch.randn(size, size)\n",
    "t_T = t.T  # Non-contiguous view\n",
    "t_T_contig = t_T.contiguous()  # Contiguous copy\n",
    "\n",
    "def time_sum(tensor, name, num_iters=100):\n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        tensor.sum()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_iters):\n",
    "        tensor.sum()\n",
    "    elapsed = (time.perf_counter() - start) / num_iters * 1000\n",
    "    print(f\"{name}: {elapsed:.3f} ms\")\n",
    "\n",
    "time_sum(t, \"Original (contiguous)\")\n",
    "time_sum(t_T, \"Transposed (non-contiguous)\")\n",
    "time_sum(t_T_contig, \"Transposed + contiguous()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Views vs Copies\n",
    "\n",
    "Understanding when PyTorch shares memory vs copies data is essential for:\n",
    "- Avoiding bugs from unexpected data sharing\n",
    "- Memory efficiency\n",
    "- Performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "View: tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "\n",
      "After modifying view[0,0]:\n",
      "Original: tensor([999,   1,   2,   3,   4,   5,   6,   7,   8,   9])\n",
      "View: tensor([[999,   1,   2,   3,   4],\n",
      "        [  5,   6,   7,   8,   9]])\n"
     ]
    }
   ],
   "source": [
    "# Views share memory with the original tensor\n",
    "original = torch.arange(10)\n",
    "view = original.view(2, 5)\n",
    "\n",
    "print(f\"Original: {original}\")\n",
    "print(f\"View: {view}\")\n",
    "\n",
    "# Modify via the view\n",
    "view[0, 0] = 999\n",
    "print(f\"\\nAfter modifying view[0,0]:\")\n",
    "print(f\"Original: {original}\")  # Also modified!\n",
    "print(f\"View: {view}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view shares memory: True\n",
      "clone shares memory: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33874/1656626046.py:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return a.storage().data_ptr() == b.storage().data_ptr()\n"
     ]
    }
   ],
   "source": [
    "# Check if tensors share memory\n",
    "def shares_memory(a, b):\n",
    "    return a.storage().data_ptr() == b.storage().data_ptr()\n",
    "\n",
    "original = torch.arange(10)\n",
    "view = original.view(2, 5)\n",
    "copy = original.clone()\n",
    "\n",
    "print(f\"view shares memory: {shares_memory(original, view)}\")\n",
    "print(f\"clone shares memory: {shares_memory(original, copy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view/reshape         shares memory: True\n",
      "transpose            shares memory: True\n",
      "slicing              shares memory: True\n",
      "squeeze/unsqueeze    shares memory: True\n",
      "expand               shares memory: True\n"
     ]
    }
   ],
   "source": [
    "# Operations that return VIEWS (share memory)\n",
    "t = torch.arange(12).reshape(3, 4)\n",
    "\n",
    "views = [\n",
    "    (\"view/reshape\", t.view(4, 3)),\n",
    "    (\"transpose\", t.T),\n",
    "    (\"slicing\", t[1:, 1:]),\n",
    "    (\"squeeze/unsqueeze\", t.unsqueeze(0)),\n",
    "    (\"expand\", t[0].expand(3, -1)),\n",
    "]\n",
    "\n",
    "for name, v in views:\n",
    "    print(f\"{name:20} shares memory: {shares_memory(t, v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clone                     shares memory: False\n",
      "contiguous (if needed)    shares memory: False\n",
      "to (different device/dtype) shares memory: False\n",
      "masked_select             shares memory: False\n",
      "nonzero                   shares memory: False\n"
     ]
    }
   ],
   "source": [
    "# Operations that return COPIES (new memory)\n",
    "t = torch.arange(12).reshape(3, 4)\n",
    "\n",
    "copies = [\n",
    "    (\"clone\", t.clone()),\n",
    "    (\"contiguous (if needed)\", t.T.contiguous()),\n",
    "    (\"to (different device/dtype)\", t.to(torch.float32)),\n",
    "    (\"masked_select\", t[t > 5]),\n",
    "    (\"nonzero\", t.nonzero()),\n",
    "]\n",
    "\n",
    "for name, c in copies:\n",
    "    print(f\"{name:25} shares memory: {shares_memory(t, c)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common View Gotcha: Arithmetic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + 1 shares memory: False\n",
      "a * 2 shares memory: False\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic operations ALWAYS create new tensors\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = a + 1  # New tensor, not a view\n",
    "c = a * 2  # New tensor\n",
    "\n",
    "print(f\"a + 1 shares memory: {shares_memory(a, b)}\")\n",
    "print(f\"a * 2 shares memory: {shares_memory(a, c)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Broadcasting\n",
    "\n",
    "Broadcasting automatically expands tensors to compatible shapes for element-wise operations, without copying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Broadcasting Rules\n",
    "\n",
    "Two tensors are \"broadcastable\" if:\n",
    "1. Each tensor has at least one dimension\n",
    "2. Iterating from trailing (rightmost) dimensions, sizes either:\n",
    "   - Are equal\n",
    "   - One of them is 1\n",
    "   - One of them doesn't exist (shorter tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor + scalar:\n",
      "tensor([[10, 11, 12],\n",
      "        [13, 14, 15]])\n",
      "\n",
      "(2,3) + (3,) -> broadcasts row to each row:\n",
      "tensor([[100, 201, 302],\n",
      "        [103, 204, 305]])\n"
     ]
    }
   ],
   "source": [
    "# Simple broadcasting: scalar with tensor\n",
    "t = torch.arange(6).reshape(2, 3)\n",
    "result = t + 10\n",
    "print(f\"Tensor + scalar:\\n{result}\\n\")\n",
    "\n",
    "# 1D tensor with 2D tensor\n",
    "row = torch.tensor([100, 200, 300])\n",
    "result = t + row\n",
    "print(f\"(2,3) + (3,) -> broadcasts row to each row:\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (2,3):\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "Column vector (2,1):\n",
      "tensor([[10],\n",
      "        [20]])\n",
      "\n",
      "Result (2,3):\n",
      "tensor([[10, 11, 12],\n",
      "        [23, 24, 25]])\n"
     ]
    }
   ],
   "source": [
    "# Column vector broadcasting\n",
    "t = torch.arange(6).reshape(2, 3)\n",
    "col = torch.tensor([[10], [20]])  # Shape (2, 1)\n",
    "\n",
    "result = t + col\n",
    "print(f\"Original (2,3):\\n{t}\\n\")\n",
    "print(f\"Column vector (2,1):\\n{col}\\n\")\n",
    "print(f\"Result (2,3):\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (3,1):\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "\n",
      "b (1,2):\n",
      "tensor([[10, 20]])\n",
      "\n",
      "Outer product (3,2):\n",
      "tensor([[10, 20],\n",
      "        [20, 40],\n",
      "        [30, 60]])\n"
     ]
    }
   ],
   "source": [
    "# Classic broadcasting example: outer product\n",
    "a = torch.tensor([1, 2, 3])      # Shape (3,)\n",
    "b = torch.tensor([10, 20])       # Shape (2,)\n",
    "\n",
    "# Reshape for broadcasting\n",
    "a = a.unsqueeze(1)  # Shape (3, 1)\n",
    "b = b.unsqueeze(0)  # Shape (1, 2)\n",
    "\n",
    "outer = a * b  # Broadcasts to (3, 2)\n",
    "print(f\"a (3,1):\\n{a}\\n\")\n",
    "print(f\"b (1,2):\\n{b}\\n\")\n",
    "print(f\"Outer product (3,2):\\n{outer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting errors\n",
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(2, 4)  # Incompatible last dimension\n",
    "\n",
    "try:\n",
    "    c = a + b\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Broadcasting is Memory Efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape: torch.Size([1000, 1]), storage size: 1000\n",
      "b.shape: torch.Size([1000, 1000]), storage size: 1000\n",
      "\n",
      "Both use the same storage!\n",
      "b strides: (1, 0)\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting doesn't actually copy data\n",
    "a = torch.randn(1000, 1)  # 1000 elements\n",
    "b = a.expand(1000, 1000)  # Appears to be 1M elements\n",
    "\n",
    "print(f\"a.shape: {a.shape}, storage size: {a.storage().size()}\")\n",
    "print(f\"b.shape: {b.shape}, storage size: {b.storage().size()}\")\n",
    "print(f\"\\nBoth use the same storage!\")\n",
    "print(f\"b strides: {b.stride()}\")  # stride of 0 means broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. In-Place Operations\n",
    "\n",
    "In-place operations modify tensors directly without allocating new memory. They're denoted with a trailing underscore (e.g., `add_()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: tensor([1., 2., 3.]), id: 131883426926416\n",
      "After t + 1: tensor([2., 3., 4.]), id: 131883428231552 (different object)\n",
      "After t.add_(1): tensor([2., 3., 4.]), id: 131883426926416 (same object)\n"
     ]
    }
   ],
   "source": [
    "# In-place vs out-of-place\n",
    "t = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"Original: {t}, id: {id(t)}\")\n",
    "\n",
    "# Out-of-place: creates new tensor\n",
    "t2 = t + 1\n",
    "print(f\"After t + 1: {t2}, id: {id(t2)} (different object)\")\n",
    "\n",
    "# In-place: modifies existing tensor\n",
    "t.add_(1)\n",
    "print(f\"After t.add_(1): {t}, id: {id(t)} (same object)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: tensor([0.6373, 0.6032, 0.9491])\n"
     ]
    }
   ],
   "source": [
    "# Common in-place operations\n",
    "t = torch.ones(3)\n",
    "\n",
    "t.add_(2)      # t = t + 2\n",
    "t.mul_(3)      # t = t * 3\n",
    "t.zero_()      # t = 0\n",
    "t.fill_(5)     # t = 5\n",
    "t.uniform_()   # t ~ U(0, 1)\n",
    "\n",
    "print(f\"Final: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 In-Place Operations and Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: In-place operations can break autograd!\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# This will cause an error during backward\n",
    "try:\n",
    "    y.add_(1)  # In-place modification of a tensor needed for gradient computation\n",
    "    loss = y.sum()\n",
    "    loss.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# Safe pattern: don't use in-place ops on tensors in the computation graph\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "y = y + 1  # Out-of-place, creates new tensor\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "print(f\"Gradient: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 In-Place Operations and Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: base = tensor([0., 0., 0., 0., 0., 0.])\n",
      "After: base = tensor([1., 1., 1., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# In-place ops on views affect the base tensor\n",
    "base = torch.zeros(6)\n",
    "view = base.view(2, 3)\n",
    "\n",
    "print(f\"Before: base = {base}\")\n",
    "\n",
    "view[0].fill_(1)\n",
    "view[1].fill_(2)\n",
    "\n",
    "print(f\"After: base = {base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "Test your understanding with these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Matrix Operations from Scratch\n",
    "\n",
    "Implement the following using only basic tensor operations (no `@`, `mm()`, or `matmul()`):\n",
    "1. Matrix multiplication\n",
    "2. Batch matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_matmul(A, B):\n",
    "    \"\"\"\n",
    "    Compute matrix multiplication A @ B using only element-wise ops and sum.\n",
    "    A: shape (m, k)\n",
    "    B: shape (k, n)\n",
    "    Returns: shape (m, n)\n",
    "    \n",
    "    Hint: Use broadcasting and einsum-like thinking\n",
    "    A[i,k] * B[k,j] summed over k\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "expected = A @ B\n",
    "result = manual_matmul(A, B)\n",
    "#  print(f\"Correct: {torch.allclose(expected, result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Memory Detective\n",
    "\n",
    "For each operation, predict whether the result shares memory with the input, then verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shares_memory(a, b):\n",
    "    return a.storage().data_ptr() == b.storage().data_ptr()\n",
    "\n",
    "t = torch.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "operations = [\n",
    "    (\"t.flatten()\", lambda x: x.flatten()),\n",
    "    (\"t.reshape(-1)\", lambda x: x.reshape(-1)),\n",
    "    (\"t[0]\", lambda x: x[0]),\n",
    "    (\"t[0].clone()\", lambda x: x[0].clone()),\n",
    "    (\"t.transpose(0,1).contiguous()\", lambda x: x.transpose(0,1).contiguous()),\n",
    "    (\"t + 0\", lambda x: x + 0),\n",
    "    (\"t.float()\", lambda x: x.float()),\n",
    "]\n",
    "\n",
    "print(\"Predict before running!\\n\")\n",
    "for name, op in operations:\n",
    "    result = op(t)\n",
    "    # print(f\"{name:35} shares memory: {shares_memory(t, result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Broadcasting Challenge\n",
    "\n",
    "Implement the following using broadcasting (no loops):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(X, Y):\n",
    "    \"\"\"\n",
    "    Compute pairwise Euclidean distances between all pairs of points.\n",
    "    \n",
    "    X: shape (n, d) - n points in d dimensions\n",
    "    Y: shape (m, d) - m points in d dimensions\n",
    "    Returns: shape (n, m) - distance[i,j] = ||X[i] - Y[j]||_2\n",
    "    \n",
    "    Hint: ||a-b||^2 = ||a||^2 + ||b||^2 - 2*a.b\n",
    "    Or use broadcasting: (X[:, None, :] - Y[None, :, :]) gives (n, m, d)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "X = torch.randn(5, 3)\n",
    "Y = torch.randn(7, 3)\n",
    "dists = pairwise_distances(X, Y)\n",
    "# print(f\"Output shape: {dists.shape}\")  # Should be (5, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 1 - Correct: True\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1 Solution\n",
    "def manual_matmul_solution(A, B):\n",
    "    \"\"\"A @ B using broadcasting.\"\"\"\n",
    "    # A: (m, k) -> (m, k, 1)\n",
    "    # B: (k, n) -> (1, k, n)\n",
    "    # Product: (m, k, n), sum over k -> (m, n)\n",
    "    return (A.unsqueeze(-1) * B.unsqueeze(0)).sum(dim=1)\n",
    "\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "expected = A @ B\n",
    "result = manual_matmul_solution(A, B)\n",
    "print(f\"Exercise 1 - Correct: {torch.allclose(expected, result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 2 - Memory sharing results:\n",
      "Operation                           Shares Memory   Why\n",
      "--------------------------------------------------------------------------------\n",
      "t.flatten()                         True            View (if contiguous)\n",
      "t.reshape(-1)                       True            View (if possible)\n",
      "t[0]                                True            Slicing is a view\n",
      "t[0].clone()                        False           Clone always copies\n",
      "t.transpose(0,1).contiguous()       False           contiguous() copies if needed\n",
      "t + 0                               False           Arithmetic creates new tensor\n",
      "t.float()                           False           Type conversion copies\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2 Solution\n",
    "t = torch.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "print(\"Exercise 2 - Memory sharing results:\")\n",
    "print(f\"{'Operation':35} {'Shares Memory':15} {'Why'}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'t.flatten()':35} {str(shares_memory(t, t.flatten())):15} {'View (if contiguous)'}\")\n",
    "print(f\"{'t.reshape(-1)':35} {str(shares_memory(t, t.reshape(-1))):15} {'View (if possible)'}\")\n",
    "print(f\"{'t[0]':35} {str(shares_memory(t, t[0])):15} {'Slicing is a view'}\")\n",
    "print(f\"{'t[0].clone()':35} {str(shares_memory(t, t[0].clone())):15} {'Clone always copies'}\")\n",
    "print(f\"{'t.transpose(0,1).contiguous()':35} {str(shares_memory(t, t.transpose(0,1).contiguous())):15} {'contiguous() copies if needed'}\")\n",
    "print(f\"{'t + 0':35} {str(shares_memory(t, t + 0)):15} {'Arithmetic creates new tensor'}\")\n",
    "print(f\"{'t.float()':35} {str(shares_memory(t, t.float())):15} {'Type conversion copies'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 3 - Output shape: torch.Size([5, 7])\n",
      "Correct: True\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3 Solution\n",
    "def pairwise_distances_solution(X, Y):\n",
    "    \"\"\"Compute pairwise Euclidean distances using broadcasting.\"\"\"\n",
    "    # Method 1: Direct broadcasting (clearer but uses more memory)\n",
    "    # diff = X[:, None, :] - Y[None, :, :]  # (n, m, d)\n",
    "    # return torch.sqrt((diff ** 2).sum(dim=-1))  # (n, m)\n",
    "    \n",
    "    # Method 2: Expanded formula (more memory efficient)\n",
    "    # ||a-b||^2 = ||a||^2 + ||b||^2 - 2*a.b\n",
    "    X_sqnorm = (X ** 2).sum(dim=1, keepdim=True)  # (n, 1)\n",
    "    Y_sqnorm = (Y ** 2).sum(dim=1, keepdim=True)  # (m, 1)\n",
    "    XY = X @ Y.T  # (n, m)\n",
    "    \n",
    "    sq_dists = X_sqnorm + Y_sqnorm.T - 2 * XY\n",
    "    sq_dists = torch.clamp(sq_dists, min=0)  # Numerical stability\n",
    "    return torch.sqrt(sq_dists)\n",
    "\n",
    "X = torch.randn(5, 3)\n",
    "Y = torch.randn(7, 3)\n",
    "dists = pairwise_distances_solution(X, Y)\n",
    "print(f\"Exercise 3 - Output shape: {dists.shape}\")  # Should be (5, 7)\n",
    "\n",
    "# Verify with explicit loop\n",
    "expected = torch.zeros(5, 7)\n",
    "for i in range(5):\n",
    "    for j in range(7):\n",
    "        expected[i, j] = torch.sqrt(((X[i] - Y[j]) ** 2).sum())\n",
    "print(f\"Correct: {torch.allclose(dists, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Key takeaways from this notebook:\n",
    "\n",
    "1. **Tensor Creation**: Use `torch.tensor()` for copies, `torch.from_numpy()` for shared memory\n",
    "2. **dtypes**: Choose based on precision/memory trade-off; float16/bfloat16 for GPU training\n",
    "3. **Devices**: Keep tensors on the same device; use device-agnostic patterns\n",
    "4. **Memory Layout**: Understand strides and contiguity for performance\n",
    "5. **Views vs Copies**: Most reshaping ops are views; be aware of shared memory\n",
    "6. **Broadcasting**: Powerful for vectorized operations; understand the rules\n",
    "7. **In-place ops**: Use carefully, especially with autograd\n",
    "\n",
    "---\n",
    "*Next: Module 1.2 - Autograd Internals*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
