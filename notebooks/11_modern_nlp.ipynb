{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Modern NLP with Transformers\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand pretrained language models** - BERT, GPT, and their variants\n",
    "2. **Use HuggingFace Transformers** - Load and use pretrained models\n",
    "3. **Fine-tune for downstream tasks** - Classification, NER, question answering\n",
    "4. **Master tokenization** - BPE, WordPiece, and handling special tokens\n",
    "5. **Apply efficient fine-tuning techniques** - LoRA, freezing layers, learning rate scheduling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers datasets tokenizers accelerate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# HuggingFace imports\n",
    "from transformers import (\n",
    "    AutoModel, AutoModelForSequenceClassification, AutoModelForTokenClassification,\n",
    "    AutoModelForQuestionAnswering, AutoModelForCausalLM,\n",
    "    AutoTokenizer, AutoConfig,\n",
    "    TrainingArguments, Trainer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Understanding Pretrained Language Models\n",
    "\n",
    "### 1.1 The Pretraining Paradigm\n",
    "\n",
    "Modern NLP follows a two-stage approach:\n",
    "1. **Pretraining**: Learn general language understanding from massive unlabeled text\n",
    "2. **Fine-tuning**: Adapt to specific downstream tasks with labeled data\n",
    "\n",
    "Key model families:\n",
    "- **Encoder-only (BERT)**: Bidirectional, good for understanding tasks\n",
    "- **Decoder-only (GPT)**: Autoregressive, good for generation\n",
    "- **Encoder-Decoder (T5, BART)**: Good for seq2seq tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model architectures\n",
    "\n",
    "model_info = {\n",
    "    'bert-base-uncased': {\n",
    "        'type': 'Encoder-only',\n",
    "        'params': '110M',\n",
    "        'pretraining': 'MLM + NSP',\n",
    "        'use_cases': 'Classification, NER, QA'\n",
    "    },\n",
    "    'gpt2': {\n",
    "        'type': 'Decoder-only', \n",
    "        'params': '124M',\n",
    "        'pretraining': 'Causal LM',\n",
    "        'use_cases': 'Text generation'\n",
    "    },\n",
    "    'roberta-base': {\n",
    "        'type': 'Encoder-only',\n",
    "        'params': '125M', \n",
    "        'pretraining': 'MLM (no NSP)',\n",
    "        'use_cases': 'Same as BERT, often better'\n",
    "    },\n",
    "    'distilbert-base-uncased': {\n",
    "        'type': 'Encoder-only',\n",
    "        'params': '66M',\n",
    "        'pretraining': 'Distillation from BERT',\n",
    "        'use_cases': 'Same as BERT, faster'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Popular Pretrained Models:\\n\")\n",
    "for model_name, info in model_info.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 BERT: Bidirectional Encoder Representations from Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Tokenizer converts text to token IDs\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Base model (without task-specific head)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model config: {model.config.hidden_size}d, {model.config.num_hidden_layers} layers, {model.config.num_attention_heads} heads\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand BERT's input format\n",
    "\n",
    "text = \"Hello, how are you doing today?\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Convert to IDs with special tokens\n",
    "encoded = tokenizer(text, return_tensors='pt')\n",
    "print(f\"\\nEncoded:\")\n",
    "print(f\"  input_ids: {encoded['input_ids']}\")\n",
    "print(f\"  token_type_ids: {encoded['token_type_ids']}\")\n",
    "print(f\"  attention_mask: {encoded['attention_mask']}\")\n",
    "\n",
    "# Decode back\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "print(f\"\\nDecoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT special tokens\n",
    "\n",
    "print(\"Special tokens:\")\n",
    "print(f\"  [CLS] (classification): {tokenizer.cls_token} -> {tokenizer.cls_token_id}\")\n",
    "print(f\"  [SEP] (separator): {tokenizer.sep_token} -> {tokenizer.sep_token_id}\")\n",
    "print(f\"  [PAD] (padding): {tokenizer.pad_token} -> {tokenizer.pad_token_id}\")\n",
    "print(f\"  [MASK] (masking): {tokenizer.mask_token} -> {tokenizer.mask_token_id}\")\n",
    "print(f\"  [UNK] (unknown): {tokenizer.unk_token} -> {tokenizer.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BERT embeddings\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# BERT outputs:\n",
    "# - last_hidden_state: (batch, seq_len, hidden_size) - token representations\n",
    "# - pooler_output: (batch, hidden_size) - [CLS] token passed through pooling layer\n",
    "\n",
    "print(f\"Last hidden state shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"Pooler output shape: {outputs.pooler_output.shape}\")\n",
    "\n",
    "# [CLS] token representation (often used for classification)\n",
    "cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "print(f\"\\n[CLS] embedding shape: {cls_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Tokenization Deep Dive\n",
    "\n",
    "### 2.1 Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different tokenization strategies\n",
    "\n",
    "# BERT uses WordPiece\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# GPT-2 uses BPE (Byte Pair Encoding)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# RoBERTa also uses BPE\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "test_text = \"I'm learning about transformers and tokenization!\"\n",
    "\n",
    "print(f\"Text: {test_text}\\n\")\n",
    "print(f\"BERT (WordPiece): {bert_tokenizer.tokenize(test_text)}\")\n",
    "print(f\"GPT-2 (BPE): {gpt2_tokenizer.tokenize(test_text)}\")\n",
    "print(f\"RoBERTa (BPE): {roberta_tokenizer.tokenize(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How subword tokenization handles unknown words\n",
    "\n",
    "unusual_text = \"The transformerification of NLP is antidisestablishmentarianism.\"\n",
    "\n",
    "print(f\"Unusual text: {unusual_text}\\n\")\n",
    "print(f\"BERT tokens: {bert_tokenizer.tokenize(unusual_text)}\")\n",
    "\n",
    "# Subword tokenization can represent ANY word as a sequence of subwords\n",
    "# This means no [UNK] tokens for out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling long sequences\n",
    "\n",
    "long_text = \"This is a very long text. \" * 100\n",
    "\n",
    "# Most models have a maximum sequence length\n",
    "print(f\"BERT max length: {bert_tokenizer.model_max_length}\")\n",
    "\n",
    "# Tokenizer can truncate and pad\n",
    "encoded = bert_tokenizer(\n",
    "    long_text,\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(f\"Encoded length: {encoded['input_ids'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch tokenization with padding\n",
    "\n",
    "texts = [\n",
    "    \"Short text.\",\n",
    "    \"This is a medium-length piece of text.\",\n",
    "    \"And this one is even longer, containing many more words and tokens to process.\"\n",
    "]\n",
    "\n",
    "# Pad to longest in batch\n",
    "batch_encoded = bert_tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(f\"Batch shape: {batch_encoded['input_ids'].shape}\")\n",
    "print(f\"\\nAttention masks (1=real token, 0=padding):\")\n",
    "for i, mask in enumerate(batch_encoded['attention_mask']):\n",
    "    print(f\"  Text {i}: {mask.sum().item()} real tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Text Classification with Fine-tuning\n",
    "\n",
    "### 3.1 Loading a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDb sentiment classification dataset\n",
    "\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "print(f\"Dataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Text: {dataset['train'][0]['text'][:200]}...\")\n",
    "print(f\"Label: {dataset['train'][0]['label']} (0=negative, 1=positive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "\n",
    "model_name = 'distilbert-base-uncased'  # Smaller model for faster training\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Use a small subset for demonstration\n",
    "small_train = dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "small_test = dataset['test'].shuffle(seed=42).select(range(200))\n",
    "\n",
    "tokenized_train = small_train.map(tokenize_function, batched=True)\n",
    "tokenized_test = small_test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_test.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(f\"Tokenized train sample keys: {tokenized_train[0].keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fine-tuning with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for sequence classification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  Base model: DistilBERT\")\n",
    "print(f\"  Classification head: Linear({model.config.hidden_size} -> 2)\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual training loop\n",
    "\n",
    "train_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(tokenized_test, batch_size=32)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Learning rate schedule with warmup\n",
    "num_epochs = 3\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = num_training_steps // 10\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs\")\n",
    "print(f\"Total steps: {num_training_steps}, warmup steps: {num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        total_loss += outputs.loss.item()\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new examples\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_sentiment(texts: List[str]):\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    probs = F.softmax(outputs.logits, dim=-1)\n",
    "    preds = probs.argmax(dim=-1)\n",
    "    \n",
    "    labels = ['Negative', 'Positive']\n",
    "    \n",
    "    for text, pred, prob in zip(texts, preds, probs):\n",
    "        print(f\"Text: {text[:50]}...\")\n",
    "        print(f\"Prediction: {labels[pred]} (confidence: {prob[pred]:.3f})\\n\")\n",
    "\n",
    "\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic! Best film I've seen all year.\",\n",
    "    \"What a waste of time. Terrible acting and boring plot.\",\n",
    "    \"It was okay, nothing special but not bad either.\"\n",
    "]\n",
    "\n",
    "predict_sentiment(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Using HuggingFace Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Trainer API makes fine-tuning easier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "\n",
    "# Reload fresh model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer configured! Call trainer.train() to start training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Named Entity Recognition (Token Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER model: classify each token\n",
    "\n",
    "ner_model_name = 'dslim/bert-base-NER'\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name)\n",
    "ner_model.to(device)\n",
    "ner_model.eval()\n",
    "\n",
    "# NER labels\n",
    "id2label = ner_model.config.id2label\n",
    "print(f\"NER labels: {id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform NER\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_entities(text: str) -> List[Dict]:\n",
    "    \"\"\"Extract named entities from text\"\"\"\n",
    "    \n",
    "    inputs = ner_tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop('offset_mapping')[0]\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    outputs = ner_model(**inputs)\n",
    "    predictions = outputs.logits.argmax(dim=-1)[0]\n",
    "    \n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for idx, (pred, offset) in enumerate(zip(predictions, offset_mapping)):\n",
    "        label = id2label[pred.item()]\n",
    "        \n",
    "        if label == 'O':\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "        elif label.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {\n",
    "                'type': label[2:],\n",
    "                'start': offset[0].item(),\n",
    "                'end': offset[1].item(),\n",
    "                'text': text[offset[0]:offset[1]]\n",
    "            }\n",
    "        elif label.startswith('I-') and current_entity:\n",
    "            if label[2:] == current_entity['type']:\n",
    "                current_entity['end'] = offset[1].item()\n",
    "                current_entity['text'] = text[current_entity['start']:current_entity['end']]\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "# Test\n",
    "test_text = \"Apple CEO Tim Cook announced a new iPhone at the event in San Francisco.\"\n",
    "entities = extract_entities(test_text)\n",
    "\n",
    "print(f\"Text: {test_text}\\n\")\n",
    "print(\"Entities found:\")\n",
    "for entity in entities:\n",
    "    print(f\"  {entity['text']} ({entity['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractive QA: find answer span in context\n",
    "\n",
    "qa_model_name = 'distilbert-base-cased-distilled-squad'\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "qa_model.to(device)\n",
    "qa_model.eval()\n",
    "\n",
    "print(f\"QA model loaded: {qa_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def answer_question(question: str, context: str) -> Dict:\n",
    "    \"\"\"Extract answer from context given a question\"\"\"\n",
    "    \n",
    "    inputs = qa_tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        return_tensors='pt',\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = qa_model(**inputs)\n",
    "    \n",
    "    # Get most likely start and end positions\n",
    "    start_scores = outputs.start_logits[0]\n",
    "    end_scores = outputs.end_logits[0]\n",
    "    \n",
    "    start_idx = start_scores.argmax().item()\n",
    "    end_idx = end_scores.argmax().item()\n",
    "    \n",
    "    # Ensure valid span\n",
    "    if end_idx < start_idx:\n",
    "        end_idx = start_idx\n",
    "    \n",
    "    # Decode answer\n",
    "    answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]\n",
    "    answer = qa_tokenizer.decode(answer_tokens)\n",
    "    \n",
    "    # Confidence\n",
    "    start_prob = F.softmax(start_scores, dim=-1)[start_idx].item()\n",
    "    end_prob = F.softmax(end_scores, dim=-1)[end_idx].item()\n",
    "    confidence = (start_prob + end_prob) / 2\n",
    "    \n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'confidence': confidence,\n",
    "        'start': start_idx,\n",
    "        'end': end_idx\n",
    "    }\n",
    "\n",
    "\n",
    "# Test\n",
    "context = \"\"\"\n",
    "PyTorch is an open source machine learning framework based on the Torch library, \n",
    "used for applications such as computer vision and natural language processing. \n",
    "It was primarily developed by Meta AI (formerly Facebook's AI Research lab). \n",
    "PyTorch was released in October 2016 and has become one of the most popular \n",
    "deep learning frameworks alongside TensorFlow.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Who developed PyTorch?\",\n",
    "    \"When was PyTorch released?\",\n",
    "    \"What is PyTorch used for?\"\n",
    "]\n",
    "\n",
    "print(f\"Context: {context[:100]}...\\n\")\n",
    "for q in questions:\n",
    "    result = answer_question(q, context)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']} (confidence: {result['confidence']:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Text Generation with GPT\n",
    "\n",
    "### 6.1 Basic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2\n",
    "\n",
    "gpt_model_name = 'gpt2'\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(gpt_model_name)\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(gpt_model_name)\n",
    "gpt_model.to(device)\n",
    "gpt_model.eval()\n",
    "\n",
    "# GPT-2 doesn't have a pad token by default\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "print(f\"GPT-2 loaded: {sum(p.numel() for p in gpt_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 0.7,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.95,\n",
    "    do_sample: bool = True\n",
    ") -> str:\n",
    "    \"\"\"Generate text continuation\"\"\"\n",
    "    \n",
    "    inputs = gpt_tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    outputs = gpt_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=do_sample,\n",
    "        pad_token_id=gpt_tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Test generation\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In a world where machines can think,\",\n",
    "    \"Once upon a time, in a land far away,\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    generated = generate_text(prompt, max_new_tokens=40)\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different decoding strategies\n",
    "\n",
    "prompt = \"The key to success in machine learning is\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "# Greedy decoding (temperature=0, no sampling)\n",
    "print(\"1. Greedy decoding:\")\n",
    "print(generate_text(prompt, do_sample=False, max_new_tokens=30))\n",
    "\n",
    "# Low temperature (more focused)\n",
    "print(\"\\n2. Low temperature (0.3):\")\n",
    "print(generate_text(prompt, temperature=0.3, max_new_tokens=30))\n",
    "\n",
    "# High temperature (more creative)\n",
    "print(\"\\n3. High temperature (1.0):\")\n",
    "print(generate_text(prompt, temperature=1.0, max_new_tokens=30))\n",
    "\n",
    "# Top-p sampling\n",
    "print(\"\\n4. Top-p (nucleus) sampling:\")\n",
    "print(generate_text(prompt, top_p=0.9, top_k=0, max_new_tokens=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Efficient Fine-tuning Techniques\n",
    "\n",
    "### 7.1 Freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model, only train classification head\n",
    "\n",
    "def freeze_base_model(model):\n",
    "    \"\"\"Freeze all parameters except the classification head\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' not in name and 'pooler' not in name:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "\n",
    "\n",
    "# Load fresh model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "print(\"Before freezing:\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "print(\"\\nAfter freezing:\")\n",
    "freeze_base_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradual unfreezing\n",
    "\n",
    "def unfreeze_layers(model, num_layers: int):\n",
    "    \"\"\"Unfreeze the top N transformer layers\"\"\"\n",
    "    # First freeze everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze classifier\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Unfreeze top N encoder layers\n",
    "    total_layers = model.config.num_hidden_layers\n",
    "    for i in range(total_layers - num_layers, total_layers):\n",
    "        for param in model.distilbert.transformer.layer[i].parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Unfroze top {num_layers} layers: {trainable:,} / {total:,} trainable ({100*trainable/total:.2f}%)\")\n",
    "\n",
    "\n",
    "# Example\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "unfreeze_layers(model, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA: Add small trainable matrices to frozen weights\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA adapter layer.\n",
    "    \n",
    "    Instead of training W, we train W + A @ B where A and B are low-rank.\n",
    "    This dramatically reduces trainable parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, original_layer: nn.Linear, rank: int = 8, alpha: float = 16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # Low-rank matrices\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) / rank)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.original_layer.weight.requires_grad = False\n",
    "        if self.original_layer.bias is not None:\n",
    "            self.original_layer.bias.requires_grad = False\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Original output + LoRA delta\n",
    "        original_output = self.original_layer(x)\n",
    "        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "        return original_output + lora_output\n",
    "\n",
    "\n",
    "# Example\n",
    "original = nn.Linear(768, 768)\n",
    "lora = LoRALayer(original, rank=8)\n",
    "\n",
    "original_params = sum(p.numel() for p in original.parameters())\n",
    "lora_params = lora.lora_A.numel() + lora.lora_B.numel()\n",
    "\n",
    "print(f\"Original layer parameters: {original_params:,}\")\n",
    "print(f\"LoRA parameters: {lora_params:,}\")\n",
    "print(f\"Reduction: {100 * (1 - lora_params/original_params):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to attention layers\n",
    "\n",
    "def add_lora_to_model(model, rank: int = 8, alpha: float = 16):\n",
    "    \"\"\"\n",
    "    Add LoRA adapters to query and value projections.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and ('query' in name or 'value' in name):\n",
    "            # Replace with LoRA version\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            child_name = name.split('.')[-1]\n",
    "            \n",
    "            parent = model\n",
    "            for attr in parent_name.split('.'):\n",
    "                if attr:\n",
    "                    parent = getattr(parent, attr)\n",
    "            \n",
    "            setattr(parent, child_name, LoRALayer(module, rank, alpha))\n",
    "    \n",
    "    # Count parameters\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"After LoRA: {trainable:,} / {total:,} trainable ({100*trainable/total:.2f}%)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"LoRA implementation complete!\")\n",
    "print(\"For production use, consider using the peft library from HuggingFace.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Multi-label Classification\n",
    "\n",
    "Modify the classification model to handle multi-label classification (multiple labels per example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Multi-label classification\n",
    "\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT-based multi-label classifier.\n",
    "    \n",
    "    Each example can have multiple labels (e.g., tags, categories).\n",
    "    \n",
    "    Key changes from single-label:\n",
    "    1. Use BCEWithLogitsLoss instead of CrossEntropyLoss\n",
    "    2. Use sigmoid instead of softmax for predictions\n",
    "    3. Labels are multi-hot encoded\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, num_labels: int):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Contrastive Learning for Sentence Embeddings\n",
    "\n",
    "Create a model that learns good sentence embeddings using contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Contrastive sentence embeddings\n",
    "\n",
    "class ContrastiveSentenceEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Learn sentence embeddings using contrastive learning.\n",
    "    \n",
    "    Similar sentences should have similar embeddings.\n",
    "    Uses InfoNCE loss (similar to SimCLR, CLIP).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: Use mean pooling over tokens for sentence embedding\n",
    "        pass\n",
    "    \n",
    "    def encode(self, input_ids, attention_mask) -> torch.Tensor:\n",
    "        \"\"\"Get sentence embeddings\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, anchor_ids, anchor_mask, positive_ids, positive_mask):\n",
    "        \"\"\"Compute contrastive loss\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Build a Simple Chatbot\n",
    "\n",
    "Create a simple chatbot using GPT-2 with context management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Simple chatbot\n",
    "\n",
    "class SimpleChatbot:\n",
    "    \"\"\"\n",
    "    A simple chatbot using GPT-2.\n",
    "    \n",
    "    Features:\n",
    "    - Maintains conversation history\n",
    "    - Handles context length limits\n",
    "    - Uses appropriate generation parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'gpt2', max_context_length: int = 512):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def respond(self, user_message: str) -> str:\n",
    "        \"\"\"Generate a response to the user message\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: Format as \"User: ... \\nAssistant: ...\"\n",
    "        pass\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Multi-label classification\n",
    "\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, model_name: str, num_labels: int):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # BCEWithLogitsLoss for multi-label\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(logits, labels.float())\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}\n",
    "    \n",
    "    def predict(self, input_ids, attention_mask, threshold: float = 0.5):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(outputs['logits'])\n",
    "            return (probs > threshold).int()\n",
    "\n",
    "\n",
    "# Test\n",
    "multi_label_model = MultiLabelClassifier('distilbert-base-uncased', num_labels=5)\n",
    "print(f\"Multi-label classifier created with {sum(p.numel() for p in multi_label_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Contrastive sentence encoder\n",
    "\n",
    "class ContrastiveSentenceEncoder(nn.Module):\n",
    "    def __init__(self, model_name: str, embedding_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
    "        )\n",
    "        self.temperature = 0.07\n",
    "    \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        \"\"\"Mean pooling over token embeddings\"\"\"\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def encode(self, input_ids, attention_mask) -> torch.Tensor:\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.mean_pooling(outputs.last_hidden_state, attention_mask)\n",
    "        embeddings = self.projection(pooled)\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    def forward(self, anchor_ids, anchor_mask, positive_ids, positive_mask):\n",
    "        # Get embeddings\n",
    "        anchor_emb = self.encode(anchor_ids, anchor_mask)\n",
    "        positive_emb = self.encode(positive_ids, positive_mask)\n",
    "        \n",
    "        # InfoNCE loss\n",
    "        batch_size = anchor_emb.size(0)\n",
    "        \n",
    "        # Similarity matrix\n",
    "        sim_matrix = torch.matmul(anchor_emb, positive_emb.T) / self.temperature\n",
    "        \n",
    "        # Labels: positive pairs are on diagonal\n",
    "        labels = torch.arange(batch_size, device=anchor_emb.device)\n",
    "        \n",
    "        # Cross entropy loss (both directions)\n",
    "        loss = (F.cross_entropy(sim_matrix, labels) + F.cross_entropy(sim_matrix.T, labels)) / 2\n",
    "        \n",
    "        return {'loss': loss, 'anchor_emb': anchor_emb, 'positive_emb': positive_emb}\n",
    "\n",
    "\n",
    "# Test\n",
    "contrastive_model = ContrastiveSentenceEncoder('distilbert-base-uncased')\n",
    "print(f\"Contrastive encoder created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Simple chatbot\n",
    "\n",
    "class SimpleChatbot:\n",
    "    def __init__(self, model_name: str = 'gpt2', max_context_length: int = 512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.max_context_length = max_context_length\n",
    "        self.history = []\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def _build_prompt(self, user_message: str) -> str:\n",
    "        \"\"\"Build prompt with conversation history\"\"\"\n",
    "        prompt = \"\"\n",
    "        for turn in self.history:\n",
    "            prompt += f\"User: {turn['user']}\\nAssistant: {turn['assistant']}\\n\"\n",
    "        prompt += f\"User: {user_message}\\nAssistant:\"\n",
    "        return prompt\n",
    "    \n",
    "    def _truncate_history(self, prompt: str) -> str:\n",
    "        \"\"\"Remove old history if prompt is too long\"\"\"\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        while len(tokens) > self.max_context_length - 50 and self.history:\n",
    "            self.history.pop(0)\n",
    "            prompt = self._build_prompt(prompt.split(\"User: \")[-1].split(\"\\n\")[0])\n",
    "            tokens = self.tokenizer.encode(prompt)\n",
    "        return prompt\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def respond(self, user_message: str) -> str:\n",
    "        prompt = self._build_prompt(user_message)\n",
    "        prompt = self._truncate_history(prompt)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.tokenizer.encode('\\n')[0]  # Stop at newline\n",
    "        )\n",
    "        \n",
    "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        assistant_response = full_response[len(prompt):].strip()\n",
    "        \n",
    "        # Clean up response\n",
    "        if 'User:' in assistant_response:\n",
    "            assistant_response = assistant_response.split('User:')[0].strip()\n",
    "        \n",
    "        # Add to history\n",
    "        self.history.append({\n",
    "            'user': user_message,\n",
    "            'assistant': assistant_response\n",
    "        })\n",
    "        \n",
    "        return assistant_response\n",
    "    \n",
    "    def clear_history(self):\n",
    "        self.history = []\n",
    "\n",
    "\n",
    "# Test\n",
    "chatbot = SimpleChatbot()\n",
    "print(\"Chatbot: Hello! How can I help you today?\")\n",
    "print(f\"User: What is machine learning?\")\n",
    "print(f\"Chatbot: {chatbot.respond('What is machine learning?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Pretrained Models**:\n",
    "   - BERT (encoder): bidirectional, good for understanding\n",
    "   - GPT (decoder): causal, good for generation\n",
    "   - Use the right model for your task\n",
    "\n",
    "2. **Tokenization**:\n",
    "   - Subword tokenization (BPE, WordPiece)\n",
    "   - Handle special tokens ([CLS], [SEP], [PAD])\n",
    "   - Attention mask indicates real vs. padding tokens\n",
    "\n",
    "3. **Fine-tuning**:\n",
    "   - Task-specific heads on top of pretrained models\n",
    "   - Lower learning rates (2e-5 to 5e-5)\n",
    "   - Warmup + linear decay schedule\n",
    "\n",
    "4. **Efficient Fine-tuning**:\n",
    "   - Freeze base model, train only head\n",
    "   - Gradual unfreezing (top layers first)\n",
    "   - LoRA: low-rank adaptation\n",
    "\n",
    "5. **HuggingFace Ecosystem**:\n",
    "   - `transformers`: models and tokenizers\n",
    "   - `datasets`: data loading\n",
    "   - `Trainer`: simplified training\n",
    "\n",
    "### Common Tasks\n",
    "\n",
    "| Task | Model Type | Output |\n",
    "|------|------------|--------|\n",
    "| Classification | AutoModelForSequenceClassification | Logits per class |\n",
    "| NER | AutoModelForTokenClassification | Logits per token |\n",
    "| QA | AutoModelForQuestionAnswering | Start/end positions |\n",
    "| Generation | AutoModelForCausalLM | Next token logits |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
