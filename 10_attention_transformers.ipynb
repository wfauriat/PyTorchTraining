{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Attention Mechanisms and Transformers\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand attention mechanisms** - How attention allows models to focus on relevant parts of input\n",
    "2. **Implement self-attention from scratch** - Scaled dot-product attention and multi-head attention\n",
    "3. **Build a Transformer encoder** - Layer normalization, feedforward networks, residual connections\n",
    "4. **Build a Transformer decoder** - Masked attention, cross-attention\n",
    "5. **Understand positional encodings** - Why they're needed and how they work\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Attention\n",
    "\n",
    "### 1.1 Why Attention?\n",
    "\n",
    "Traditional seq2seq models compress entire input into a single fixed-size vector. Attention allows the model to **look back** at all input positions when generating each output.\n",
    "\n",
    "Key insight: Not all input tokens are equally relevant for predicting each output token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple attention intuition\n",
    "\n",
    "def simple_attention(query: torch.Tensor, keys: torch.Tensor, values: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Basic attention mechanism.\n",
    "    \n",
    "    Query asks: \"What am I looking for?\"\n",
    "    Keys answer: \"What do I contain?\"\n",
    "    Values contain: \"What information do I have?\"\n",
    "    \n",
    "    Args:\n",
    "        query: What we're searching for (d_k,)\n",
    "        keys: What each position contains (seq_len, d_k)\n",
    "        values: Information at each position (seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        Weighted sum of values based on query-key similarity\n",
    "    \"\"\"\n",
    "    # Compute similarity scores\n",
    "    scores = torch.matmul(keys, query)  # (seq_len,)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    weights = F.softmax(scores, dim=0)  # (seq_len,)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = torch.matmul(weights, values)  # (d_v,)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "\n",
    "# Example: finding relevant information\n",
    "d_k, d_v, seq_len = 4, 8, 5\n",
    "\n",
    "query = torch.randn(d_k)\n",
    "keys = torch.randn(seq_len, d_k)\n",
    "values = torch.randn(seq_len, d_v)\n",
    "\n",
    "output, weights = simple_attention(query, keys, values)\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Keys shape: {keys.shape}\")\n",
    "print(f\"Values shape: {values.shape}\")\n",
    "print(f\"Attention weights: {weights}\")\n",
    "print(f\"Weights sum to 1: {weights.sum():.4f}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Scaled Dot-Product Attention\n",
    "\n",
    "The Transformer uses **scaled** dot-product attention:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The scaling by $\\sqrt{d_k}$ prevents the dot products from getting too large (which would make softmax saturate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    dropout: float = 0.0\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        query: (batch, num_heads, seq_len_q, d_k)\n",
    "        key: (batch, num_heads, seq_len_k, d_k)\n",
    "        value: (batch, num_heads, seq_len_k, d_v)\n",
    "        mask: Optional mask (batch, 1, 1, seq_len_k) or (batch, 1, seq_len_q, seq_len_k)\n",
    "        dropout: Dropout probability\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, num_heads, seq_len_q, d_v)\n",
    "        attention_weights: (batch, num_heads, seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    # scores: (batch, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    # Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply dropout\n",
    "    if dropout > 0:\n",
    "        attention_weights = F.dropout(attention_weights, p=dropout)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Test\n",
    "batch, num_heads, seq_len, d_k = 2, 4, 10, 64\n",
    "d_v = d_k\n",
    "\n",
    "Q = torch.randn(batch, num_heads, seq_len, d_k)\n",
    "K = torch.randn(batch, num_heads, seq_len, d_k)\n",
    "V = torch.randn(batch, num_heads, seq_len, d_v)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why scaling matters\n",
    "\n",
    "def show_scaling_effect():\n",
    "    \"\"\"Demonstrate why we scale by sqrt(d_k)\"\"\"\n",
    "    d_k_values = [16, 64, 256, 1024]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, d_k in zip(axes, d_k_values):\n",
    "        # Random Q, K\n",
    "        Q = torch.randn(1, 1, 10, d_k)\n",
    "        K = torch.randn(1, 1, 10, d_k)\n",
    "        \n",
    "        # Unscaled scores\n",
    "        scores_unscaled = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        \n",
    "        # Scaled scores\n",
    "        scores_scaled = scores_unscaled / math.sqrt(d_k)\n",
    "        \n",
    "        # Softmax\n",
    "        attn_unscaled = F.softmax(scores_unscaled, dim=-1)\n",
    "        attn_scaled = F.softmax(scores_scaled, dim=-1)\n",
    "        \n",
    "        # Visualize\n",
    "        ax.bar(range(10), attn_unscaled[0, 0, 0].numpy(), alpha=0.5, label='Unscaled')\n",
    "        ax.bar(range(10), attn_scaled[0, 0, 0].numpy(), alpha=0.5, label='Scaled')\n",
    "        ax.set_xlabel('Position')\n",
    "        ax.set_ylabel('Attention Weight')\n",
    "        ax.set_title(f'd_k = {d_k}')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.suptitle('Scaling prevents softmax saturation at high dimensions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_scaling_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Multi-Head Attention\n",
    "\n",
    "Instead of a single attention function, we project Q, K, V into multiple subspaces (\"heads\") and attend in parallel. This allows the model to attend to different aspects of the input simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention from \"Attention Is All You Need\".\n",
    "    \n",
    "    MultiHead(Q, K, V) = Concat(head_1, ..., head_h) @ W_o\n",
    "    where head_i = Attention(Q @ W_q^i, K @ W_k^i, V @ W_v^i)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch, seq_len_q, d_model)\n",
    "            key: (batch, seq_len_k, d_model)\n",
    "            value: (batch, seq_len_k, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len_q, d_model)\n",
    "            attention_weights: (batch, num_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)  # (batch, seq_len_q, d_model)\n",
    "        K = self.W_k(key)    # (batch, seq_len_k, d_model)\n",
    "        V = self.W_v(value)  # (batch, seq_len_k, d_model)\n",
    "        \n",
    "        # Reshape to (batch, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# Test\n",
    "mha = MultiHeadAttention(d_model=256, num_heads=8)\n",
    "x = torch.randn(4, 20, 256)  # (batch, seq_len, d_model)\n",
    "\n",
    "# Self-attention: Q=K=V=x\n",
    "output, attn = mha(x, x, x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention shape: {attn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "\n",
    "def visualize_attention_heads(attn_weights: torch.Tensor, num_heads_to_show: int = 4):\n",
    "    \"\"\"Visualize attention patterns from different heads\"\"\"\n",
    "    # attn_weights: (batch, num_heads, seq_len_q, seq_len_k)\n",
    "    attn = attn_weights[0].detach().cpu().numpy()  # First batch item\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_heads_to_show, figsize=(4*num_heads_to_show, 4))\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(attn[i], cmap='viridis', aspect='auto')\n",
    "        ax.set_xlabel('Key position')\n",
    "        ax.set_ylabel('Query position')\n",
    "        ax.set_title(f'Head {i}')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    plt.suptitle('Different heads learn different attention patterns')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_heads(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Positional Encoding\n",
    "\n",
    "Transformers have no inherent notion of position (unlike RNNs). We add **positional encodings** to give the model position information.\n",
    "\n",
    "The original Transformer uses sinusoidal positional encodings:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding from \"Attention Is All You Need\".\n",
    "    \n",
    "    Properties:\n",
    "    - Fixed (not learned)\n",
    "    - Can handle any sequence length\n",
    "    - PE(pos+k) can be represented as a linear function of PE(pos)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute the positional encodings in log space for numerical stability\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        \n",
    "        # Add batch dimension: (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of state)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            x + positional encoding\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Test\n",
    "pe = PositionalEncoding(d_model=256)\n",
    "x = torch.randn(4, 100, 256)\n",
    "output = pe(x)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encodings\n",
    "\n",
    "def visualize_positional_encodings(d_model: int = 128, max_len: int = 100):\n",
    "    \"\"\"Visualize the sinusoidal positional encodings\"\"\"\n",
    "    pe = PositionalEncoding(d_model, max_len, dropout=0)\n",
    "    \n",
    "    # Get the encodings\n",
    "    encodings = pe.pe[0, :max_len, :].numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Heatmap of all encodings\n",
    "    im = axes[0].imshow(encodings.T, aspect='auto', cmap='RdBu')\n",
    "    axes[0].set_xlabel('Position')\n",
    "    axes[0].set_ylabel('Dimension')\n",
    "    axes[0].set_title('Positional Encodings')\n",
    "    plt.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # Individual dimensions\n",
    "    for dim in [0, 1, 10, 50, 100]:\n",
    "        if dim < d_model:\n",
    "            axes[1].plot(encodings[:, dim], label=f'dim {dim}')\n",
    "    axes[1].set_xlabel('Position')\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].set_title('Individual Dimensions')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_positional_encodings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Learned positional embeddings\n",
    "\n",
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Learned positional embeddings (used in BERT, GPT)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        pos_embedding = self.embedding(positions)  # (seq_len, d_model)\n",
    "        return self.dropout(x + pos_embedding)\n",
    "\n",
    "\n",
    "# Comparison\n",
    "print(\"Sinusoidal PE: Fixed, can extrapolate to longer sequences\")\n",
    "print(\"Learned PE: More flexible, but limited to training length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Transformer Encoder\n",
    "\n",
    "The encoder consists of stacked encoder layers, each containing:\n",
    "1. Multi-head self-attention\n",
    "2. Position-wise feedforward network\n",
    "3. Residual connections and layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    FFN(x) = ReLU(x @ W_1 + b_1) @ W_2 + b_2\n",
    "    \n",
    "    Typically d_ff = 4 * d_model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Layer.\n",
    "    \n",
    "    Structure:\n",
    "        x -> LayerNorm -> MultiHeadAttention -> + -> LayerNorm -> FFN -> +\n",
    "             |__________________________________|    |_________________|\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of Transformer Encoder Layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, \n",
    "                 d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# Test\n",
    "encoder = TransformerEncoder(\n",
    "    num_layers=6,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "x = torch.randn(4, 50, 256)\n",
    "output = encoder(x)\n",
    "print(f\"Encoder input: {x.shape}\")\n",
    "print(f\"Encoder output: {output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Transformer Decoder\n",
    "\n",
    "The decoder is similar to the encoder but has:\n",
    "1. **Masked self-attention**: Prevents attending to future tokens\n",
    "2. **Cross-attention**: Attends to encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (look-ahead) mask for decoder self-attention.\n",
    "    \n",
    "    Prevents position i from attending to positions > i.\n",
    "    \n",
    "    Returns:\n",
    "        mask: (1, 1, seq_len, seq_len) where 1 = attend, 0 = mask\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    mask = mask == 0  # Invert: True where we can attend\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "# Visualize causal mask\n",
    "mask = create_causal_mask(10)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(mask[0, 0].float(), cmap='gray')\n",
    "plt.xlabel('Key position')\n",
    "plt.ylabel('Query position')\n",
    "plt.title('Causal Mask (white=attend, black=mask)')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Decoder Layer.\n",
    "    \n",
    "    Structure:\n",
    "        x -> LayerNorm -> MaskedSelfAttn -> + -> LayerNorm -> CrossAttn -> + -> LayerNorm -> FFN -> +\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        self_attn_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        # Masked self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, self_attn_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross-attention (attend to encoder output)\n",
    "        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, cross_attn_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of Transformer Decoder Layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int,\n",
    "                 d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        self_attn_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, self_attn_mask, cross_attn_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# Test decoder\n",
    "decoder = TransformerDecoder(\n",
    "    num_layers=6,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    d_ff=1024\n",
    ")\n",
    "\n",
    "tgt = torch.randn(4, 30, 256)  # Target sequence\n",
    "memory = torch.randn(4, 50, 256)  # Encoder output\n",
    "causal_mask = create_causal_mask(30)\n",
    "\n",
    "output = decoder(tgt, memory, causal_mask)\n",
    "print(f\"Target shape: {tgt.shape}\")\n",
    "print(f\"Encoder output shape: {memory.shape}\")\n",
    "print(f\"Decoder output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Complete Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer model for sequence-to-sequence tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        d_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        d_ff: int = 2048,\n",
    "        max_len: int = 5000,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Encoder and decoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_encoder_layers, d_model, num_heads, d_ff, dropout\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_decoder_layers, d_model, num_heads, d_ff, dropout\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with Xavier uniform\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def encode(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"Encode source sequence\"\"\"\n",
    "        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        return self.encoder(src_emb, src_mask)\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Decode target sequence\"\"\"\n",
    "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "        return self.decoder(tgt_emb, memory, tgt_mask)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        src_mask: Optional[torch.Tensor] = None,\n",
    "        tgt_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Source tokens (batch, src_len)\n",
    "            tgt: Target tokens (batch, tgt_len)\n",
    "            src_mask: Source padding mask\n",
    "            tgt_mask: Target causal + padding mask\n",
    "        \n",
    "        Returns:\n",
    "            Logits over target vocabulary (batch, tgt_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        memory = self.encode(src, src_mask)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output = self.decode(tgt, memory, tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        return self.output_proj(decoder_output)\n",
    "\n",
    "\n",
    "# Test complete Transformer\n",
    "transformer = Transformer(\n",
    "    src_vocab_size=10000,\n",
    "    tgt_vocab_size=10000,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3,\n",
    "    d_ff=1024\n",
    ")\n",
    "\n",
    "src = torch.randint(0, 10000, (4, 30))\n",
    "tgt = torch.randint(0, 10000, (4, 20))\n",
    "tgt_mask = create_causal_mask(20)\n",
    "\n",
    "output = transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "print(f\"Source shape: {src.shape}\")\n",
    "print(f\"Target shape: {tgt.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in transformer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. PyTorch's nn.Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch provides a built-in Transformer\n",
    "\n",
    "pytorch_transformer = nn.Transformer(\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.1,\n",
    "    batch_first=True  # PyTorch 1.9+\n",
    ")\n",
    "\n",
    "# Note: PyTorch Transformer expects already embedded inputs\n",
    "src = torch.randn(4, 30, 256)  # (batch, src_len, d_model)\n",
    "tgt = torch.randn(4, 20, 256)  # (batch, tgt_len, d_model)\n",
    "\n",
    "# Generate masks\n",
    "tgt_mask = nn.Transformer.generate_square_subsequent_mask(20)\n",
    "\n",
    "output = pytorch_transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "print(f\"PyTorch Transformer output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Decoder-Only Transformer (GPT-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    \"\"\"Single GPT-style decoder block (no cross-attention)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Pre-norm architecture (GPT-2 style)\n",
    "        attn_output, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "        x = x + attn_output\n",
    "        \n",
    "        ff_output = self.feed_forward(self.norm2(x))\n",
    "        x = x + ff_output\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-style decoder-only Transformer.\n",
    "    \n",
    "    Used for language modeling and text generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        num_layers: int = 12,\n",
    "        d_ff: int = 3072,\n",
    "        max_len: int = 1024,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.output_proj.weight = self.token_embedding.weight\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Token indices (batch, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Logits (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = torch.arange(seq_len, device=device)\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        \n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = create_causal_mask(seq_len).to(device)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Test GPT\n",
    "gpt = GPT(\n",
    "    vocab_size=50257,  # GPT-2 vocab size\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    d_ff=1024,\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "x = torch.randint(0, 50257, (4, 100))\n",
    "logits = gpt(x)\n",
    "print(f\"GPT input: {x.shape}\")\n",
    "print(f\"GPT output: {logits.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in gpt.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation with GPT\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, start_tokens: torch.Tensor, max_new_tokens: int = 50, \n",
    "             temperature: float = 1.0, top_k: int = 50) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate text autoregressively.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    x = start_tokens.to(device)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Get logits for last position\n",
    "        logits = model(x)[:, -1, :] / temperature\n",
    "        \n",
    "        # Top-k sampling\n",
    "        if top_k > 0:\n",
    "            values, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < values[:, -1:]] = float('-inf')\n",
    "        \n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "        \n",
    "        x = torch.cat([x, next_token], dim=1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "# Test generation\n",
    "start = torch.randint(0, 50257, (1, 5))\n",
    "generated = generate(gpt, start, max_new_tokens=20)\n",
    "print(f\"Generated sequence: {generated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Relative Positional Encoding\n",
    "\n",
    "Instead of absolute positions, encode relative distances between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Relative positional encoding\n",
    "\n",
    "class RelativePositionalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention with relative positional encodings.\n",
    "    \n",
    "    Instead of PE(i) + PE(j), we learn embeddings for relative positions (i - j).\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        max_relative_position: Maximum relative position to consider\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, max_relative_position: int = 32):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: Create learnable embeddings for relative positions from\n",
    "        # -max_relative_position to +max_relative_position\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: Compute attention scores including relative position bias\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Multi-Query Attention\n",
    "\n",
    "Multi-Query Attention (MQA) uses shared K and V across heads for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Multi-Query Attention\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Query Attention: each head has its own Q, but K and V are shared.\n",
    "    \n",
    "    Much faster during inference (smaller KV cache).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: W_q should project to d_model (for all heads)\n",
    "        # W_k and W_v should project to d_k (shared across heads)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Visualize Attention Patterns\n",
    "\n",
    "Create a function to visualize attention patterns for a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Attention visualization\n",
    "\n",
    "def visualize_attention_patterns(\n",
    "    model,\n",
    "    tokens: torch.Tensor,\n",
    "    layer_idx: int = 0,\n",
    "    head_idx: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns from a transformer model.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model\n",
    "        tokens: Input token indices (1, seq_len)\n",
    "        layer_idx: Which layer to visualize\n",
    "        head_idx: Which head (None = average across heads)\n",
    "    \n",
    "    Should:\n",
    "    1. Hook into the model to capture attention weights\n",
    "    2. Run forward pass\n",
    "    3. Visualize the attention matrix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Relative Positional Encoding\n",
    "\n",
    "class RelativePositionalAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, max_relative_position: int = 32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.max_relative_position = max_relative_position\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Relative position embeddings\n",
    "        # Range: [-max_relative_position, max_relative_position]\n",
    "        num_positions = 2 * max_relative_position + 1\n",
    "        self.relative_position_embedding = nn.Embedding(num_positions, self.d_k)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project Q, K, V\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute relative positions\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        relative_positions = positions.unsqueeze(0) - positions.unsqueeze(1)\n",
    "        relative_positions = relative_positions.clamp(-self.max_relative_position, self.max_relative_position)\n",
    "        relative_positions = relative_positions + self.max_relative_position  # Shift to [0, 2*max]\n",
    "        \n",
    "        # Get relative position embeddings\n",
    "        rel_pos_emb = self.relative_position_embedding(relative_positions)  # (seq, seq, d_k)\n",
    "        \n",
    "        # Content-based attention\n",
    "        content_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Position-based attention\n",
    "        # Q: (batch, heads, seq, d_k) @ rel_pos_emb: (seq, seq, d_k).T\n",
    "        position_scores = torch.einsum('bhqd,qkd->bhqk', Q, rel_pos_emb) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Combine\n",
    "        scores = content_scores + position_scores\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape and project\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(attn_output)\n",
    "\n",
    "\n",
    "# Test\n",
    "rel_attn = RelativePositionalAttention(256, 8, max_relative_position=16)\n",
    "x = torch.randn(4, 50, 256)\n",
    "output = rel_attn(x)\n",
    "print(f\"Relative attention output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Multi-Query Attention\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Each head has its own Q projection\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # K and V are shared (single head dimension)\n",
    "        self.W_k = nn.Linear(d_model, self.d_k)\n",
    "        self.W_v = nn.Linear(d_model, self.d_k)\n",
    "        \n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size, seq_len_q, _ = query.shape\n",
    "        seq_len_k = key.shape[1]\n",
    "        \n",
    "        # Project Q (per head), K, V (shared)\n",
    "        Q = self.W_q(query).view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key)  # (batch, seq_k, d_k)\n",
    "        V = self.W_v(value)  # (batch, seq_k, d_k)\n",
    "        \n",
    "        # Expand K, V for all heads\n",
    "        K = K.unsqueeze(1)  # (batch, 1, seq_k, d_k)\n",
    "        V = V.unsqueeze(1)  # (batch, 1, seq_k, d_k)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to V (broadcast across heads)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        \n",
    "        return self.W_o(attn_output), attn_weights\n",
    "\n",
    "\n",
    "# Compare parameter counts\n",
    "mha = MultiHeadAttention(256, 8)\n",
    "mqa = MultiQueryAttention(256, 8)\n",
    "\n",
    "print(f\"Multi-Head Attention params: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "print(f\"Multi-Query Attention params: {sum(p.numel() for p in mqa.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Attention visualization\n",
    "\n",
    "def visualize_attention_patterns(\n",
    "    model,\n",
    "    tokens: torch.Tensor,\n",
    "    token_labels: Optional[List[str]] = None,\n",
    "    layer_idx: int = 0,\n",
    "    head_idx: Optional[int] = None\n",
    "):\n",
    "    \"\"\"Visualize attention patterns\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    tokens = tokens.to(device)\n",
    "    \n",
    "    # Store attention weights\n",
    "    attention_weights = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # output is (attn_output, attn_weights)\n",
    "        if isinstance(output, tuple) and len(output) == 2:\n",
    "            attention_weights.append(output[1].detach().cpu())\n",
    "    \n",
    "    # Register hooks\n",
    "    hooks = []\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        if i == layer_idx:\n",
    "            hooks.append(block.attn.register_forward_hook(hook_fn))\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(tokens)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    if not attention_weights:\n",
    "        print(\"No attention weights captured\")\n",
    "        return\n",
    "    \n",
    "    attn = attention_weights[0][0]  # First batch item\n",
    "    seq_len = attn.shape[-1]\n",
    "    \n",
    "    if head_idx is not None:\n",
    "        # Single head\n",
    "        attn_to_show = attn[head_idx].numpy()\n",
    "        title = f'Layer {layer_idx}, Head {head_idx}'\n",
    "    else:\n",
    "        # Average across heads\n",
    "        attn_to_show = attn.mean(dim=0).numpy()\n",
    "        title = f'Layer {layer_idx}, Average across heads'\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attn_to_show, cmap='viridis')\n",
    "    plt.colorbar(label='Attention weight')\n",
    "    \n",
    "    if token_labels:\n",
    "        plt.xticks(range(len(token_labels)), token_labels, rotation=45, ha='right')\n",
    "        plt.yticks(range(len(token_labels)), token_labels)\n",
    "    \n",
    "    plt.xlabel('Key position')\n",
    "    plt.ylabel('Query position')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Test with our GPT model\n",
    "# Note: Need to modify GPT to return attention weights for this to work properly\n",
    "print(\"Attention visualization function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Attention Mechanism**:\n",
    "   - Query, Key, Value paradigm\n",
    "   - Scaled dot-product prevents softmax saturation\n",
    "   - Allows attending to all positions in parallel\n",
    "\n",
    "2. **Multi-Head Attention**:\n",
    "   - Multiple attention patterns learned simultaneously\n",
    "   - Each head can focus on different aspects\n",
    "   - Typical: 8-16 heads\n",
    "\n",
    "3. **Positional Encoding**:\n",
    "   - Transformers have no built-in position sense\n",
    "   - Sinusoidal: fixed, can extrapolate\n",
    "   - Learned: more flexible, fixed max length\n",
    "\n",
    "4. **Transformer Architecture**:\n",
    "   - Encoder: self-attention + FFN\n",
    "   - Decoder: masked self-attention + cross-attention + FFN\n",
    "   - Residual connections and layer norm throughout\n",
    "\n",
    "5. **Variants**:\n",
    "   - Encoder-only (BERT): bidirectional, for understanding\n",
    "   - Decoder-only (GPT): causal, for generation\n",
    "   - Encoder-decoder: for seq2seq tasks\n",
    "\n",
    "### Complexity\n",
    "\n",
    "| Operation | Time Complexity | Space Complexity |\n",
    "|-----------|-----------------|------------------|\n",
    "| Self-attention | O(n  d) | O(n) |\n",
    "| FFN | O(n  d  d_ff) | O(n  d_ff) |\n",
    "\n",
    "Where n = sequence length, d = model dimension"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
