{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 12: Variational Autoencoders (VAEs)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand autoencoders and their limitations for generation\n",
    "2. Master VAE theory: ELBO, KL divergence, and the reparameterization trick\n",
    "3. Implement VAEs from scratch and train on MNIST\n",
    "4. Explore the latent space and generate new samples\n",
    "\n",
    "**Prerequisites**: Notebooks 01-04 (PyTorch fundamentals, training loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Autoencoders: Compression and Reconstruction\n",
    "\n",
    "An **autoencoder** learns to compress data into a lower-dimensional representation (encoding) and reconstruct it back (decoding).\n",
    "\n",
    "```\n",
    "Input x → [Encoder] → Latent z → [Decoder] → Reconstruction x̂\n",
    "```\n",
    "\n",
    "The bottleneck forces the network to learn meaningful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Simple autoencoder for MNIST (28x28 = 784 dimensions).\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: 784 -> 256 -> 128 -> latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder: latent_dim -> 128 -> 256 -> 784\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 784),\n",
    "            nn.Sigmoid()  # Output in [0, 1] for pixel values\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x.view(-1, 784))\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z).view(-1, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, train_loader, epochs=10, lr=1e-3):\n",
    "    \"\"\"Train autoencoder with MSE reconstruction loss.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            \n",
    "            x_recon, _ = model(x)\n",
    "            loss = F.mse_loss(x_recon, x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train\n",
    "ae = Autoencoder(latent_dim=32).to(device)\n",
    "ae = train_autoencoder(ae, train_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model, test_loader, n=8):\n",
    "    \"\"\"Show original images and their reconstructions.\"\"\"\n",
    "    model.eval()\n",
    "    x, _ = next(iter(test_loader))\n",
    "    x = x[:n].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x_recon, _ = model(x)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n, figsize=(n*1.5, 3))\n",
    "    for i in range(n):\n",
    "        axes[0, i].imshow(x[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(x_recon[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "    axes[0, 0].set_ylabel('Original')\n",
    "    axes[1, 0].set_ylabel('Reconstructed')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_reconstructions(ae, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem with Standard Autoencoders for Generation\n",
    "\n",
    "Autoencoders learn to reconstruct, but their latent space is **not structured for generation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space_2d(model, test_loader):\n",
    "    \"\"\"Visualize 2D latent space colored by digit class.\"\"\"\n",
    "    model.eval()\n",
    "    latents, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            _, z = model(x.to(device))\n",
    "            latents.append(z[:, :2].cpu())  # Take first 2 dimensions\n",
    "            labels.append(y)\n",
    "    \n",
    "    latents = torch.cat(latents).numpy()\n",
    "    labels = torch.cat(labels).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='tab10', alpha=0.5, s=1)\n",
    "    plt.colorbar(scatter, label='Digit')\n",
    "    plt.xlabel('Latent dim 1')\n",
    "    plt.ylabel('Latent dim 2')\n",
    "    plt.title('Autoencoder Latent Space (first 2 dims)')\n",
    "    plt.show()\n",
    "\n",
    "# Train a 2D autoencoder for visualization\n",
    "ae_2d = Autoencoder(latent_dim=2).to(device)\n",
    "ae_2d = train_autoencoder(ae_2d, train_loader, epochs=15)\n",
    "visualize_latent_space_2d(ae_2d, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try generating from random latent points\n",
    "ae_2d.eval()\n",
    "with torch.no_grad():\n",
    "    # Random points - may hit \"holes\" in latent space\n",
    "    random_z = torch.randn(8, 2).to(device) * 3\n",
    "    generated = ae_2d.decode(random_z)\n",
    "\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 1.5))\n",
    "for i in range(8):\n",
    "    axes[i].imshow(generated[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Generated from random latent points (often poor quality)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key issues with autoencoders for generation:**\n",
    "1. **Holes in latent space**: Random samples may decode to nonsense\n",
    "2. **No probabilistic interpretation**: Can't sample meaningfully\n",
    "3. **Discontinuous**: Similar latent points may decode to very different outputs\n",
    "\n",
    "---\n",
    "## 2. VAE Theory: Learning a Probabilistic Latent Space\n",
    "\n",
    "VAEs fix these issues by:\n",
    "1. **Encoding to a distribution** (mean μ and variance σ²), not a point\n",
    "2. **Regularizing** the latent space to be close to a standard normal N(0, I)\n",
    "\n",
    "### The VAE Objective: Evidence Lower Bound (ELBO)\n",
    "\n",
    "$$\\mathcal{L} = \\underbrace{\\mathbb{E}_{q(z|x)}[\\log p(x|z)]}_{\\text{Reconstruction}} - \\underbrace{D_{KL}(q(z|x) \\| p(z))}_{\\text{Regularization}}$$\n",
    "\n",
    "Where:\n",
    "- $q(z|x)$ = encoder (approximate posterior): $\\mathcal{N}(\\mu(x), \\sigma^2(x))$\n",
    "- $p(x|z)$ = decoder (likelihood)\n",
    "- $p(z)$ = prior: $\\mathcal{N}(0, I)$\n",
    "\n",
    "### KL Divergence for Gaussians (Closed Form)\n",
    "\n",
    "For $q = \\mathcal{N}(\\mu, \\sigma^2)$ and $p = \\mathcal{N}(0, 1)$:\n",
    "\n",
    "$$D_{KL}(q \\| p) = -\\frac{1}{2} \\sum_{j=1}^{J} (1 + \\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu, logvar):\n",
    "    \"\"\"\n",
    "    KL divergence between N(mu, sigma^2) and N(0, 1).\n",
    "    \n",
    "    Args:\n",
    "        mu: Mean of approximate posterior [batch, latent_dim]\n",
    "        logvar: Log variance (more numerically stable than variance)\n",
    "    \n",
    "    Returns:\n",
    "        KL divergence summed over latent dimensions, averaged over batch\n",
    "    \"\"\"\n",
    "    # KL = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    return kl.mean()\n",
    "\n",
    "# Example: KL divergence increases as we move away from N(0,1)\n",
    "mu_test = torch.tensor([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]])\n",
    "logvar_test = torch.zeros_like(mu_test)\n",
    "\n",
    "for i, m in enumerate(mu_test):\n",
    "    kl = kl_divergence(m.unsqueeze(0), logvar_test[i].unsqueeze(0))\n",
    "    print(f\"μ = {m.tolist()}, KL = {kl.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reparameterization Trick\n",
    "\n",
    "**Problem**: Sampling $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is not differentiable.\n",
    "\n",
    "**Solution**: Reparameterize as $z = \\mu + \\sigma \\cdot \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "This moves the randomness outside the computational graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    \"\"\"\n",
    "    Reparameterization trick: z = mu + std * epsilon\n",
    "    \n",
    "    Args:\n",
    "        mu: Mean [batch, latent_dim]\n",
    "        logvar: Log variance [batch, latent_dim]\n",
    "    \n",
    "    Returns:\n",
    "        Sampled z with gradients flowing through mu and logvar\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)  # sigma = exp(0.5 * log(sigma^2))\n",
    "    eps = torch.randn_like(std)    # epsilon ~ N(0, 1)\n",
    "    return mu + std * eps\n",
    "\n",
    "# Verify gradients flow through\n",
    "mu = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "logvar = torch.tensor([[0.0, 0.0]], requires_grad=True)\n",
    "\n",
    "z = reparameterize(mu, logvar)\n",
    "loss = z.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"z = {z.detach()}\")\n",
    "print(f\"∂loss/∂μ = {mu.grad}\")  # Should be [1, 1]\n",
    "print(f\"∂loss/∂logvar exists: {logvar.grad is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. VAE Implementation\n",
    "\n",
    "Now let's implement a complete VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"Variational Autoencoder for MNIST.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder outputs mu and logvar (2 * latent_dim)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent distribution parameters.\"\"\"\n",
    "        h = self.encoder(x.view(-1, 784))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Sample from latent distribution using reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent vector to reconstruction.\"\"\"\n",
    "        return self.decoder(z).view(-1, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Test\n",
    "vae = VAE(latent_dim=20).to(device)\n",
    "x_test = torch.randn(4, 1, 28, 28).to(device)\n",
    "x_recon, mu, logvar = vae(x_test)\n",
    "print(f\"Input: {x_test.shape}, Output: {x_recon.shape}\")\n",
    "print(f\"μ: {mu.shape}, logvar: {logvar.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_recon, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    VAE loss = Reconstruction + β * KL divergence\n",
    "    \n",
    "    Args:\n",
    "        x_recon: Reconstructed images\n",
    "        x: Original images\n",
    "        mu, logvar: Latent distribution parameters\n",
    "        beta: Weight for KL term (beta=1 is standard VAE)\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (binary cross-entropy for normalized pixels)\n",
    "    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum') / x.size(0)\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    \n",
    "    return recon_loss + beta * kl_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, train_loader, epochs=20, lr=1e-3, beta=1.0):\n",
    "    \"\"\"Train VAE with ELBO objective.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    history = {'loss': [], 'recon': [], 'kl': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss, total_recon, total_kl = 0, 0, 0\n",
    "        \n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            \n",
    "            x_recon, mu, logvar = model(x)\n",
    "            loss, recon, kl = vae_loss(x_recon, x, mu, logvar, beta)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon += recon.item()\n",
    "            total_kl += kl.item()\n",
    "        \n",
    "        n = len(train_loader)\n",
    "        history['loss'].append(total_loss/n)\n",
    "        history['recon'].append(total_recon/n)\n",
    "        history['kl'].append(total_kl/n)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/n:.1f}, \"\n",
    "                  f\"Recon: {total_recon/n:.1f}, KL: {total_kl/n:.1f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train VAE\n",
    "vae = VAE(latent_dim=20).to(device)\n",
    "vae, history = train_vae(vae, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "axes[0].plot(history['loss'])\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[1].plot(history['recon'])\n",
    "axes[1].set_title('Reconstruction Loss')\n",
    "axes[2].plot(history['kl'])\n",
    "axes[2].set_title('KL Divergence')\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Epoch')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploring the VAE\n",
    "\n",
    "### Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vae_reconstructions(model, test_loader, n=8):\n",
    "    \"\"\"Show reconstructions with the encoded distribution info.\"\"\"\n",
    "    model.eval()\n",
    "    x, _ = next(iter(test_loader))\n",
    "    x = x[:n].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x_recon, mu, logvar = model(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n, figsize=(n*1.5, 3))\n",
    "    for i in range(n):\n",
    "        axes[0, i].imshow(x[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(x_recon[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "    axes[0, 0].set_ylabel('Original')\n",
    "    axes[1, 0].set_ylabel('Reconstructed')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_vae_reconstructions(vae, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation: Sampling from the Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, n_samples=16):\n",
    "    \"\"\"Generate new samples by sampling from prior N(0, I).\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample from prior\n",
    "        z = torch.randn(n_samples, model.latent_dim).to(device)\n",
    "        samples = model.decode(z)\n",
    "    \n",
    "    # Display\n",
    "    n_rows = int(np.sqrt(n_samples))\n",
    "    fig, axes = plt.subplots(n_rows, n_rows, figsize=(n_rows*1.5, n_rows*1.5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(samples[i].cpu().squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle('Generated Samples (z ~ N(0, I))')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_samples(vae, n_samples=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a 2D VAE for visualization\n",
    "vae_2d = VAE(latent_dim=2).to(device)\n",
    "vae_2d, _ = train_vae(vae_2d, train_loader, epochs=30)\n",
    "\n",
    "# Visualize latent space\n",
    "vae_2d.eval()\n",
    "latents, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        mu, _ = vae_2d.encode(x.to(device))\n",
    "        latents.append(mu.cpu())\n",
    "        labels.append(y)\n",
    "\n",
    "latents = torch.cat(latents).numpy()\n",
    "labels = torch.cat(labels).numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='tab10', alpha=0.5, s=1)\n",
    "plt.colorbar(scatter, label='Digit')\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('VAE Latent Space (2D)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_latent(model, x1, x2, steps=10):\n",
    "    \"\"\"Interpolate between two images in latent space.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mu1, _ = model.encode(x1)\n",
    "        mu2, _ = model.encode(x2)\n",
    "        \n",
    "        # Linear interpolation in latent space\n",
    "        interpolations = []\n",
    "        for alpha in np.linspace(0, 1, steps):\n",
    "            z = (1 - alpha) * mu1 + alpha * mu2\n",
    "            img = model.decode(z)\n",
    "            interpolations.append(img)\n",
    "    \n",
    "    return torch.cat(interpolations)\n",
    "\n",
    "# Get two different digits\n",
    "test_batch, test_labels = next(iter(test_loader))\n",
    "idx1 = (test_labels == 3).nonzero()[0].item()\n",
    "idx2 = (test_labels == 8).nonzero()[0].item()\n",
    "\n",
    "x1 = test_batch[idx1:idx1+1].to(device)\n",
    "x2 = test_batch[idx2:idx2+1].to(device)\n",
    "\n",
    "interp = interpolate_latent(vae, x1, x2, steps=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 1.5))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(interp[i].cpu().squeeze(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Latent Space Interpolation (3 → 8)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space Traversal (2D Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_grid(model, n=15, range_val=3):\n",
    "    \"\"\"Generate images by traversing 2D latent space on a grid.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create grid of latent values\n",
    "    grid_x = np.linspace(-range_val, range_val, n)\n",
    "    grid_y = np.linspace(-range_val, range_val, n)[::-1]\n",
    "    \n",
    "    figure = np.zeros((28 * n, 28 * n))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, yi in enumerate(grid_y):\n",
    "            for j, xi in enumerate(grid_x):\n",
    "                z = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
    "                img = model.decode(z).cpu().squeeze().numpy()\n",
    "                figure[i*28:(i+1)*28, j*28:(j+1)*28] = img\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap='gray')\n",
    "    plt.xlabel('z₁')\n",
    "    plt.ylabel('z₂')\n",
    "    plt.title('Latent Space Traversal')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "plot_latent_grid(vae_2d, n=15, range_val=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Convolutional VAE\n",
    "\n",
    "For better image quality, we use convolutional layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    \"\"\"Convolutional VAE for MNIST.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: 1x28x28 -> 32x14x14 -> 64x7x7 -> flatten -> latent\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),  # -> 32x14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # -> 64x7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        \n",
    "        # Decoder: latent -> 64x7x7 -> 32x14x14 -> 1x28x28\n",
    "        self.fc_decode = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # -> 32x14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),   # -> 1x28x28\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = self.fc_decode(z).view(-1, 64, 7, 7)\n",
    "        return self.decoder(h)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Train ConvVAE\n",
    "conv_vae = ConvVAE(latent_dim=20).to(device)\n",
    "conv_vae, conv_history = train_vae(conv_vae, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MLP VAE vs Conv VAE\n",
    "fig, axes = plt.subplots(3, 8, figsize=(12, 4.5))\n",
    "\n",
    "x, _ = next(iter(test_loader))\n",
    "x = x[:8].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mlp_recon, _, _ = vae(x)\n",
    "    conv_recon, _, _ = conv_vae(x)\n",
    "\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(x[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(mlp_recon[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    axes[2, i].imshow(conv_recon[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original')\n",
    "axes[1, 0].set_ylabel('MLP VAE')\n",
    "axes[2, 0].set_ylabel('Conv VAE')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from ConvVAE\n",
    "generate_samples(conv_vae, n_samples=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. β-VAE: Disentangled Representations\n",
    "\n",
    "**β-VAE** uses β > 1 to encourage disentangled latent representations where each dimension captures independent factors of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with different beta values\n",
    "betas = [0.1, 1.0, 4.0]\n",
    "models = {}\n",
    "\n",
    "for beta in betas:\n",
    "    print(f\"\\nTraining with β = {beta}\")\n",
    "    model = VAE(latent_dim=10).to(device)\n",
    "    model, _ = train_vae(model, train_loader, epochs=15, beta=beta)\n",
    "    models[beta] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare reconstructions and generations\n",
    "fig, axes = plt.subplots(len(betas) + 1, 8, figsize=(12, 6))\n",
    "\n",
    "x, _ = next(iter(test_loader))\n",
    "x = x[:8].to(device)\n",
    "\n",
    "# Original\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(x[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "axes[0, 0].set_ylabel('Original')\n",
    "\n",
    "# Reconstructions for each beta\n",
    "for row, beta in enumerate(betas, 1):\n",
    "    with torch.no_grad():\n",
    "        recon, _, _ = models[beta](x)\n",
    "    for i in range(8):\n",
    "        axes[row, i].imshow(recon[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[row, i].axis('off')\n",
    "    axes[row, 0].set_ylabel(f'β={beta}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trade-off**: Higher β = more regularized latent space (better disentanglement) but worse reconstruction quality.\n",
    "\n",
    "---\n",
    "## 7. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Autoencoder** | Encoder-decoder that learns compressed representations |\n",
    "| **VAE** | Probabilistic autoencoder with regularized latent space |\n",
    "| **ELBO** | Evidence Lower Bound = Reconstruction - KL divergence |\n",
    "| **Reparameterization** | z = μ + σ·ε enables backprop through sampling |\n",
    "| **KL Divergence** | Measures how far q(z|x) is from prior p(z) |\n",
    "| **β-VAE** | β > 1 encourages disentangled representations |\n",
    "\n",
    "### VAE vs Autoencoder\n",
    "\n",
    "| Aspect | Autoencoder | VAE |\n",
    "|--------|-------------|-----|\n",
    "| Latent | Deterministic point | Distribution (μ, σ²) |\n",
    "| Generation | Poor (holes in latent space) | Good (structured space) |\n",
    "| Interpolation | May be discontinuous | Smooth |\n",
    "| Loss | Reconstruction only | Reconstruction + KL |\n",
    "\n",
    "### When to Use VAEs\n",
    "\n",
    "- **Good for**: Smooth latent spaces, interpolation, controllable generation\n",
    "- **Limitations**: Blurry outputs (due to Gaussian assumption), mode averaging\n",
    "- **Alternatives**: GANs for sharper images, Diffusion models for highest quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Conditional VAE (CVAE)\n",
    "Implement a Conditional VAE that can generate specific digits by conditioning on the class label.\n",
    "\n",
    "### Exercise 2: Latent Dimension Analysis\n",
    "Train VAEs with different latent dimensions (2, 10, 50, 100) and analyze the trade-off between reconstruction quality and generation diversity.\n",
    "\n",
    "### Exercise 3: VAE for Fashion-MNIST\n",
    "Adapt the ConvVAE to Fashion-MNIST and visualize the latent space - are clothing categories separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Conditional VAE\n",
    "class CVAE(nn.Module):\n",
    "    \"\"\"Conditional VAE - conditions on class label.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=20, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Encoder: input + one-hot label\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784 + num_classes, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "        \n",
    "        # Decoder: latent + one-hot label\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x, y):\n",
    "        y_onehot = F.one_hot(y, self.num_classes).float()\n",
    "        h = torch.cat([x.view(-1, 784), y_onehot], dim=1)\n",
    "        h = self.encoder(h)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        return mu + std * torch.randn_like(std)\n",
    "    \n",
    "    def decode(self, z, y):\n",
    "        y_onehot = F.one_hot(y, self.num_classes).float()\n",
    "        h = torch.cat([z, y_onehot], dim=1)\n",
    "        return self.decoder(h).view(-1, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        mu, logvar = self.encode(x, y)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, y), mu, logvar\n",
    "\n",
    "# Train CVAE\n",
    "cvae = CVAE(latent_dim=20).to(device)\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x_recon, mu, logvar = cvae(x, y)\n",
    "        loss, _, _ = vae_loss(x_recon, x, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.1f}\")\n",
    "\n",
    "# Generate specific digits\n",
    "cvae.eval()\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "with torch.no_grad():\n",
    "    for digit in range(10):\n",
    "        z = torch.randn(2, 20).to(device)\n",
    "        y = torch.tensor([digit, digit]).to(device)\n",
    "        samples = cvae.decode(z, y)\n",
    "        for row in range(2):\n",
    "            axes[row, digit].imshow(samples[row].cpu().squeeze(), cmap='gray')\n",
    "            axes[row, digit].axis('off')\n",
    "        axes[0, digit].set_title(str(digit))\n",
    "plt.suptitle('CVAE: Conditional Generation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Latent Dimension Analysis\n",
    "latent_dims = [2, 10, 50, 100]\n",
    "results = {}\n",
    "\n",
    "for dim in latent_dims:\n",
    "    print(f\"\\nTraining VAE with latent_dim={dim}\")\n",
    "    model = VAE(latent_dim=dim).to(device)\n",
    "    model, history = train_vae(model, train_loader, epochs=10)\n",
    "    \n",
    "    # Evaluate reconstruction\n",
    "    model.eval()\n",
    "    total_recon = 0\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            x_recon, mu, logvar = model(x)\n",
    "            total_recon += F.mse_loss(x_recon, x).item()\n",
    "    \n",
    "    results[dim] = {\n",
    "        'model': model,\n",
    "        'recon_mse': total_recon / len(test_loader),\n",
    "        'final_kl': history['kl'][-1]\n",
    "    }\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "dims = list(results.keys())\n",
    "recons = [results[d]['recon_mse'] for d in dims]\n",
    "kls = [results[d]['final_kl'] for d in dims]\n",
    "\n",
    "axes[0].bar(range(len(dims)), recons)\n",
    "axes[0].set_xticks(range(len(dims)))\n",
    "axes[0].set_xticklabels(dims)\n",
    "axes[0].set_xlabel('Latent Dimension')\n",
    "axes[0].set_ylabel('Reconstruction MSE')\n",
    "axes[0].set_title('Reconstruction Quality')\n",
    "\n",
    "axes[1].bar(range(len(dims)), kls)\n",
    "axes[1].set_xticks(range(len(dims)))\n",
    "axes[1].set_xticklabels(dims)\n",
    "axes[1].set_xlabel('Latent Dimension')\n",
    "axes[1].set_ylabel('KL Divergence')\n",
    "axes[1].set_title('Latent Space Regularization')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- Higher latent dim → better reconstruction (more capacity)\")\n",
    "print(\"- Higher latent dim → higher KL (more dimensions to regularize)\")\n",
    "print(\"- Sweet spot depends on data complexity (MNIST: 10-20 often sufficient)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Fashion-MNIST VAE\n",
    "fashion_train = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
    "fashion_test = datasets.FashionMNIST('./data', train=False, transform=transform)\n",
    "fashion_train_loader = DataLoader(fashion_train, batch_size=128, shuffle=True)\n",
    "fashion_test_loader = DataLoader(fashion_test, batch_size=128)\n",
    "\n",
    "fashion_classes = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Boot']\n",
    "\n",
    "# Train ConvVAE on Fashion-MNIST\n",
    "fashion_vae = ConvVAE(latent_dim=20).to(device)\n",
    "fashion_vae, _ = train_vae(fashion_vae, fashion_train_loader, epochs=20)\n",
    "\n",
    "# Visualize latent space with t-SNE (since 20D)\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "fashion_vae.eval()\n",
    "latents, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in fashion_test_loader:\n",
    "        mu, _ = fashion_vae.encode(x.to(device))\n",
    "        latents.append(mu.cpu())\n",
    "        labels.append(y)\n",
    "        if len(latents) * 128 >= 5000:\n",
    "            break\n",
    "\n",
    "latents = torch.cat(latents).numpy()[:5000]\n",
    "labels = torch.cat(labels).numpy()[:5000]\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "latents_2d = tsne.fit_transform(latents)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(10):\n",
    "    mask = labels == i\n",
    "    plt.scatter(latents_2d[mask, 0], latents_2d[mask, 1], \n",
    "                label=fashion_classes[i], alpha=0.5, s=10)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title('Fashion-MNIST VAE Latent Space (t-SNE)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate samples\n",
    "generate_samples(fashion_vae, n_samples=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
