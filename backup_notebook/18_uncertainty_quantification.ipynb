{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 18: Uncertainty Quantification in Deep Learning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand aleatoric vs epistemic uncertainty\n",
    "2. Implement MC Dropout for uncertainty estimation\n",
    "3. Build and use Deep Ensembles\n",
    "4. Create heteroscedastic networks that predict variance\n",
    "5. Understand Bayesian Neural Networks basics\n",
    "6. Evaluate model calibration\n",
    "\n",
    "**Prerequisites**: Notebooks 01-04 (PyTorch fundamentals, training loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why Uncertainty Matters\n",
    "\n",
    "Standard neural networks output point predictions without indicating confidence. This is problematic:\n",
    "\n",
    "- **Safety-critical applications**: Medical diagnosis, autonomous vehicles\n",
    "- **Out-of-distribution (OOD) detection**: Knowing when inputs are unlike training data\n",
    "- **Active learning**: Selecting informative samples to label\n",
    "- **Decision making**: When to defer to humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: overconfident predictions\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x.view(-1, 784))\n",
    "\n",
    "# Load MNIST\n",
    "transform = transforms.ToTensor()\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST('./data', train=False, transform=transform)\n",
    "train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=128)\n",
    "\n",
    "# Quick training\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        loss = F.cross_entropy(model(x), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Model trained on MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on MNIST (in-distribution) vs random noise (OOD)\n",
    "model.eval()\n",
    "\n",
    "# In-distribution\n",
    "x_id, _ = next(iter(test_loader))\n",
    "x_id = x_id[:5].to(device)\n",
    "\n",
    "# Out-of-distribution: random noise\n",
    "x_ood = torch.rand(5, 1, 28, 28).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    probs_id = F.softmax(model(x_id), dim=1)\n",
    "    probs_ood = F.softmax(model(x_ood), dim=1)\n",
    "\n",
    "print(\"In-distribution (MNIST) - Max confidence:\")\n",
    "print(probs_id.max(dim=1).values.cpu().numpy())\n",
    "\n",
    "print(\"\\nOut-of-distribution (noise) - Max confidence:\")\n",
    "print(probs_ood.max(dim=1).values.cpu().numpy())\n",
    "print(\"\\n→ The model is equally confident on random noise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Types of Uncertainty\n",
    "\n",
    "### Aleatoric Uncertainty (Data Uncertainty)\n",
    "- Inherent noise in the data\n",
    "- **Irreducible** - more data won't help\n",
    "- Example: Noisy sensor measurements, ambiguous labels\n",
    "\n",
    "### Epistemic Uncertainty (Model Uncertainty)\n",
    "- Uncertainty due to limited knowledge/data\n",
    "- **Reducible** - more data helps\n",
    "- Example: Predictions far from training distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both types with a regression example\n",
    "def generate_regression_data(n=100):\n",
    "    \"\"\"Generate data with heteroscedastic noise (varying aleatoric uncertainty).\"\"\"\n",
    "    x = np.linspace(-3, 3, n)\n",
    "    # Noise increases with x (heteroscedastic)\n",
    "    noise_std = 0.1 + 0.3 * np.abs(x)\n",
    "    y = np.sin(x) + np.random.randn(n) * noise_std\n",
    "    return x.astype(np.float32), y.astype(np.float32), noise_std\n",
    "\n",
    "x_train, y_train, true_noise = generate_regression_data(100)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(x_train, y_train, alpha=0.5, label='Data')\n",
    "plt.plot(x_train, np.sin(x_train), 'r-', label='True function', linewidth=2)\n",
    "plt.fill_between(x_train, np.sin(x_train) - 2*true_noise, \n",
    "                 np.sin(x_train) + 2*true_noise, alpha=0.2, color='red',\n",
    "                 label='Aleatoric uncertainty (±2σ)')\n",
    "plt.axvspan(3, 5, alpha=0.1, color='gray', label='No training data (epistemic)')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Aleatoric vs Epistemic Uncertainty')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. MC Dropout: Simple Uncertainty Estimation\n",
    "\n",
    "**Key insight**: Dropout at test time approximates Bayesian inference!\n",
    "\n",
    "Instead of disabling dropout during inference:\n",
    "1. Keep dropout **enabled**\n",
    "2. Run multiple forward passes\n",
    "3. Use variance of predictions as uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropoutNet(nn.Module):\n",
    "    \"\"\"Network with dropout that stays on during inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_p=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        x = F.relu(self.dropout(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def predict_with_uncertainty(self, x, n_samples=50):\n",
    "        \"\"\"Run multiple forward passes with dropout enabled.\"\"\"\n",
    "        self.train()  # Keep dropout active\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(n_samples):\n",
    "                logits = self(x)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                predictions.append(probs)\n",
    "        \n",
    "        predictions = torch.stack(predictions)  # [n_samples, batch, classes]\n",
    "        \n",
    "        # Mean prediction\n",
    "        mean_pred = predictions.mean(dim=0)\n",
    "        \n",
    "        # Uncertainty: predictive entropy or variance\n",
    "        variance = predictions.var(dim=0)  # Variance per class\n",
    "        entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)\n",
    "        \n",
    "        return mean_pred, entropy, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MC Dropout model\n",
    "mc_model = MCDropoutNet(dropout_p=0.3).to(device)\n",
    "optimizer = torch.optim.Adam(mc_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    mc_model.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        loss = F.cross_entropy(mc_model(x), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"MC Dropout model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare uncertainty on ID vs OOD\n",
    "mean_id, entropy_id, _ = mc_model.predict_with_uncertainty(x_id, n_samples=50)\n",
    "mean_ood, entropy_ood, _ = mc_model.predict_with_uncertainty(x_ood, n_samples=50)\n",
    "\n",
    "print(\"MC Dropout Uncertainty (entropy):\")\n",
    "print(f\"In-distribution:  {entropy_id.cpu().numpy()}\")\n",
    "print(f\"Out-of-distribution: {entropy_ood.cpu().numpy()}\")\n",
    "print(f\"\\n→ Higher entropy on OOD data indicates uncertainty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MC Dropout predictions\n",
    "def visualize_mc_dropout(model, x, n_samples=50):\n",
    "    model.train()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            probs = F.softmax(model(x), dim=1)\n",
    "            predictions.append(probs[0].cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.boxplot(predictions, labels=range(10))\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('MC Dropout: Distribution of predictions over 50 samples')\n",
    "    plt.show()\n",
    "\n",
    "print(\"ID sample:\")\n",
    "visualize_mc_dropout(mc_model, x_id[0:1])\n",
    "\n",
    "print(\"OOD sample (noise):\")\n",
    "visualize_mc_dropout(mc_model, x_ood[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deep Ensembles\n",
    "\n",
    "Train multiple models independently, aggregate their predictions.\n",
    "\n",
    "**Advantages over MC Dropout:**\n",
    "- Often better uncertainty estimates\n",
    "- Each model sees full capacity (no dropout)\n",
    "- Embarrassingly parallel training\n",
    "\n",
    "**Disadvantages:**\n",
    "- N× training cost\n",
    "- N× memory for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepEnsemble:\n",
    "    \"\"\"Ensemble of independently trained models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, n_models=5, **model_kwargs):\n",
    "        self.models = [model_class(**model_kwargs) for _ in range(n_models)]\n",
    "        self.n_models = n_models\n",
    "    \n",
    "    def to(self, device):\n",
    "        for model in self.models:\n",
    "            model.to(device)\n",
    "        return self\n",
    "    \n",
    "    def train_all(self, train_loader, epochs=10, lr=1e-3):\n",
    "        \"\"\"Train each model independently.\"\"\"\n",
    "        for i, model in enumerate(self.models):\n",
    "            print(f\"Training model {i+1}/{self.n_models}\")\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                for x, y in train_loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    loss = F.cross_entropy(model(x), y)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "    \n",
    "    def predict_with_uncertainty(self, x):\n",
    "        \"\"\"Aggregate predictions from all models.\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                probs = F.softmax(model(x), dim=1)\n",
    "                predictions.append(probs)\n",
    "        \n",
    "        predictions = torch.stack(predictions)  # [n_models, batch, classes]\n",
    "        \n",
    "        mean_pred = predictions.mean(dim=0)\n",
    "        variance = predictions.var(dim=0)\n",
    "        entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)\n",
    "        \n",
    "        return mean_pred, entropy, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble\n",
    "ensemble = DeepEnsemble(SimpleNet, n_models=5).to(device)\n",
    "ensemble.train_all(train_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ensemble uncertainty\n",
    "mean_id_ens, entropy_id_ens, _ = ensemble.predict_with_uncertainty(x_id)\n",
    "mean_ood_ens, entropy_ood_ens, _ = ensemble.predict_with_uncertainty(x_ood)\n",
    "\n",
    "print(\"Deep Ensemble Uncertainty (entropy):\")\n",
    "print(f\"In-distribution:  {entropy_id_ens.cpu().numpy()}\")\n",
    "print(f\"Out-of-distribution: {entropy_ood_ens.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MC Dropout vs Ensemble on OOD detection\n",
    "def evaluate_ood_detection(id_entropy, ood_entropy):\n",
    "    \"\"\"Simple evaluation: can we separate ID from OOD using entropy?\"\"\"\n",
    "    id_vals = id_entropy.cpu().numpy()\n",
    "    ood_vals = ood_entropy.cpu().numpy()\n",
    "    \n",
    "    threshold = (id_vals.mean() + ood_vals.mean()) / 2\n",
    "    \n",
    "    id_correct = (id_vals < threshold).mean()\n",
    "    ood_correct = (ood_vals >= threshold).mean()\n",
    "    \n",
    "    return (id_correct + ood_correct) / 2\n",
    "\n",
    "# Evaluate on larger set\n",
    "x_id_large = next(iter(test_loader))[0][:100].to(device)\n",
    "x_ood_large = torch.rand(100, 1, 28, 28).to(device)\n",
    "\n",
    "_, ent_id_mc, _ = mc_model.predict_with_uncertainty(x_id_large, n_samples=30)\n",
    "_, ent_ood_mc, _ = mc_model.predict_with_uncertainty(x_ood_large, n_samples=30)\n",
    "\n",
    "_, ent_id_ens, _ = ensemble.predict_with_uncertainty(x_id_large)\n",
    "_, ent_ood_ens, _ = ensemble.predict_with_uncertainty(x_ood_large)\n",
    "\n",
    "print(f\"OOD Detection Accuracy:\")\n",
    "print(f\"MC Dropout: {evaluate_ood_detection(ent_id_mc, ent_ood_mc):.2%}\")\n",
    "print(f\"Ensemble:   {evaluate_ood_detection(ent_id_ens, ent_ood_ens):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Heteroscedastic Networks: Learning Aleatoric Uncertainty\n",
    "\n",
    "Instead of predicting just the mean, predict **both mean and variance**.\n",
    "\n",
    "For regression: $p(y|x) = \\mathcal{N}(\\mu(x), \\sigma^2(x))$\n",
    "\n",
    "Loss: Negative log-likelihood\n",
    "$$\\mathcal{L} = \\frac{1}{2\\sigma^2}(y - \\mu)^2 + \\frac{1}{2}\\log\\sigma^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroscedasticNet(nn.Module):\n",
    "    \"\"\"Network that predicts mean and variance.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=1, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden_dim, 1)\n",
    "        self.logvar_head = nn.Linear(hidden_dim, 1)  # Log variance for stability\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.shared(x)\n",
    "        mean = self.mean_head(h)\n",
    "        logvar = self.logvar_head(h)\n",
    "        return mean, logvar\n",
    "\n",
    "\n",
    "def heteroscedastic_loss(y_true, mean, logvar):\n",
    "    \"\"\"Negative log-likelihood for Gaussian with predicted variance.\"\"\"\n",
    "    precision = torch.exp(-logvar)\n",
    "    return torch.mean(0.5 * precision * (y_true - mean)**2 + 0.5 * logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare regression data\n",
    "x_tensor = torch.tensor(x_train).unsqueeze(1)\n",
    "y_tensor = torch.tensor(y_train).unsqueeze(1)\n",
    "reg_dataset = TensorDataset(x_tensor, y_tensor)\n",
    "reg_loader = DataLoader(reg_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train heteroscedastic model\n",
    "het_model = HeteroscedasticNet().to(device)\n",
    "optimizer = torch.optim.Adam(het_model.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(500):\n",
    "    for x_batch, y_batch in reg_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        mean, logvar = het_model(x_batch)\n",
    "        loss = heteroscedastic_loss(y_batch, mean, logvar)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Heteroscedastic model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned uncertainty\n",
    "het_model.eval()\n",
    "x_test = torch.linspace(-4, 4, 200).unsqueeze(1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mean_pred, logvar_pred = het_model(x_test)\n",
    "    std_pred = torch.exp(0.5 * logvar_pred)\n",
    "\n",
    "x_np = x_test.cpu().numpy().squeeze()\n",
    "mean_np = mean_pred.cpu().numpy().squeeze()\n",
    "std_np = std_pred.cpu().numpy().squeeze()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(x_train, y_train, alpha=0.5, label='Training data')\n",
    "plt.plot(x_np, mean_np, 'r-', label='Predicted mean', linewidth=2)\n",
    "plt.fill_between(x_np, mean_np - 2*std_np, mean_np + 2*std_np, \n",
    "                 alpha=0.3, color='red', label='Predicted ±2σ')\n",
    "plt.plot(x_np, np.sin(x_np), 'g--', label='True function', linewidth=2)\n",
    "plt.axvline(x=3, color='gray', linestyle=':', label='Training boundary')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Heteroscedastic Network: Learned Aleatoric Uncertainty')\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Model learns higher variance where data is noisier\")\n",
    "print(\"But uncertainty doesn't increase much outside training range (epistemic!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Combining Methods: Epistemic + Aleatoric\n",
    "\n",
    "Use ensembles of heteroscedastic networks to capture both types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroscedasticEnsemble:\n",
    "    \"\"\"Ensemble of heteroscedastic models for both uncertainty types.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_models=5):\n",
    "        self.models = [HeteroscedasticNet() for _ in range(n_models)]\n",
    "        self.n_models = n_models\n",
    "    \n",
    "    def to(self, device):\n",
    "        for m in self.models:\n",
    "            m.to(device)\n",
    "        return self\n",
    "    \n",
    "    def train_all(self, loader, epochs=500, lr=1e-2):\n",
    "        for i, model in enumerate(self.models):\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            for epoch in range(epochs):\n",
    "                for x, y in loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    mean, logvar = model(x)\n",
    "                    loss = heteroscedastic_loss(y, mean, logvar)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            print(f\"Model {i+1}/{self.n_models} trained\")\n",
    "    \n",
    "    def predict(self, x):\n",
    "        means, variances = [], []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                mean, logvar = model(x)\n",
    "                means.append(mean)\n",
    "                variances.append(torch.exp(logvar))\n",
    "        \n",
    "        means = torch.stack(means)\n",
    "        variances = torch.stack(variances)\n",
    "        \n",
    "        # Total uncertainty decomposition\n",
    "        mean_pred = means.mean(dim=0)\n",
    "        aleatoric = variances.mean(dim=0)  # Average predicted variance\n",
    "        epistemic = means.var(dim=0)       # Variance of means\n",
    "        total = aleatoric + epistemic\n",
    "        \n",
    "        return mean_pred, aleatoric, epistemic, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble\n",
    "het_ensemble = HeteroscedasticEnsemble(n_models=5).to(device)\n",
    "het_ensemble.train_all(reg_loader, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decomposed uncertainty\n",
    "mean_pred, aleatoric, epistemic, total = het_ensemble.predict(x_test)\n",
    "\n",
    "mean_np = mean_pred.cpu().numpy().squeeze()\n",
    "ale_np = np.sqrt(aleatoric.cpu().numpy().squeeze())\n",
    "epi_np = np.sqrt(epistemic.cpu().numpy().squeeze())\n",
    "tot_np = np.sqrt(total.cpu().numpy().squeeze())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (unc, name) in zip(axes, [(ale_np, 'Aleatoric'), (epi_np, 'Epistemic'), (tot_np, 'Total')]):\n",
    "    ax.scatter(x_train, y_train, alpha=0.3, s=10)\n",
    "    ax.plot(x_np, mean_np, 'r-', linewidth=2)\n",
    "    ax.fill_between(x_np, mean_np - 2*unc, mean_np + 2*unc, alpha=0.3, color='red')\n",
    "    ax.axvline(x=3, color='gray', linestyle=':')\n",
    "    ax.axvline(x=-3, color='gray', linestyle=':')\n",
    "    ax.set_title(f'{name} Uncertainty')\n",
    "    ax.set_xlabel('x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Aleatoric: Higher where data is noisier (center-right)\")\n",
    "print(\"- Epistemic: Higher outside training range (extrapolation)\")\n",
    "print(\"- Total: Combines both sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Bayesian Neural Networks (Introduction)\n",
    "\n",
    "Instead of point estimates for weights, learn **distributions** over weights.\n",
    "\n",
    "### Bayes by Backprop\n",
    "- Weights: $w \\sim q_\\theta(w) = \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "- Learn $\\mu$ and $\\sigma$ via variational inference\n",
    "- Loss: $\\mathcal{L} = \\text{KL}(q(w) \\| p(w)) - \\mathbb{E}_{q(w)}[\\log p(D|w)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinear(nn.Module):\n",
    "    \"\"\"Linear layer with weight uncertainty (Bayes by Backprop).\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, prior_std=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Weight mean and log variance (rho parameterization)\n",
    "        self.weight_mu = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        self.weight_rho = nn.Parameter(torch.zeros(out_features, in_features) - 3)\n",
    "        \n",
    "        # Bias\n",
    "        self.bias_mu = nn.Parameter(torch.zeros(out_features))\n",
    "        self.bias_rho = nn.Parameter(torch.zeros(out_features) - 3)\n",
    "        \n",
    "        # Prior\n",
    "        self.prior_std = prior_std\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.kaiming_normal_(self.weight_mu)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reparameterization: sigma = log(1 + exp(rho))\n",
    "        weight_sigma = F.softplus(self.weight_rho)\n",
    "        bias_sigma = F.softplus(self.bias_rho)\n",
    "        \n",
    "        # Sample weights\n",
    "        weight = self.weight_mu + weight_sigma * torch.randn_like(weight_sigma)\n",
    "        bias = self.bias_mu + bias_sigma * torch.randn_like(bias_sigma)\n",
    "        \n",
    "        return F.linear(x, weight, bias)\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        \"\"\"KL divergence from posterior to prior.\"\"\"\n",
    "        weight_sigma = F.softplus(self.weight_rho)\n",
    "        bias_sigma = F.softplus(self.bias_rho)\n",
    "        \n",
    "        # KL for Gaussian: 0.5 * (sigma^2/prior^2 + mu^2/prior^2 - 1 - log(sigma^2/prior^2))\n",
    "        kl_weight = 0.5 * (weight_sigma**2 / self.prior_std**2 + \n",
    "                          self.weight_mu**2 / self.prior_std**2 - 1 - \n",
    "                          torch.log(weight_sigma**2 / self.prior_std**2))\n",
    "        kl_bias = 0.5 * (bias_sigma**2 / self.prior_std**2 + \n",
    "                        self.bias_mu**2 / self.prior_std**2 - 1 - \n",
    "                        torch.log(bias_sigma**2 / self.prior_std**2))\n",
    "        \n",
    "        return kl_weight.sum() + kl_bias.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNet(nn.Module):\n",
    "    \"\"\"Simple Bayesian neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dim=128, output_dim=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = BayesianLinear(input_dim, hidden_dim)\n",
    "        self.fc2 = BayesianLinear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = BayesianLinear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        return self.fc1.kl_divergence() + self.fc2.kl_divergence() + self.fc3.kl_divergence()\n",
    "    \n",
    "    def predict_with_uncertainty(self, x, n_samples=30):\n",
    "        predictions = []\n",
    "        for _ in range(n_samples):\n",
    "            probs = F.softmax(self(x), dim=1)\n",
    "            predictions.append(probs)\n",
    "        \n",
    "        predictions = torch.stack(predictions)\n",
    "        mean_pred = predictions.mean(dim=0)\n",
    "        entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)\n",
    "        \n",
    "        return mean_pred, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BNN\n",
    "bnn = BayesianNet().to(device)\n",
    "optimizer = torch.optim.Adam(bnn.parameters(), lr=1e-3)\n",
    "n_batches = len(train_loader)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # ELBO loss: NLL + KL / n_batches\n",
    "        logits = bnn(x)\n",
    "        nll = F.cross_entropy(logits, y)\n",
    "        kl = bnn.kl_divergence() / (n_batches * x.size(0))\n",
    "        loss = nll + kl\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/n_batches:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BNN uncertainty\n",
    "bnn.eval()\n",
    "mean_id_bnn, entropy_id_bnn = bnn.predict_with_uncertainty(x_id)\n",
    "mean_ood_bnn, entropy_ood_bnn = bnn.predict_with_uncertainty(x_ood)\n",
    "\n",
    "print(\"BNN Uncertainty (entropy):\")\n",
    "print(f\"In-distribution:  {entropy_id_bnn.detach().cpu().numpy()}\")\n",
    "print(f\"Out-of-distribution: {entropy_ood_bnn.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model Calibration\n",
    "\n",
    "A model is **calibrated** if its confidence matches its accuracy.\n",
    "\n",
    "If model says 80% confident → should be correct 80% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_calibration(model, loader, n_bins=10, mc_samples=None):\n",
    "    \"\"\"Compute Expected Calibration Error and reliability diagram data.\"\"\"\n",
    "    confidences, accuracies, predictions = [], [], []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            if mc_samples and hasattr(model, 'predict_with_uncertainty'):\n",
    "                probs, _ = model.predict_with_uncertainty(x, n_samples=mc_samples)\n",
    "            else:\n",
    "                probs = F.softmax(model(x), dim=1)\n",
    "            \n",
    "            conf, pred = probs.max(dim=1)\n",
    "            correct = (pred == y).float()\n",
    "            \n",
    "            confidences.extend(conf.cpu().numpy())\n",
    "            accuracies.extend(correct.cpu().numpy())\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "    \n",
    "    confidences = np.array(confidences)\n",
    "    accuracies = np.array(accuracies)\n",
    "    \n",
    "    # Bin by confidence\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_accs, bin_confs, bin_counts = [], [], []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            bin_accs.append(accuracies[mask].mean())\n",
    "            bin_confs.append(confidences[mask].mean())\n",
    "            bin_counts.append(mask.sum())\n",
    "        else:\n",
    "            bin_accs.append(0)\n",
    "            bin_confs.append((bin_boundaries[i] + bin_boundaries[i+1]) / 2)\n",
    "            bin_counts.append(0)\n",
    "    \n",
    "    # ECE: weighted average of |accuracy - confidence|\n",
    "    ece = sum(c * abs(a - cf) for a, cf, c in zip(bin_accs, bin_confs, bin_counts)) / sum(bin_counts)\n",
    "    \n",
    "    return ece, bin_accs, bin_confs, bin_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_diagram(bin_accs, bin_confs, ece, title):\n",
    "    \"\"\"Plot reliability diagram.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    \n",
    "    # Bar chart\n",
    "    width = 0.08\n",
    "    plt.bar(bin_confs, bin_accs, width=width, alpha=0.7, label='Model')\n",
    "    \n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{title}\\nECE = {ece:.4f}')\n",
    "    plt.legend()\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "# Compare calibration\n",
    "print(\"Computing calibration...\")\n",
    "\n",
    "ece_simple, acc_s, conf_s, _ = compute_calibration(model, test_loader)\n",
    "ece_mc, acc_mc, conf_mc, _ = compute_calibration(mc_model, test_loader, mc_samples=30)\n",
    "\n",
    "plot_reliability_diagram(acc_s, conf_s, ece_simple, 'Standard Network')\n",
    "plot_reliability_diagram(acc_mc, conf_mc, ece_mc, 'MC Dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Scaling\n",
    "\n",
    "Simple post-hoc calibration: divide logits by learned temperature T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemperatureScaling(nn.Module):\n",
    "    \"\"\"Post-hoc calibration via temperature scaling.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits / self.temperature\n",
    "    \n",
    "    def calibrate(self, val_loader, lr=0.01, epochs=50):\n",
    "        \"\"\"Learn temperature on validation set.\"\"\"\n",
    "        self.model.eval()\n",
    "        optimizer = torch.optim.LBFGS([self.temperature], lr=lr, max_iter=epochs)\n",
    "        \n",
    "        # Collect all logits and labels\n",
    "        logits_list, labels_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits_list.append(self.model(x))\n",
    "                labels_list.append(y)\n",
    "        \n",
    "        all_logits = torch.cat(logits_list)\n",
    "        all_labels = torch.cat(labels_list)\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(all_logits / self.temperature, all_labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "        print(f\"Learned temperature: {self.temperature.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply temperature scaling\n",
    "# Use part of test set as validation\n",
    "val_loader = DataLoader(Subset(mnist_test, range(5000)), batch_size=128)\n",
    "test_loader_half = DataLoader(Subset(mnist_test, range(5000, 10000)), batch_size=128)\n",
    "\n",
    "temp_model = TemperatureScaling(model).to(device)\n",
    "temp_model.calibrate(val_loader)\n",
    "\n",
    "# Compare calibration\n",
    "ece_temp, acc_t, conf_t, _ = compute_calibration(temp_model, test_loader_half)\n",
    "ece_orig, acc_o, conf_o, _ = compute_calibration(model, test_loader_half)\n",
    "\n",
    "print(f\"\\nECE before temperature scaling: {ece_orig:.4f}\")\n",
    "print(f\"ECE after temperature scaling:  {ece_temp:.4f}\")\n",
    "\n",
    "plot_reliability_diagram(acc_t, conf_t, ece_temp, 'After Temperature Scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary\n",
    "\n",
    "### Methods Comparison\n",
    "\n",
    "| Method | Captures | Compute Cost | Memory | Quality |\n",
    "|--------|----------|--------------|--------|--------|\n",
    "| MC Dropout | Epistemic | N forward passes | 1× | Good |\n",
    "| Deep Ensemble | Epistemic | N× training | N× | Best |\n",
    "| Heteroscedastic | Aleatoric | 1× | 1× | Good |\n",
    "| BNN | Both | N forward passes | ~2× | Variable |\n",
    "| Temperature Scaling | Calibration only | 1× | 1× | Limited |\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "- **MC Dropout**: Quick uncertainty with minimal changes\n",
    "- **Deep Ensemble**: Best uncertainty, if compute allows\n",
    "- **Heteroscedastic**: When data has varying noise\n",
    "- **BNN**: Research, principled uncertainty\n",
    "- **Temperature Scaling**: Quick calibration fix\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Aleatoric** = data noise (irreducible), **Epistemic** = model uncertainty (reducible)\n",
    "2. Softmax confidence ≠ calibrated probability\n",
    "3. Ensembles are simple and effective\n",
    "4. Always evaluate calibration (ECE, reliability diagrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: OOD Detection with Fashion-MNIST\n",
    "Train on MNIST, test uncertainty on Fashion-MNIST. Compare MC Dropout vs Ensemble.\n",
    "\n",
    "### Exercise 2: Uncertainty-Aware Prediction\n",
    "Build a classifier that abstains (outputs \"unknown\") when uncertainty is high. Measure accuracy vs coverage trade-off.\n",
    "\n",
    "### Exercise 3: Active Learning Simulation\n",
    "Start with 100 labeled samples. Use uncertainty to select which samples to label next. Compare to random selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: OOD Detection with Fashion-MNIST\n",
    "fashion_test = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
    "fashion_loader = DataLoader(fashion_test, batch_size=128)\n",
    "\n",
    "def evaluate_ood(model, id_loader, ood_loader, method='mc', n_samples=30):\n",
    "    \"\"\"Evaluate OOD detection using entropy.\"\"\"\n",
    "    id_entropies, ood_entropies = [], []\n",
    "    \n",
    "    # ID data\n",
    "    for x, _ in id_loader:\n",
    "        x = x.to(device)\n",
    "        if method == 'mc' and hasattr(model, 'predict_with_uncertainty'):\n",
    "            _, entropy = model.predict_with_uncertainty(x, n_samples=n_samples)\n",
    "        elif method == 'ensemble':\n",
    "            _, entropy, _ = model.predict_with_uncertainty(x)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                probs = F.softmax(model(x), dim=1)\n",
    "                entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1)\n",
    "        id_entropies.extend(entropy.cpu().numpy())\n",
    "    \n",
    "    # OOD data\n",
    "    for x, _ in ood_loader:\n",
    "        x = x.to(device)\n",
    "        if method == 'mc' and hasattr(model, 'predict_with_uncertainty'):\n",
    "            _, entropy = model.predict_with_uncertainty(x, n_samples=n_samples)\n",
    "        elif method == 'ensemble':\n",
    "            _, entropy, _ = model.predict_with_uncertainty(x)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                probs = F.softmax(model(x), dim=1)\n",
    "                entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1)\n",
    "        ood_entropies.extend(entropy.cpu().numpy())\n",
    "    \n",
    "    id_entropies = np.array(id_entropies)\n",
    "    ood_entropies = np.array(ood_entropies)\n",
    "    \n",
    "    # AUROC approximation\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    labels = np.concatenate([np.zeros(len(id_entropies)), np.ones(len(ood_entropies))])\n",
    "    scores = np.concatenate([id_entropies, ood_entropies])\n",
    "    auroc = roc_auc_score(labels, scores)\n",
    "    \n",
    "    return auroc, id_entropies, ood_entropies\n",
    "\n",
    "# Evaluate\n",
    "auroc_mc, id_mc, ood_mc = evaluate_ood(mc_model, test_loader, fashion_loader, method='mc')\n",
    "auroc_ens, id_ens, ood_ens = evaluate_ood(ensemble, test_loader, fashion_loader, method='ensemble')\n",
    "\n",
    "print(f\"OOD Detection AUROC (MNIST vs Fashion-MNIST):\")\n",
    "print(f\"MC Dropout: {auroc_mc:.4f}\")\n",
    "print(f\"Ensemble:   {auroc_ens:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(id_mc, bins=50, alpha=0.5, label='MNIST (ID)', density=True)\n",
    "axes[0].hist(ood_mc, bins=50, alpha=0.5, label='Fashion (OOD)', density=True)\n",
    "axes[0].set_title(f'MC Dropout (AUROC={auroc_mc:.3f})')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(id_ens, bins=50, alpha=0.5, label='MNIST (ID)', density=True)\n",
    "axes[1].hist(ood_ens, bins=50, alpha=0.5, label='Fashion (OOD)', density=True)\n",
    "axes[1].set_title(f'Ensemble (AUROC={auroc_ens:.3f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Uncertainty-Aware Prediction (Selective Prediction)\n",
    "def selective_prediction(model, loader, thresholds, n_samples=30):\n",
    "    \"\"\"Evaluate accuracy at different coverage levels.\"\"\"\n",
    "    all_preds, all_labels, all_entropy = [], [], []\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        if hasattr(model, 'predict_with_uncertainty'):\n",
    "            probs, entropy = model.predict_with_uncertainty(x, n_samples=n_samples)\n",
    "        else:\n",
    "            _, entropy, _ = model.predict_with_uncertainty(x)\n",
    "            probs = _\n",
    "        \n",
    "        preds = probs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.numpy())\n",
    "        all_entropy.extend(entropy.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_entropy = np.array(all_entropy)\n",
    "    \n",
    "    results = []\n",
    "    for thresh in thresholds:\n",
    "        # Only predict when entropy < threshold\n",
    "        mask = all_entropy < thresh\n",
    "        coverage = mask.mean()\n",
    "        if coverage > 0:\n",
    "            accuracy = (all_preds[mask] == all_labels[mask]).mean()\n",
    "        else:\n",
    "            accuracy = 0\n",
    "        results.append((coverage, accuracy))\n",
    "    \n",
    "    return results\n",
    "\n",
    "thresholds = np.linspace(0.01, 2.5, 50)\n",
    "results_mc = selective_prediction(mc_model, test_loader, thresholds)\n",
    "results_ens = selective_prediction(ensemble, test_loader, thresholds)\n",
    "\n",
    "coverages_mc, accs_mc = zip(*results_mc)\n",
    "coverages_ens, accs_ens = zip(*results_ens)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(coverages_mc, accs_mc, 'b-', label='MC Dropout')\n",
    "plt.plot(coverages_ens, accs_ens, 'r-', label='Ensemble')\n",
    "plt.xlabel('Coverage (fraction of predictions made)')\n",
    "plt.ylabel('Accuracy (on predictions made)')\n",
    "plt.title('Selective Prediction: Accuracy vs Coverage')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: By abstaining on uncertain samples, we can achieve higher accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Active Learning Simulation\n",
    "def active_learning_experiment(train_data, test_loader, n_initial=100, n_rounds=10, n_per_round=100):\n",
    "    \"\"\"Compare uncertainty sampling vs random sampling.\"\"\"\n",
    "    n_total = len(train_data)\n",
    "    \n",
    "    # Initial random subset\n",
    "    all_indices = list(range(n_total))\n",
    "    np.random.shuffle(all_indices)\n",
    "    \n",
    "    labeled_random = all_indices[:n_initial].copy()\n",
    "    labeled_uncertainty = all_indices[:n_initial].copy()\n",
    "    pool_random = all_indices[n_initial:].copy()\n",
    "    pool_uncertainty = all_indices[n_initial:].copy()\n",
    "    \n",
    "    results_random, results_uncertainty = [], []\n",
    "    \n",
    "    for round_idx in range(n_rounds):\n",
    "        # Train model on current labeled set - Random\n",
    "        model_r = SimpleNet().to(device)\n",
    "        opt_r = torch.optim.Adam(model_r.parameters(), lr=1e-3)\n",
    "        loader_r = DataLoader(Subset(train_data, labeled_random), batch_size=64, shuffle=True)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            for x, y in loader_r:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                loss = F.cross_entropy(model_r(x), y)\n",
    "                opt_r.zero_grad()\n",
    "                loss.backward()\n",
    "                opt_r.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model_r.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                pred = model_r(x).argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        results_random.append(correct / total)\n",
    "        \n",
    "        # Train model - Uncertainty\n",
    "        model_u = MCDropoutNet(dropout_p=0.2).to(device)\n",
    "        opt_u = torch.optim.Adam(model_u.parameters(), lr=1e-3)\n",
    "        loader_u = DataLoader(Subset(train_data, labeled_uncertainty), batch_size=64, shuffle=True)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            model_u.train()\n",
    "            for x, y in loader_u:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                loss = F.cross_entropy(model_u(x), y)\n",
    "                opt_u.zero_grad()\n",
    "                loss.backward()\n",
    "                opt_u.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model_u.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                pred = model_u(x).argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        results_uncertainty.append(correct / total)\n",
    "        \n",
    "        # Select next samples\n",
    "        if len(pool_random) >= n_per_round:\n",
    "            # Random: just take next n\n",
    "            selected_r = pool_random[:n_per_round]\n",
    "            pool_random = pool_random[n_per_round:]\n",
    "            labeled_random.extend(selected_r)\n",
    "            \n",
    "            # Uncertainty: compute entropy on pool\n",
    "            pool_loader = DataLoader(Subset(train_data, pool_uncertainty), batch_size=128)\n",
    "            entropies = []\n",
    "            for x, _ in pool_loader:\n",
    "                x = x.to(device)\n",
    "                _, ent = model_u.predict_with_uncertainty(x, n_samples=10)\n",
    "                entropies.extend(ent.cpu().numpy())\n",
    "            \n",
    "            # Select highest entropy\n",
    "            top_indices = np.argsort(entropies)[-n_per_round:]\n",
    "            selected_u = [pool_uncertainty[i] for i in top_indices]\n",
    "            pool_uncertainty = [p for i, p in enumerate(pool_uncertainty) if i not in top_indices]\n",
    "            labeled_uncertainty.extend(selected_u)\n",
    "        \n",
    "        print(f\"Round {round_idx+1}: Random={results_random[-1]:.4f}, Uncertainty={results_uncertainty[-1]:.4f}\")\n",
    "    \n",
    "    return results_random, results_uncertainty\n",
    "\n",
    "# Run experiment (reduced for speed)\n",
    "results_r, results_u = active_learning_experiment(mnist_train, test_loader, \n",
    "                                                   n_initial=100, n_rounds=5, n_per_round=200)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "x_axis = [100 + i*200 for i in range(len(results_r))]\n",
    "plt.plot(x_axis, results_r, 'b-o', label='Random Sampling')\n",
    "plt.plot(x_axis, results_u, 'r-o', label='Uncertainty Sampling')\n",
    "plt.xlabel('Number of labeled samples')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Active Learning: Uncertainty vs Random Sampling')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
