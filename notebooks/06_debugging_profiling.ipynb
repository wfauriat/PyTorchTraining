{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Debugging and Profiling PyTorch Code\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Identify common bugs** - Shape mismatches, NaN gradients, memory leaks, device mismatches\n",
    "2. **Use debugging tools** - `torch.autograd.detect_anomaly`, gradient hooks, assertions\n",
    "3. **Profile performance** - PyTorch Profiler for CPU/GPU/memory bottlenecks\n",
    "4. **Visualize training** - TensorBoard integration for loss curves and model inspection\n",
    "5. **Debug memory issues** - Track GPU memory, find leaks, optimize usage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Common PyTorch Bugs and How to Fix Them\n",
    "\n",
    "### 1.1 Shape Mismatches\n",
    "\n",
    "The most common bug category. Let's see examples and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG 1: Incorrect input shape to linear layer\n",
    "\n",
    "class BuggyModel1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 32, 3)  # Output: (B, 32, H-2, W-2)\n",
    "        self.fc = nn.Linear(32, 10)  # Expects flattened input!\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))  # (B, 32, 26, 26) for 28x28 input\n",
    "        x = self.fc(x)  # BUG: x is (B, 32, 26, 26), not (B, 32)\n",
    "        return x\n",
    "\n",
    "# This will fail\n",
    "try:\n",
    "    model = BuggyModel1()\n",
    "    x = torch.randn(4, 1, 28, 28)\n",
    "    output = model(x)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nShape mismatch: Conv output wasn't flattened before Linear layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Properly flatten before linear layer\n",
    "\n",
    "class FixedModel1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 32, 3)  # Output: (B, 32, 26, 26)\n",
    "        self.fc = nn.Linear(32 * 26 * 26, 10)  # Correct input size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = x.flatten(1)  # (B, 32*26*26)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = FixedModel1()\n",
    "x = torch.randn(4, 1, 28, 28)\n",
    "output = model(x)\n",
    "print(f\"Output shape: {output.shape}\")  # (4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG 2: Forgetting batch dimension\n",
    "\n",
    "model = nn.Linear(10, 5)\n",
    "\n",
    "# Wrong: passing 1D tensor instead of 2D\n",
    "try:\n",
    "    x = torch.randn(10)  # Missing batch dimension\n",
    "    output = model(x)\n",
    "    print(f\"Accidentally worked! Output shape: {output.shape}\")\n",
    "    # This works but might cause issues later with BatchNorm, etc.\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Correct: always use batch dimension\n",
    "x = torch.randn(1, 10)  # Batch size of 1\n",
    "output = model(x)\n",
    "print(f\"Correct output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG 3: Channel order mismatch (NCHW vs NHWC)\n",
    "\n",
    "# PyTorch uses NCHW (Batch, Channels, Height, Width)\n",
    "# Some other frameworks use NHWC\n",
    "\n",
    "# Wrong: NumPy image in HWC format\n",
    "numpy_image = np.random.rand(28, 28, 3)  # HWC\n",
    "try:\n",
    "    x = torch.from_numpy(numpy_image).float()\n",
    "    conv = nn.Conv2d(3, 16, 3)\n",
    "    output = conv(x.unsqueeze(0))  # Adds batch but wrong channel position!\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Correct: Convert HWC to CHW\n",
    "x = torch.from_numpy(numpy_image).float()\n",
    "x = x.permute(2, 0, 1)  # HWC -> CHW\n",
    "x = x.unsqueeze(0)  # Add batch: CHW -> NCHW\n",
    "print(f\"Correct shape: {x.shape}\")  # (1, 3, 28, 28)\n",
    "output = conv(x)\n",
    "print(f\"Conv output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Device Mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG 4: Mixing CPU and GPU tensors\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = nn.Linear(10, 5).cuda()\n",
    "    x_cpu = torch.randn(4, 10)  # On CPU\n",
    "    \n",
    "    try:\n",
    "        output = model(x_cpu)  # Model on GPU, data on CPU\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Fix: Move data to same device as model\n",
    "    x_gpu = x_cpu.to(model.weight.device)\n",
    "    output = model(x_gpu)\n",
    "    print(f\"Success! Output device: {output.device}\")\n",
    "else:\n",
    "    print(\"GPU not available - skipping device mismatch demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to ensure all data on correct device\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Recursively move data to device\"\"\"\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        return type(data)(to_device(d, device) for d in data)\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Usage in training loop\n",
    "def train_step(model, batch, device):\n",
    "    batch = to_device(batch, device)\n",
    "    x, y = batch\n",
    "    return model(x)\n",
    "\n",
    "print(\"Always use a consistent device handling pattern!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 NaN and Inf Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG 5: Numerical instability causing NaN\n",
    "\n",
    "def unstable_softmax(x):\n",
    "    \"\"\"Numerically unstable softmax\"\"\"\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=-1, keepdim=True)\n",
    "\n",
    "def stable_softmax(x):\n",
    "    \"\"\"Numerically stable softmax\"\"\"\n",
    "    x_max = x.max(dim=-1, keepdim=True).values\n",
    "    exp_x = torch.exp(x - x_max)\n",
    "    return exp_x / exp_x.sum(dim=-1, keepdim=True)\n",
    "\n",
    "# Test with large values\n",
    "x = torch.tensor([1000.0, 1001.0, 1002.0])\n",
    "\n",
    "print(f\"Unstable softmax: {unstable_softmax(x)}\")  # NaN!\n",
    "print(f\"Stable softmax: {stable_softmax(x)}\")  # Works\n",
    "print(f\"PyTorch softmax: {F.softmax(x, dim=-1)}\")  # PyTorch handles this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG 6: Log of zero\n",
    "\n",
    "def buggy_cross_entropy(pred, target):\n",
    "    \"\"\"Can produce -inf when pred contains 0\"\"\"\n",
    "    return -torch.sum(target * torch.log(pred))\n",
    "\n",
    "def safe_cross_entropy(pred, target, eps=1e-8):\n",
    "    \"\"\"Clamp to avoid log(0)\"\"\"\n",
    "    return -torch.sum(target * torch.log(pred.clamp(min=eps)))\n",
    "\n",
    "# Test\n",
    "pred = torch.tensor([0.0, 0.5, 0.5])  # Contains 0!\n",
    "target = torch.tensor([1.0, 0.0, 0.0])\n",
    "\n",
    "print(f\"Buggy CE: {buggy_cross_entropy(pred, target)}\")  # -inf\n",
    "print(f\"Safe CE: {safe_cross_entropy(pred, target)}\")  # Large but finite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting NaN/Inf in tensors\n",
    "\n",
    "def check_tensor(tensor, name=\"tensor\"):\n",
    "    \"\"\"Check tensor for NaN and Inf values\"\"\"\n",
    "    has_nan = torch.isnan(tensor).any().item()\n",
    "    has_inf = torch.isinf(tensor).any().item()\n",
    "    \n",
    "    if has_nan:\n",
    "        print(f\"WARNING: {name} contains NaN!\")\n",
    "        print(f\"  NaN count: {torch.isnan(tensor).sum().item()}\")\n",
    "    if has_inf:\n",
    "        print(f\"WARNING: {name} contains Inf!\")\n",
    "        print(f\"  Inf count: {torch.isinf(tensor).sum().item()}\")\n",
    "    \n",
    "    if not has_nan and not has_inf:\n",
    "        print(f\"{name}: OK (min={tensor.min():.4f}, max={tensor.max():.4f})\")\n",
    "    \n",
    "    return not (has_nan or has_inf)\n",
    "\n",
    "# Test\n",
    "good_tensor = torch.randn(100)\n",
    "bad_tensor = torch.tensor([1.0, float('nan'), float('inf'), -float('inf')])\n",
    "\n",
    "check_tensor(good_tensor, \"good_tensor\")\n",
    "check_tensor(bad_tensor, \"bad_tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Memory Leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG 7: Accumulating tensors in a list (keeps computation graph!)\n",
    "\n",
    "def buggy_training_loop(model, data_loader, epochs=3):\n",
    "    \"\"\"Memory leak: storing tensors with gradients\"\"\"\n",
    "    losses = []  # Stores tensors with computation graphs!\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in data_loader:\n",
    "            output = model(x)\n",
    "            loss = F.mse_loss(output, y)\n",
    "            losses.append(loss)  # BUG: loss has grad_fn attached!\n",
    "    \n",
    "    return losses  # All computation graphs are retained in memory\n",
    "\n",
    "def fixed_training_loop(model, data_loader, epochs=3):\n",
    "    \"\"\"Fixed: detach or use .item() for scalars\"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in data_loader:\n",
    "            output = model(x)\n",
    "            loss = F.mse_loss(output, y)\n",
    "            losses.append(loss.item())  # .item() extracts Python float\n",
    "            # Or: losses.append(loss.detach())  # Remove from graph\n",
    "    \n",
    "    return losses\n",
    "\n",
    "print(\"Always use .item() when storing loss values!\")\n",
    "print(\"Or use .detach() if you need to keep the tensor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG 8: Not using torch.no_grad() during evaluation\n",
    "\n",
    "def buggy_evaluate(model, data_loader):\n",
    "    \"\"\"Builds computation graphs during evaluation - wastes memory!\"\"\"\n",
    "    total_loss = 0\n",
    "    for x, y in data_loader:\n",
    "        output = model(x)  # Builds graph even though we don't need gradients\n",
    "        loss = F.mse_loss(output, y)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "@torch.no_grad()  # Decorator form\n",
    "def fixed_evaluate(model, data_loader):\n",
    "    \"\"\"No computation graphs built - saves memory\"\"\"\n",
    "    total_loss = 0\n",
    "    for x, y in data_loader:\n",
    "        output = model(x)\n",
    "        loss = F.mse_loss(output, y)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "# Alternative: context manager\n",
    "def fixed_evaluate_v2(model, data_loader):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            output = model(x)\n",
    "            loss = F.mse_loss(output, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "print(\"Always use torch.no_grad() or torch.inference_mode() during evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Debugging Tools\n",
    "\n",
    "### 2.1 torch.autograd.detect_anomaly\n",
    "\n",
    "Enables detailed error messages for backward pass issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that produces NaN during training\n",
    "\n",
    "class ProblematicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # Intentionally cause NaN: sqrt of negative number\n",
    "        x = torch.sqrt(x)  # NaN for negative values!\n",
    "        return x\n",
    "\n",
    "# Without anomaly detection\n",
    "model = ProblematicModel()\n",
    "x = torch.randn(4, 10)\n",
    "output = model(x)\n",
    "print(f\"Output has NaN: {torch.isnan(output).any().item()}\")\n",
    "\n",
    "try:\n",
    "    loss = output.sum()\n",
    "    loss.backward()  # Might silently produce NaN gradients\n",
    "    print(f\"Gradient has NaN: {torch.isnan(model.linear.weight.grad).any().item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With anomaly detection - get detailed error location\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "model = ProblematicModel()\n",
    "x = torch.randn(4, 10)\n",
    "\n",
    "try:\n",
    "    output = model(x)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"Caught anomaly!\")\n",
    "    print(f\"Error: {str(e)[:200]}...\")\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)  # Disable (has performance cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using as context manager (recommended)\n",
    "\n",
    "model = ProblematicModel()\n",
    "x = torch.randn(4, 10)\n",
    "\n",
    "with torch.autograd.detect_anomaly():\n",
    "    try:\n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "    except RuntimeError as e:\n",
    "        print(\"Anomaly detected in context manager!\")\n",
    "        # The error message includes the forward pass location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient Hooks for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDebugger:\n",
    "    \"\"\"\n",
    "    Utility class to monitor gradients during training.\n",
    "    Helps identify vanishing/exploding gradients and NaN values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        self.gradient_stats: Dict[str, Dict] = {}\n",
    "        self.hooks = []\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register backward hooks on all parameters\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                hook = param.register_hook(\n",
    "                    lambda grad, n=name: self._gradient_hook(n, grad)\n",
    "                )\n",
    "                self.hooks.append(hook)\n",
    "        print(f\"Registered hooks on {len(self.hooks)} parameters\")\n",
    "    \n",
    "    def _gradient_hook(self, name: str, grad: torch.Tensor):\n",
    "        \"\"\"Called during backward pass for each parameter\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.gradient_stats[name] = {\n",
    "                'mean': grad.mean().item(),\n",
    "                'std': grad.std().item(),\n",
    "                'min': grad.min().item(),\n",
    "                'max': grad.max().item(),\n",
    "                'norm': grad.norm().item(),\n",
    "                'has_nan': torch.isnan(grad).any().item(),\n",
    "                'has_inf': torch.isinf(grad).any().item(),\n",
    "            }\n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"Print gradient statistics\"\"\"\n",
    "        print(\"\\nGradient Statistics:\")\n",
    "        print(\"-\" * 80)\n",
    "        for name, stats in self.gradient_stats.items():\n",
    "            status = \"OK\"\n",
    "            if stats['has_nan']:\n",
    "                status = \"NaN!\"\n",
    "            elif stats['has_inf']:\n",
    "                status = \"Inf!\"\n",
    "            elif stats['norm'] < 1e-7:\n",
    "                status = \"Vanishing?\"\n",
    "            elif stats['norm'] > 1e3:\n",
    "                status = \"Exploding?\"\n",
    "            \n",
    "            print(f\"{name:40} | norm: {stats['norm']:10.4f} | {status}\")\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "\n",
    "\n",
    "# Demo\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 1)\n",
    ")\n",
    "\n",
    "debugger = GradientDebugger(model)\n",
    "debugger.register_hooks()\n",
    "\n",
    "# Forward and backward pass\n",
    "x = torch.randn(32, 10)\n",
    "y = torch.randn(32, 1)\n",
    "output = model(x)\n",
    "loss = F.mse_loss(output, y)\n",
    "loss.backward()\n",
    "\n",
    "debugger.print_stats()\n",
    "debugger.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Shape Assertions and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_shape(tensor: torch.Tensor, expected_shape: tuple, name: str = \"tensor\"):\n",
    "    \"\"\"\n",
    "    Assert tensor has expected shape.\n",
    "    Use -1 for dimensions that can be any size.\n",
    "    \"\"\"\n",
    "    actual = tensor.shape\n",
    "    \n",
    "    if len(actual) != len(expected_shape):\n",
    "        raise AssertionError(\n",
    "            f\"{name}: Expected {len(expected_shape)} dims, got {len(actual)}. \"\n",
    "            f\"Shape: {tuple(actual)}\"\n",
    "        )\n",
    "    \n",
    "    for i, (a, e) in enumerate(zip(actual, expected_shape)):\n",
    "        if e != -1 and a != e:\n",
    "            raise AssertionError(\n",
    "                f\"{name}: Dimension {i} mismatch. \"\n",
    "                f\"Expected {expected_shape}, got {tuple(actual)}\"\n",
    "            )\n",
    "\n",
    "\n",
    "class DebuggedModel(nn.Module):\n",
    "    \"\"\"Model with shape assertions for debugging\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=10, hidden_dim=50, output_dim=5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Assert input shape (batch_size can be anything, hence -1)\n",
    "        assert_shape(x, (-1, self.input_dim), \"input\")\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        assert_shape(x, (-1, self.hidden_dim), \"after fc1\")\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        assert_shape(x, (-1, self.output_dim), \"output\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test\n",
    "model = DebuggedModel(input_dim=10, hidden_dim=50, output_dim=5)\n",
    "\n",
    "# Correct input\n",
    "x = torch.randn(32, 10)\n",
    "output = model(x)\n",
    "print(f\"Success! Output shape: {output.shape}\")\n",
    "\n",
    "# Wrong input\n",
    "try:\n",
    "    x_wrong = torch.randn(32, 15)  # Wrong input dimension\n",
    "    output = model(x_wrong)\n",
    "except AssertionError as e:\n",
    "    print(f\"Caught error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PyTorch Profiler\n",
    "\n",
    "The PyTorch Profiler helps identify performance bottlenecks.\n",
    "\n",
    "### 3.1 Basic Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and data for profiling\n",
    "\n",
    "class ModelForProfiling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = ModelForProfiling().to(device)\n",
    "x = torch.randn(32, 3, 64, 64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic profiling\n",
    "\n",
    "activities = [ProfilerActivity.CPU]\n",
    "if torch.cuda.is_available():\n",
    "    activities.append(ProfilerActivity.CUDA)\n",
    "\n",
    "with profile(\n",
    "    activities=activities,\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    for _ in range(10):\n",
    "        output = model(x)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "# Print results sorted by CPU time\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile with custom labels using record_function\n",
    "\n",
    "class LabeledModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with record_function(\"conv_block_1\"):\n",
    "            x = F.relu(self.conv1(x))\n",
    "        \n",
    "        with record_function(\"conv_block_2\"):\n",
    "            x = F.relu(self.conv2(x))\n",
    "        \n",
    "        with record_function(\"pooling_and_fc\"):\n",
    "            x = self.pool(x)\n",
    "            x = x.flatten(1)\n",
    "            x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "labeled_model = LabeledModel().to(device)\n",
    "\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    for _ in range(10):\n",
    "        output = labeled_model(x)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "# Filter for our custom labels\n",
    "print(\"\\nCustom region timing:\")\n",
    "for event in prof.key_averages():\n",
    "    if event.key in [\"conv_block_1\", \"conv_block_2\", \"pooling_and_fc\"]:\n",
    "        print(f\"{event.key}: {event.cpu_time_total / 1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Profiling Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "train_data = TensorDataset(\n",
    "    torch.randn(1000, 3, 64, 64),\n",
    "    torch.randint(0, 10, (1000,))\n",
    ")\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "model = ModelForProfiling().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_step(model, batch, optimizer, criterion):\n",
    "    x, y = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    with record_function(\"forward\"):\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "    \n",
    "    with record_function(\"backward\"):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "    with record_function(\"optimizer_step\"):\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Profile training\n",
    "with profile(\n",
    "    activities=activities,\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,     # Skip first batch (warmup)\n",
    "        warmup=1,   # Warmup profiler\n",
    "        active=3,   # Profile 3 batches\n",
    "        repeat=1\n",
    "    ),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    ") as prof:\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        if batch_idx >= 5:  # Only profile first 5 batches\n",
    "            break\n",
    "        loss = train_step(model, batch, optimizer, criterion)\n",
    "        prof.step()  # Signal profiler\n",
    "\n",
    "print(\"\\nTraining step breakdown:\")\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory profiling\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    print(\"GPU Memory Profiling:\")\n",
    "    print(f\"Initial allocated: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Create model and data\n",
    "    model = ModelForProfiling().cuda()\n",
    "    print(f\"After model: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    \n",
    "    x = torch.randn(64, 3, 64, 64, device='cuda')\n",
    "    print(f\"After input: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    print(f\"After forward: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Backward pass\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    print(f\"After backward: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    \n",
    "    print(f\"\\nPeak memory: {torch.cuda.max_memory_allocated() / 1e6:.1f} MB\")\n",
    "else:\n",
    "    print(\"GPU not available for memory profiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed memory breakdown\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        profile_memory=True,\n",
    "        record_shapes=True,\n",
    "    ) as prof:\n",
    "        model = ModelForProfiling().cuda()\n",
    "        x = torch.randn(64, 3, 64, 64, device='cuda')\n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "    \n",
    "    # Sort by memory usage\n",
    "    print(\"\\nOperations by CUDA memory:\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_memory_usage\", \n",
    "        row_limit=15\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. TensorBoard Integration\n",
    "\n",
    "### 4.1 Basic TensorBoard Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorBoard writer\n",
    "log_dir = '../runs/debug_demo'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "print(\"Run 'tensorboard --logdir=../runs' to view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log scalars (loss, accuracy, learning rate)\n",
    "\n",
    "# Simulate training\n",
    "for epoch in range(100):\n",
    "    # Fake metrics\n",
    "    train_loss = 1.0 / (epoch + 1) + 0.1 * np.random.randn()\n",
    "    val_loss = 1.2 / (epoch + 1) + 0.1 * np.random.randn()\n",
    "    accuracy = 1 - 1.0 / (epoch + 2)\n",
    "    lr = 0.001 * (0.95 ** epoch)\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "    writer.add_scalar('LearningRate', lr, epoch)\n",
    "\n",
    "print(\"Logged 100 epochs of training metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log histograms (weights, gradients, activations)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10)\n",
    ")\n",
    "\n",
    "# Log initial weights\n",
    "for name, param in model.named_parameters():\n",
    "    writer.add_histogram(f'weights/{name}', param, 0)\n",
    "\n",
    "# Simulate training and log weight evolution\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(50):\n",
    "    x = torch.randn(32, 10)\n",
    "    y = torch.randn(32, 10)\n",
    "    \n",
    "    output = model(x)\n",
    "    loss = F.mse_loss(output, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(f'weights/{name}', param, epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f'gradients/{name}', param.grad, epoch)\n",
    "\n",
    "print(\"Logged weight and gradient histograms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log model graph\n",
    "\n",
    "model = ModelForProfiling()\n",
    "x = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "writer.add_graph(model, x)\n",
    "print(\"Logged model graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log images\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load some MNIST images\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "mnist = MNIST('../data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create a grid of images\n",
    "images = torch.stack([mnist[i][0] for i in range(16)])\n",
    "grid = torchvision.utils.make_grid(images, nrow=4, normalize=True)\n",
    "\n",
    "writer.add_image('mnist_samples', grid, 0)\n",
    "print(\"Logged image grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log embeddings for visualization\n",
    "\n",
    "# Get embeddings from model\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 32),  # 32-dim embedding\n",
    ")\n",
    "\n",
    "# Get embeddings for 500 images\n",
    "n_samples = 500\n",
    "images = torch.stack([mnist[i][0] for i in range(n_samples)])\n",
    "labels = [mnist[i][1] for i in range(n_samples)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(images)\n",
    "\n",
    "# Log embeddings\n",
    "writer.add_embedding(\n",
    "    embeddings,\n",
    "    metadata=labels,\n",
    "    label_img=images,\n",
    "    global_step=0,\n",
    "    tag='mnist_embeddings'\n",
    ")\n",
    "\n",
    "print(\"Logged embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close writer\n",
    "writer.close()\n",
    "print(\"\\nTensorBoard logging complete!\")\n",
    "print(\"Run: tensorboard --logdir=../runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Comprehensive Training Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLogger:\n",
    "    \"\"\"\n",
    "    Comprehensive logger for training experiments.\n",
    "    Logs metrics, gradients, learning rate, and more to TensorBoard.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir: str, model: nn.Module = None):\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.model = model\n",
    "        self.step = 0\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def log_scalars(self, metrics: Dict[str, float], prefix: str = \"\"):\n",
    "        \"\"\"Log multiple scalar values\"\"\"\n",
    "        for name, value in metrics.items():\n",
    "            tag = f\"{prefix}/{name}\" if prefix else name\n",
    "            self.writer.add_scalar(tag, value, self.step)\n",
    "    \n",
    "    def log_gradients(self):\n",
    "        \"\"\"Log gradient statistics for all parameters\"\"\"\n",
    "        if self.model is None:\n",
    "            return\n",
    "        \n",
    "        total_norm = 0.0\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                total_norm += grad_norm ** 2\n",
    "                self.writer.add_scalar(f'gradients/norm/{name}', grad_norm, self.step)\n",
    "        \n",
    "        total_norm = total_norm ** 0.5\n",
    "        self.writer.add_scalar('gradients/total_norm', total_norm, self.step)\n",
    "    \n",
    "    def log_weights(self):\n",
    "        \"\"\"Log weight histograms\"\"\"\n",
    "        if self.model is None:\n",
    "            return\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            self.writer.add_histogram(f'weights/{name}', param, self.step)\n",
    "    \n",
    "    def log_learning_rate(self, optimizer):\n",
    "        \"\"\"Log learning rate from optimizer\"\"\"\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            self.writer.add_scalar(f'learning_rate/group_{i}', param_group['lr'], self.step)\n",
    "    \n",
    "    def log_images(self, tag: str, images: torch.Tensor, nrow: int = 8):\n",
    "        \"\"\"Log image grid\"\"\"\n",
    "        grid = torchvision.utils.make_grid(images, nrow=nrow, normalize=True)\n",
    "        self.writer.add_image(tag, grid, self.step)\n",
    "    \n",
    "    def step_batch(self):\n",
    "        \"\"\"Increment batch step counter\"\"\"\n",
    "        self.step += 1\n",
    "    \n",
    "    def step_epoch(self):\n",
    "        \"\"\"Increment epoch counter and log weights\"\"\"\n",
    "        self.epoch += 1\n",
    "        self.log_weights()\n",
    "    \n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "\n",
    "print(\"TrainingLogger class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Memory Debugging\n",
    "\n",
    "### 5.1 GPU Memory Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMemoryTracker:\n",
    "    \"\"\"\n",
    "    Track GPU memory usage during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.snapshots = []\n",
    "    \n",
    "    def snapshot(self, label: str = \"\"):\n",
    "        \"\"\"Take a memory snapshot\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "        \n",
    "        self.snapshots.append({\n",
    "            'label': label,\n",
    "            'allocated': torch.cuda.memory_allocated() / 1e6,\n",
    "            'reserved': torch.cuda.memory_reserved() / 1e6,\n",
    "            'max_allocated': torch.cuda.max_memory_allocated() / 1e6,\n",
    "        })\n",
    "    \n",
    "    def print_snapshots(self):\n",
    "        \"\"\"Print all snapshots\"\"\"\n",
    "        print(\"\\nGPU Memory Snapshots:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Label':<30} {'Allocated':>12} {'Reserved':>12} {'Peak':>12}\")\n",
    "        print(\"-\" * 70)\n",
    "        for snap in self.snapshots:\n",
    "            print(f\"{snap['label']:<30} {snap['allocated']:>10.1f}MB {snap['reserved']:>10.1f}MB {snap['max_allocated']:>10.1f}MB\")\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"Plot memory usage over time\"\"\"\n",
    "        if not self.snapshots:\n",
    "            print(\"No snapshots to plot\")\n",
    "            return\n",
    "        \n",
    "        labels = [s['label'] for s in self.snapshots]\n",
    "        allocated = [s['allocated'] for s in self.snapshots]\n",
    "        reserved = [s['reserved'] for s in self.snapshots]\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        x = range(len(labels))\n",
    "        plt.bar(x, reserved, alpha=0.5, label='Reserved')\n",
    "        plt.bar(x, allocated, alpha=0.8, label='Allocated')\n",
    "        plt.xticks(x, labels, rotation=45, ha='right')\n",
    "        plt.ylabel('Memory (MB)')\n",
    "        plt.title('GPU Memory Usage')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Demo\n",
    "if torch.cuda.is_available():\n",
    "    tracker = GPUMemoryTracker()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    tracker.snapshot(\"Initial\")\n",
    "    \n",
    "    model = ModelForProfiling().cuda()\n",
    "    tracker.snapshot(\"After model creation\")\n",
    "    \n",
    "    x = torch.randn(128, 3, 64, 64, device='cuda')\n",
    "    tracker.snapshot(\"After input tensor\")\n",
    "    \n",
    "    output = model(x)\n",
    "    tracker.snapshot(\"After forward pass\")\n",
    "    \n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    tracker.snapshot(\"After backward pass\")\n",
    "    \n",
    "    del x, output, loss\n",
    "    torch.cuda.empty_cache()\n",
    "    tracker.snapshot(\"After cleanup\")\n",
    "    \n",
    "    tracker.print_snapshots()\n",
    "    tracker.plot()\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Finding Memory Leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tensors_in_memory():\n",
    "    \"\"\"\n",
    "    Find all tensors currently in memory.\n",
    "    Useful for debugging memory leaks.\n",
    "    \"\"\"\n",
    "    tensors = []\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                tensors.append({\n",
    "                    'shape': tuple(obj.shape),\n",
    "                    'dtype': obj.dtype,\n",
    "                    'device': str(obj.device),\n",
    "                    'size_mb': obj.element_size() * obj.nelement() / 1e6,\n",
    "                    'requires_grad': obj.requires_grad,\n",
    "                    'has_grad': obj.grad is not None,\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return tensors\n",
    "\n",
    "\n",
    "def summarize_tensors(tensors):\n",
    "    \"\"\"Summarize tensor memory usage\"\"\"\n",
    "    total_size = sum(t['size_mb'] for t in tensors)\n",
    "    by_device = {}\n",
    "    \n",
    "    for t in tensors:\n",
    "        device = t['device']\n",
    "        if device not in by_device:\n",
    "            by_device[device] = {'count': 0, 'size_mb': 0}\n",
    "        by_device[device]['count'] += 1\n",
    "        by_device[device]['size_mb'] += t['size_mb']\n",
    "    \n",
    "    print(f\"\\nTotal tensors: {len(tensors)}\")\n",
    "    print(f\"Total size: {total_size:.2f} MB\")\n",
    "    print(\"\\nBy device:\")\n",
    "    for device, stats in by_device.items():\n",
    "        print(f\"  {device}: {stats['count']} tensors, {stats['size_mb']:.2f} MB\")\n",
    "\n",
    "\n",
    "# Demo\n",
    "print(\"Before creating tensors:\")\n",
    "tensors_before = find_tensors_in_memory()\n",
    "summarize_tensors(tensors_before)\n",
    "\n",
    "# Create some tensors\n",
    "a = torch.randn(1000, 1000)\n",
    "b = torch.randn(1000, 1000, requires_grad=True)\n",
    "c = a @ b\n",
    "\n",
    "print(\"\\nAfter creating tensors:\")\n",
    "tensors_after = find_tensors_in_memory()\n",
    "summarize_tensors(tensors_after)\n",
    "\n",
    "# Clean up\n",
    "del a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Gradient Checkpointing for Memory Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class MemoryEfficientModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model using gradient checkpointing to reduce memory usage.\n",
    "    Trades compute for memory - recomputes activations during backward.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_checkpointing: bool = False):\n",
    "        super().__init__()\n",
    "        self.use_checkpointing = use_checkpointing\n",
    "        \n",
    "        # Create multiple transformer-like blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024, 256),\n",
    "            )\n",
    "            for _ in range(8)\n",
    "        ])\n",
    "        \n",
    "        self.head = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            if self.use_checkpointing and self.training:\n",
    "                # Recompute activations during backward\n",
    "                x = checkpoint(block, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        \n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "# Compare memory usage\n",
    "if torch.cuda.is_available():\n",
    "    for use_checkpoint in [False, True]:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        model = MemoryEfficientModel(use_checkpointing=use_checkpoint).cuda()\n",
    "        x = torch.randn(64, 256, device='cuda')\n",
    "        \n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1e6\n",
    "        checkpoint_str = \"with\" if use_checkpoint else \"without\"\n",
    "        print(f\"Peak memory {checkpoint_str} checkpointing: {peak_memory:.1f} MB\")\n",
    "        \n",
    "        del model, x, output, loss\n",
    "else:\n",
    "    print(\"GPU not available for checkpointing demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Debug a Broken Training Loop\n",
    "\n",
    "Find and fix all bugs in the training loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Fix the bugs in this training loop\n",
    "\n",
    "def buggy_training():\n",
    "    # Setup\n",
    "    model = nn.Linear(10, 2)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Data\n",
    "    X = torch.randn(100, 10)\n",
    "    y = torch.randint(0, 2, (100,))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        # BUG 1: Something wrong with the forward pass\n",
    "        output = model(X.cuda())  # Hint: is model on CUDA?\n",
    "        \n",
    "        # BUG 2: Wrong loss function usage\n",
    "        loss = F.cross_entropy(output, y.float())  # Hint: check y dtype\n",
    "        \n",
    "        # BUG 3: Missing something before backward\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # BUG 4: Memory leak\n",
    "        losses.append(loss)  # Hint: computation graph?\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss = {loss}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# YOUR TASK: Fix all bugs and make this run correctly\n",
    "# Try running it first to see the errors!\n",
    "\n",
    "# try:\n",
    "#     losses = buggy_training()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Profile and Optimize\n",
    "\n",
    "Use the profiler to identify bottlenecks and suggest optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Profile this model and identify bottlenecks\n",
    "\n",
    "class SlowModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inefficient: creates many intermediate tensors\n",
    "        x = self.fc1(x)\n",
    "        x = x.cpu()  # Unnecessary device transfer!\n",
    "        x = F.relu(x)\n",
    "        x = x.cuda() if torch.cuda.is_available() else x\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = x.cpu()  # Another unnecessary transfer!\n",
    "        x = F.relu(x)\n",
    "        x = x.cuda() if torch.cuda.is_available() else x\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Profile this model using PyTorch Profiler\n",
    "# 2. Identify the bottlenecks\n",
    "# 3. Create an optimized version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Training Monitor\n",
    "\n",
    "Implement a training monitor that logs to both console and TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement a comprehensive training monitor\n",
    "\n",
    "class TrainingMonitor:\n",
    "    \"\"\"\n",
    "    Monitor training progress with:\n",
    "    - Loss tracking and early stopping detection\n",
    "    - Gradient norm monitoring\n",
    "    - NaN/Inf detection\n",
    "    - TensorBoard logging\n",
    "    - Console progress output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, patience: int = 5, log_dir: str = None):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def on_batch_end(self, loss: torch.Tensor, batch_idx: int):\n",
    "        \"\"\"Called after each training batch\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # - Check for NaN/Inf in loss\n",
    "        # - Log gradient norms\n",
    "        # - Update running average\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, val_loss: float, epoch: int):\n",
    "        \"\"\"Called after each epoch\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # - Check for early stopping\n",
    "        # - Log to TensorBoard\n",
    "        # - Print progress\n",
    "        pass\n",
    "    \n",
    "    def should_stop(self) -> bool:\n",
    "        \"\"\"Returns True if training should stop early\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Fixed training loop\n",
    "\n",
    "def fixed_training():\n",
    "    # Setup - keep everything on same device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = nn.Linear(10, 2).to(device)  # FIX 1: Move model to device\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Data\n",
    "    X = torch.randn(100, 10).to(device)  # Move data to device\n",
    "    y = torch.randint(0, 2, (100,)).to(device)  # FIX 2: Keep as long, move to device\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        output = model(X)\n",
    "        \n",
    "        # FIX 2: y should be Long tensor for cross_entropy\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        \n",
    "        # FIX 3: Zero gradients before backward!\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # FIX 4: Use .item() to avoid memory leak\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "losses = fixed_training()\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Optimized model\n",
    "\n",
    "class OptimizedModel(nn.Module):\n",
    "    \"\"\"Optimized version without unnecessary device transfers\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # No device transfers - stay on same device throughout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Compare performance\n",
    "if torch.cuda.is_available():\n",
    "    import time\n",
    "    \n",
    "    # Slow model\n",
    "    slow_model = SlowModel().cuda()\n",
    "    x = torch.randn(64, 1000, device='cuda')\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = slow_model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    slow_time = time.time() - start\n",
    "    \n",
    "    # Optimized model\n",
    "    fast_model = OptimizedModel().cuda()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = fast_model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    fast_time = time.time() - start\n",
    "    \n",
    "    print(f\"Slow model: {slow_time:.3f}s\")\n",
    "    print(f\"Optimized model: {fast_time:.3f}s\")\n",
    "    print(f\"Speedup: {slow_time / fast_time:.1f}x\")\n",
    "else:\n",
    "    print(\"GPU not available for benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Complete Training Monitor\n",
    "\n",
    "class TrainingMonitor:\n",
    "    \"\"\"\n",
    "    Comprehensive training monitor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, patience: int = 5, log_dir: str = None):\n",
    "        self.model = model\n",
    "        self.patience = patience\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.running_loss = 0.0\n",
    "        self.batch_count = 0\n",
    "        self.global_step = 0\n",
    "        \n",
    "        if log_dir:\n",
    "            self.writer = SummaryWriter(log_dir)\n",
    "        else:\n",
    "            self.writer = None\n",
    "    \n",
    "    def on_batch_end(self, loss: torch.Tensor, batch_idx: int):\n",
    "        \"\"\"Called after each training batch\"\"\"\n",
    "        loss_value = loss.item()\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if np.isnan(loss_value) or np.isinf(loss_value):\n",
    "            print(f\"WARNING: Loss is {loss_value} at batch {batch_idx}!\")\n",
    "            return False  # Signal to stop\n",
    "        \n",
    "        # Update running average\n",
    "        self.running_loss += loss_value\n",
    "        self.batch_count += 1\n",
    "        self.global_step += 1\n",
    "        \n",
    "        # Log gradient norms\n",
    "        total_norm = 0.0\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                total_norm += param.grad.norm().item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        \n",
    "        if total_norm > 100:\n",
    "            print(f\"WARNING: Large gradient norm ({total_norm:.1f}) at batch {batch_idx}\")\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        if self.writer:\n",
    "            self.writer.add_scalar('train/loss', loss_value, self.global_step)\n",
    "            self.writer.add_scalar('train/grad_norm', total_norm, self.global_step)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def on_epoch_end(self, val_loss: float, epoch: int):\n",
    "        \"\"\"Called after each epoch\"\"\"\n",
    "        avg_train_loss = self.running_loss / max(self.batch_count, 1)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        if self.writer:\n",
    "            self.writer.add_scalar('epoch/train_loss', avg_train_loss, epoch)\n",
    "            self.writer.add_scalar('epoch/val_loss', val_loss, epoch)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.patience_counter = 0\n",
    "            print(f\"  New best validation loss!\")\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            print(f\"  No improvement ({self.patience_counter}/{self.patience})\")\n",
    "        \n",
    "        # Reset running stats\n",
    "        self.running_loss = 0.0\n",
    "        self.batch_count = 0\n",
    "    \n",
    "    def should_stop(self) -> bool:\n",
    "        \"\"\"Returns True if training should stop early\"\"\"\n",
    "        return self.patience_counter >= self.patience\n",
    "    \n",
    "    def close(self):\n",
    "        if self.writer:\n",
    "            self.writer.close()\n",
    "\n",
    "\n",
    "# Test the monitor\n",
    "model = nn.Linear(10, 2)\n",
    "monitor = TrainingMonitor(model, patience=3)\n",
    "\n",
    "# Simulate training\n",
    "for epoch in range(10):\n",
    "    # Simulate batches\n",
    "    for batch_idx in range(5):\n",
    "        fake_loss = torch.tensor(1.0 / (epoch + 1) + 0.1 * np.random.randn())\n",
    "        monitor.on_batch_end(fake_loss, batch_idx)\n",
    "    \n",
    "    # Simulate validation\n",
    "    val_loss = 1.0 / (epoch + 1) + 0.15  # Slightly worse than train\n",
    "    monitor.on_epoch_end(val_loss, epoch)\n",
    "    \n",
    "    if monitor.should_stop():\n",
    "        print(\"\\nEarly stopping triggered!\")\n",
    "        break\n",
    "\n",
    "monitor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Common Bugs**:\n",
    "   - Shape mismatches: Always check tensor dimensions\n",
    "   - Device mismatches: Keep all tensors on the same device\n",
    "   - NaN/Inf values: Use numerical stability techniques\n",
    "   - Memory leaks: Use `.item()` for scalars, `torch.no_grad()` for eval\n",
    "\n",
    "2. **Debugging Tools**:\n",
    "   - `torch.autograd.detect_anomaly()` for gradient issues\n",
    "   - Gradient hooks for monitoring weight updates\n",
    "   - Shape assertions for catching dimension errors early\n",
    "\n",
    "3. **Profiling**:\n",
    "   - PyTorch Profiler for CPU/GPU time and memory\n",
    "   - `record_function()` for custom region labeling\n",
    "   - Memory snapshots for tracking allocation\n",
    "\n",
    "4. **TensorBoard**:\n",
    "   - Log scalars, histograms, images, graphs\n",
    "   - Track gradients and weights over time\n",
    "   - Visualize embeddings\n",
    "\n",
    "5. **Memory Management**:\n",
    "   - Use `torch.cuda.memory_allocated()` to track usage\n",
    "   - Gradient checkpointing trades compute for memory\n",
    "   - `gc.collect()` and `torch.cuda.empty_cache()` for cleanup\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Check for NaN/Inf\n",
    "torch.isnan(tensor).any()\n",
    "torch.isinf(tensor).any()\n",
    "\n",
    "# Detect anomalies in backward\n",
    "with torch.autograd.detect_anomaly():\n",
    "    loss.backward()\n",
    "\n",
    "# Profile code\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    model(x)\n",
    "print(prof.key_averages().table())\n",
    "\n",
    "# TensorBoard\n",
    "writer = SummaryWriter('runs/exp')\n",
    "writer.add_scalar('loss', value, step)\n",
    "\n",
    "# GPU memory\n",
    "torch.cuda.memory_allocated()  # Current\n",
    "torch.cuda.max_memory_allocated()  # Peak\n",
    "torch.cuda.empty_cache()  # Free cached memory\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
