{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 14: Diffusion Models\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the forward and reverse diffusion processes\n",
    "2. Implement a noise scheduler and the training objective\n",
    "3. Build a U-Net architecture for denoising\n",
    "4. Train a diffusion model on MNIST\n",
    "5. Implement sampling algorithms (DDPM, DDIM)\n",
    "\n",
    "**Prerequisites**: Notebooks 01-04, 07 (CNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Diffusion Process Overview\n",
    "\n",
    "Diffusion models work in two phases:\n",
    "\n",
    "### Forward Process (Fixed)\n",
    "Gradually add noise to data over T timesteps until it becomes pure noise:\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$$\n",
    "\n",
    "### Reverse Process (Learned)\n",
    "Learn to denoise step by step:\n",
    "$$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)$$\n",
    "\n",
    "```\n",
    "Forward:  x_0 (data) → x_1 → x_2 → ... → x_T (noise)\n",
    "Reverse:  x_T (noise) → x_{T-1} → ... → x_0 (generated data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Scale to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Noise Schedule\n",
    "\n",
    "The noise schedule $\\{\\beta_t\\}_{t=1}^T$ controls how much noise is added at each step.\n",
    "\n",
    "Key quantities:\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "- $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$ (cumulative product)\n",
    "\n",
    "With these, we can sample $x_t$ directly from $x_0$:\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    \"\"\"Manages the noise schedule for diffusion.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02, device='cpu'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # Linear schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, device=device)\n",
    "        \n",
    "        # Precompute useful quantities\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # For q(x_t | x_0)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        \n",
    "        # For posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "    \n",
    "    def add_noise(self, x_0, t, noise=None):\n",
    "        \"\"\"Add noise to x_0 to get x_t: q(x_t | x_0)\"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        \n",
    "        return sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise\n",
    "    \n",
    "    def sample_timesteps(self, batch_size):\n",
    "        \"\"\"Sample random timesteps.\"\"\"\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,), device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the noise schedule\n",
    "scheduler = NoiseScheduler(num_timesteps=1000, device=device)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "axes[0].plot(scheduler.betas.cpu())\n",
    "axes[0].set_title('β_t (noise added per step)')\n",
    "axes[0].set_xlabel('Timestep')\n",
    "\n",
    "axes[1].plot(scheduler.alphas_cumprod.cpu())\n",
    "axes[1].set_title('ᾱ_t (signal remaining)')\n",
    "axes[1].set_xlabel('Timestep')\n",
    "\n",
    "axes[2].plot(scheduler.sqrt_one_minus_alphas_cumprod.cpu())\n",
    "axes[2].set_title('√(1-ᾱ_t) (noise level)')\n",
    "axes[2].set_xlabel('Timestep')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forward diffusion process\n",
    "x_0, _ = next(iter(train_loader))\n",
    "x_0 = x_0[:1].to(device)  # Single image\n",
    "\n",
    "timesteps = [0, 50, 100, 200, 400, 600, 800, 999]\n",
    "fig, axes = plt.subplots(1, len(timesteps), figsize=(16, 2))\n",
    "\n",
    "for ax, t in zip(axes, timesteps):\n",
    "    t_tensor = torch.tensor([t], device=device)\n",
    "    x_t = scheduler.add_noise(x_0, t_tensor)\n",
    "    img = (x_t[0, 0].cpu() + 1) / 2  # Denormalize\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f't={t}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Forward Diffusion: Adding Noise Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. U-Net Architecture\n",
    "\n",
    "The denoising network predicts the noise $\\epsilon$ given $x_t$ and $t$.\n",
    "\n",
    "U-Net structure:\n",
    "- **Encoder**: Downsample with residual blocks\n",
    "- **Bottleneck**: Process at lowest resolution\n",
    "- **Decoder**: Upsample with skip connections\n",
    "- **Time embedding**: Inject timestep information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"Sinusoidal embeddings for timesteps (like in Transformers).\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = t[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Residual block with time embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        \n",
    "        if in_ch != out_ch:\n",
    "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        h = F.silu(h)\n",
    "        \n",
    "        # Add time embedding\n",
    "        h = h + self.time_mlp(t_emb)[:, :, None, None]\n",
    "        \n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        \n",
    "        return h + self.shortcut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"Simplified U-Net for MNIST (28x28).\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv_in = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "        \n",
    "        self.down1 = ResBlock(base_channels, base_channels, time_emb_dim)\n",
    "        self.down2 = ResBlock(base_channels, base_channels * 2, time_emb_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bot1 = ResBlock(base_channels * 2, base_channels * 4, time_emb_dim)\n",
    "        self.bot2 = ResBlock(base_channels * 4, base_channels * 2, time_emb_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up1 = ResBlock(base_channels * 4, base_channels, time_emb_dim)  # Skip connection doubles channels\n",
    "        self.up2 = ResBlock(base_channels * 2, base_channels, time_emb_dim)\n",
    "        \n",
    "        self.conv_out = nn.Conv2d(base_channels, in_channels, 1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # Time embedding\n",
    "        t_emb = self.time_mlp(t)\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.conv_in(x)           # 28x28\n",
    "        x1 = self.down1(x1, t_emb)     # 28x28\n",
    "        \n",
    "        x2 = self.pool(x1)             # 14x14\n",
    "        x2 = self.down2(x2, t_emb)     # 14x14\n",
    "        \n",
    "        # Bottleneck\n",
    "        x3 = self.pool(x2)             # 7x7\n",
    "        x3 = self.bot1(x3, t_emb)\n",
    "        x3 = self.bot2(x3, t_emb)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.up(x3)                # 14x14\n",
    "        x = torch.cat([x, x2], dim=1)  # Skip connection\n",
    "        x = self.up1(x, t_emb)\n",
    "        \n",
    "        x = self.up(x)                 # 28x28\n",
    "        x = torch.cat([x, x1], dim=1)  # Skip connection\n",
    "        x = self.up2(x, t_emb)\n",
    "        \n",
    "        return self.conv_out(x)\n",
    "\n",
    "# Test\n",
    "model = SimpleUNet().to(device)\n",
    "x_test = torch.randn(4, 1, 28, 28).to(device)\n",
    "t_test = torch.randint(0, 1000, (4,)).to(device)\n",
    "out = model(x_test, t_test)\n",
    "print(f\"Input: {x_test.shape}, Output: {out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training: Predict the Noise\n",
    "\n",
    "The training objective is to predict the noise that was added:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]$$\n",
    "\n",
    "Training loop:\n",
    "1. Sample $x_0$ from data\n",
    "2. Sample $t \\sim \\text{Uniform}(1, T)$\n",
    "3. Sample $\\epsilon \\sim \\mathcal{N}(0, I)$\n",
    "4. Compute $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon$\n",
    "5. Predict $\\hat{\\epsilon} = \\epsilon_\\theta(x_t, t)$\n",
    "6. Loss = $\\|\\epsilon - \\hat{\\epsilon}\\|^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion(model, scheduler, train_loader, epochs=10, lr=1e-3):\n",
    "    \"\"\"Train diffusion model.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for x_0, _ in pbar:\n",
    "            x_0 = x_0.to(device)\n",
    "            batch_size = x_0.size(0)\n",
    "            \n",
    "            # Sample timesteps\n",
    "            t = scheduler.sample_timesteps(batch_size)\n",
    "            \n",
    "            # Sample noise\n",
    "            noise = torch.randn_like(x_0)\n",
    "            \n",
    "            # Get noisy images\n",
    "            x_t = scheduler.add_noise(x_0, t, noise)\n",
    "            \n",
    "            # Predict noise\n",
    "            predicted_noise = model(x_t, t)\n",
    "            \n",
    "            # MSE loss\n",
    "            loss = F.mse_loss(predicted_noise, noise)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = SimpleUNet(base_channels=64).to(device)\n",
    "scheduler = NoiseScheduler(num_timesteps=1000, device=device)\n",
    "\n",
    "losses = train_diffusion(model, scheduler, train_loader, epochs=15, lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Diffusion Model Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Sampling: DDPM\n",
    "\n",
    "DDPM (Denoising Diffusion Probabilistic Models) sampling:\n",
    "\n",
    "Starting from $x_T \\sim \\mathcal{N}(0, I)$, iteratively denoise:\n",
    "\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z$$\n",
    "\n",
    "where $z \\sim \\mathcal{N}(0, I)$ and $\\sigma_t = \\sqrt{\\beta_t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_ddpm(model, scheduler, n_samples=16, img_size=28, channels=1):\n",
    "    \"\"\"Generate samples using DDPM sampling.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from pure noise\n",
    "    x = torch.randn(n_samples, channels, img_size, img_size, device=device)\n",
    "    \n",
    "    # Reverse diffusion\n",
    "    for t in tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Sampling\"):\n",
    "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = model(x, t_batch)\n",
    "        \n",
    "        # Get scheduler parameters\n",
    "        alpha = scheduler.alphas[t]\n",
    "        alpha_cumprod = scheduler.alphas_cumprod[t]\n",
    "        beta = scheduler.betas[t]\n",
    "        \n",
    "        # Compute x_{t-1}\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma = torch.sqrt(beta)\n",
    "        else:\n",
    "            noise = torch.zeros_like(x)\n",
    "            sigma = 0\n",
    "        \n",
    "        x = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise)\n",
    "        x = x + sigma * noise\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    x = torch.clamp(x, -1, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "samples = sample_ddpm(model, scheduler, n_samples=16)\n",
    "\n",
    "# Visualize\n",
    "samples = (samples + 1) / 2  # Denormalize to [0, 1]\n",
    "grid = make_grid(samples, nrow=4, padding=2)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.cpu().permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('DDPM Generated Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the denoising process\n",
    "@torch.no_grad()\n",
    "def visualize_denoising(model, scheduler, save_every=100):\n",
    "    \"\"\"Visualize the reverse diffusion process.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.randn(1, 1, 28, 28, device=device)\n",
    "    images = [x.clone()]\n",
    "    timesteps_saved = [scheduler.num_timesteps]\n",
    "    \n",
    "    for t in reversed(range(scheduler.num_timesteps)):\n",
    "        t_batch = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "        predicted_noise = model(x, t_batch)\n",
    "        \n",
    "        alpha = scheduler.alphas[t]\n",
    "        alpha_cumprod = scheduler.alphas_cumprod[t]\n",
    "        beta = scheduler.betas[t]\n",
    "        \n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma = torch.sqrt(beta)\n",
    "        else:\n",
    "            noise = torch.zeros_like(x)\n",
    "            sigma = 0\n",
    "        \n",
    "        x = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise)\n",
    "        x = x + sigma * noise\n",
    "        \n",
    "        if t % save_every == 0 or t == 0:\n",
    "            images.append(x.clone())\n",
    "            timesteps_saved.append(t)\n",
    "    \n",
    "    return images, timesteps_saved\n",
    "\n",
    "images, timesteps = visualize_denoising(model, scheduler, save_every=100)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(16, 2))\n",
    "for ax, img, t in zip(axes, images, timesteps):\n",
    "    img = (img[0, 0].cpu() + 1) / 2\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f't={t}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Reverse Diffusion: Denoising Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Faster Sampling: DDIM\n",
    "\n",
    "DDPM requires T steps (e.g., 1000). DDIM (Denoising Diffusion Implicit Models) enables faster sampling by skipping steps.\n",
    "\n",
    "DDIM is deterministic (no noise added during sampling):\n",
    "$$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\hat{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\cdot \\epsilon_\\theta(x_t, t)$$\n",
    "\n",
    "where $\\hat{x}_0 = \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t} \\epsilon_\\theta(x_t, t)}{\\sqrt{\\bar{\\alpha}_t}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_ddim(model, scheduler, n_samples=16, num_inference_steps=50, \n",
    "                img_size=28, channels=1):\n",
    "    \"\"\"Generate samples using DDIM (faster, deterministic).\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create timestep schedule (subset of training timesteps)\n",
    "    step_size = scheduler.num_timesteps // num_inference_steps\n",
    "    timesteps = list(range(0, scheduler.num_timesteps, step_size))\n",
    "    timesteps = list(reversed(timesteps))\n",
    "    \n",
    "    # Start from pure noise\n",
    "    x = torch.randn(n_samples, channels, img_size, img_size, device=device)\n",
    "    \n",
    "    for i, t in enumerate(tqdm(timesteps, desc=\"DDIM Sampling\")):\n",
    "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = model(x, t_batch)\n",
    "        \n",
    "        # Get alpha values\n",
    "        alpha_cumprod_t = scheduler.alphas_cumprod[t]\n",
    "        \n",
    "        if i + 1 < len(timesteps):\n",
    "            t_prev = timesteps[i + 1]\n",
    "            alpha_cumprod_t_prev = scheduler.alphas_cumprod[t_prev]\n",
    "        else:\n",
    "            alpha_cumprod_t_prev = torch.tensor(1.0, device=device)\n",
    "        \n",
    "        # Predict x_0\n",
    "        pred_x0 = (x - torch.sqrt(1 - alpha_cumprod_t) * predicted_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "        pred_x0 = torch.clamp(pred_x0, -1, 1)\n",
    "        \n",
    "        # DDIM update (deterministic)\n",
    "        x = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + \\\n",
    "            torch.sqrt(1 - alpha_cumprod_t_prev) * predicted_noise\n",
    "    \n",
    "    return torch.clamp(x, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DDPM (1000 steps) vs DDIM (50 steps)\n",
    "import time\n",
    "\n",
    "# DDIM sampling (fast)\n",
    "start = time.time()\n",
    "samples_ddim = sample_ddim(model, scheduler, n_samples=16, num_inference_steps=50)\n",
    "ddim_time = time.time() - start\n",
    "\n",
    "print(f\"DDIM (50 steps): {ddim_time:.2f}s\")\n",
    "\n",
    "# Visualize\n",
    "samples_ddim = (samples_ddim + 1) / 2\n",
    "grid = make_grid(samples_ddim, nrow=4, padding=2)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.cpu().permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(f'DDIM Samples (50 steps, {ddim_time:.1f}s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare quality at different step counts\n",
    "step_counts = [10, 25, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(step_counts), figsize=(16, 4))\n",
    "\n",
    "for ax, steps in zip(axes, step_counts):\n",
    "    samples = sample_ddim(model, scheduler, n_samples=4, num_inference_steps=steps)\n",
    "    samples = (samples + 1) / 2\n",
    "    grid = make_grid(samples, nrow=2, padding=1)\n",
    "    ax.imshow(grid.cpu().permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "    ax.set_title(f'{steps} steps')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('DDIM: Quality vs Number of Steps')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Cosine Schedule (Improved)\n",
    "\n",
    "The linear schedule degrades images too quickly. A cosine schedule provides smoother noise addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineNoiseScheduler(NoiseScheduler):\n",
    "    \"\"\"Cosine noise schedule (from 'Improved DDPM').\"\"\"\n",
    "    \n",
    "    def __init__(self, num_timesteps=1000, s=0.008, device='cpu'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # Cosine schedule\n",
    "        steps = torch.linspace(0, num_timesteps, num_timesteps + 1, device=device)\n",
    "        alphas_cumprod = torch.cos(((steps / num_timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        \n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        self.betas = torch.clamp(betas, 0.0001, 0.9999)\n",
    "        \n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.posterior_variance = self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "\n",
    "# Compare schedules\n",
    "linear_sched = NoiseScheduler(1000, device=device)\n",
    "cosine_sched = CosineNoiseScheduler(1000, device=device)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(linear_sched.alphas_cumprod.cpu(), label='Linear')\n",
    "plt.plot(cosine_sched.alphas_cumprod.cpu(), label='Cosine')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('ᾱ_t (signal remaining)')\n",
    "plt.title('Linear vs Cosine Schedule')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Forward Process** | Add noise gradually: $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon$ |\n",
    "| **Reverse Process** | Learn to denoise step by step |\n",
    "| **Training Objective** | Predict the noise: $\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2$ |\n",
    "| **U-Net** | Encoder-decoder with skip connections and time embedding |\n",
    "| **DDPM Sampling** | Stochastic, requires T steps |\n",
    "| **DDIM Sampling** | Deterministic, can skip steps |\n",
    "\n",
    "### Diffusion vs GANs vs VAEs\n",
    "\n",
    "| Aspect | Diffusion | GAN | VAE |\n",
    "|--------|-----------|-----|-----|\n",
    "| Training | Stable | Unstable | Stable |\n",
    "| Quality | Excellent | Excellent | Good (blurry) |\n",
    "| Speed | Slow sampling | Fast | Fast |\n",
    "| Mode coverage | Good | Mode collapse risk | Good |\n",
    "\n",
    "### Modern Extensions\n",
    "- **Latent Diffusion (Stable Diffusion)**: Run diffusion in VAE latent space\n",
    "- **Classifier-Free Guidance**: Condition generation without separate classifier\n",
    "- **ControlNet**: Add spatial conditioning (edges, poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Conditional Diffusion\n",
    "Modify the model to condition on class labels for generating specific digits.\n",
    "\n",
    "### Exercise 2: Cosine Schedule Training\n",
    "Retrain with the cosine schedule and compare sample quality.\n",
    "\n",
    "### Exercise 3: Interpolation in Noise Space\n",
    "Implement spherical interpolation (slerp) between two noise vectors and generate the interpolated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Conditional Diffusion\n",
    "class ConditionalUNet(nn.Module):\n",
    "    \"\"\"U-Net with class conditioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time + class embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        self.class_emb = nn.Embedding(num_classes, time_emb_dim)\n",
    "        \n",
    "        # Same architecture as before\n",
    "        self.conv_in = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "        self.down1 = ResBlock(base_channels, base_channels, time_emb_dim)\n",
    "        self.down2 = ResBlock(base_channels, base_channels * 2, time_emb_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bot1 = ResBlock(base_channels * 2, base_channels * 4, time_emb_dim)\n",
    "        self.bot2 = ResBlock(base_channels * 4, base_channels * 2, time_emb_dim)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up1 = ResBlock(base_channels * 4, base_channels, time_emb_dim)\n",
    "        self.up2 = ResBlock(base_channels * 2, base_channels, time_emb_dim)\n",
    "        self.conv_out = nn.Conv2d(base_channels, in_channels, 1)\n",
    "    \n",
    "    def forward(self, x, t, y):\n",
    "        # Combine time and class embeddings\n",
    "        t_emb = self.time_mlp(t) + self.class_emb(y)\n",
    "        \n",
    "        x1 = self.conv_in(x)\n",
    "        x1 = self.down1(x1, t_emb)\n",
    "        x2 = self.pool(x1)\n",
    "        x2 = self.down2(x2, t_emb)\n",
    "        x3 = self.pool(x2)\n",
    "        x3 = self.bot1(x3, t_emb)\n",
    "        x3 = self.bot2(x3, t_emb)\n",
    "        x = self.up(x3)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.up1(x, t_emb)\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up2(x, t_emb)\n",
    "        return self.conv_out(x)\n",
    "\n",
    "# Train conditional model\n",
    "cond_model = ConditionalUNet(base_channels=64).to(device)\n",
    "optimizer = torch.optim.Adam(cond_model.parameters(), lr=2e-4)\n",
    "\n",
    "print(\"Training conditional diffusion model...\")\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    for x_0, y in train_loader:\n",
    "        x_0, y = x_0.to(device), y.to(device)\n",
    "        t = scheduler.sample_timesteps(x_0.size(0))\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t = scheduler.add_noise(x_0, t, noise)\n",
    "        \n",
    "        predicted_noise = cond_model(x_t, t, y)\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Generate specific digits\n",
    "@torch.no_grad()\n",
    "def sample_conditional(model, scheduler, labels, num_steps=50):\n",
    "    model.eval()\n",
    "    n = len(labels)\n",
    "    x = torch.randn(n, 1, 28, 28, device=device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    step_size = scheduler.num_timesteps // num_steps\n",
    "    timesteps = list(reversed(range(0, scheduler.num_timesteps, step_size)))\n",
    "    \n",
    "    for i, t in enumerate(timesteps):\n",
    "        t_batch = torch.full((n,), t, device=device, dtype=torch.long)\n",
    "        pred_noise = model(x, t_batch, labels)\n",
    "        \n",
    "        alpha_cumprod_t = scheduler.alphas_cumprod[t]\n",
    "        alpha_cumprod_t_prev = scheduler.alphas_cumprod[timesteps[i+1]] if i+1 < len(timesteps) else torch.tensor(1.0)\n",
    "        \n",
    "        pred_x0 = (x - torch.sqrt(1 - alpha_cumprod_t) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "        pred_x0 = torch.clamp(pred_x0, -1, 1)\n",
    "        x = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + torch.sqrt(1 - alpha_cumprod_t_prev) * pred_noise\n",
    "    \n",
    "    return torch.clamp(x, -1, 1)\n",
    "\n",
    "# Generate each digit\n",
    "labels = torch.arange(10).repeat(2)  # 2 samples per digit\n",
    "samples = sample_conditional(cond_model, scheduler, labels, num_steps=50)\n",
    "samples = (samples + 1) / 2\n",
    "\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(20):\n",
    "    ax = axes[i // 10, i % 10]\n",
    "    ax.imshow(samples[i, 0].cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    if i < 10:\n",
    "        ax.set_title(str(i))\n",
    "plt.suptitle('Conditional Generation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Cosine Schedule Training\n",
    "cosine_scheduler = CosineNoiseScheduler(num_timesteps=1000, device=device)\n",
    "cosine_model = SimpleUNet(base_channels=64).to(device)\n",
    "\n",
    "print(\"Training with cosine schedule...\")\n",
    "cosine_losses = train_diffusion(cosine_model, cosine_scheduler, train_loader, epochs=10, lr=2e-4)\n",
    "\n",
    "# Compare samples\n",
    "samples_cosine = sample_ddim(cosine_model, cosine_scheduler, n_samples=16, num_inference_steps=50)\n",
    "samples_cosine = (samples_cosine + 1) / 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Linear schedule samples (from earlier)\n",
    "samples_linear = sample_ddim(model, scheduler, n_samples=16, num_inference_steps=50)\n",
    "samples_linear = (samples_linear + 1) / 2\n",
    "\n",
    "grid_linear = make_grid(samples_linear, nrow=4, padding=2)\n",
    "grid_cosine = make_grid(samples_cosine, nrow=4, padding=2)\n",
    "\n",
    "axes[0].imshow(grid_linear.cpu().permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "axes[0].set_title('Linear Schedule')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(grid_cosine.cpu().permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "axes[1].set_title('Cosine Schedule')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Spherical Interpolation (Slerp)\n",
    "def slerp(z1, z2, alpha):\n",
    "    \"\"\"Spherical linear interpolation between two noise vectors.\"\"\"\n",
    "    z1_norm = z1 / z1.norm(dim=[1,2,3], keepdim=True)\n",
    "    z2_norm = z2 / z2.norm(dim=[1,2,3], keepdim=True)\n",
    "    \n",
    "    # Compute angle\n",
    "    dot = (z1_norm * z2_norm).sum(dim=[1,2,3], keepdim=True).clamp(-1, 1)\n",
    "    omega = torch.acos(dot)\n",
    "    \n",
    "    # Slerp formula\n",
    "    so = torch.sin(omega)\n",
    "    z = (torch.sin((1 - alpha) * omega) / so) * z1 + (torch.sin(alpha * omega) / so) * z2\n",
    "    \n",
    "    return z\n",
    "\n",
    "@torch.no_grad()\n",
    "def interpolate_noise(model, scheduler, n_interp=10, num_steps=50):\n",
    "    \"\"\"Generate interpolated samples between two noise vectors.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Two random noise vectors\n",
    "    z1 = torch.randn(1, 1, 28, 28, device=device)\n",
    "    z2 = torch.randn(1, 1, 28, 28, device=device)\n",
    "    \n",
    "    images = []\n",
    "    alphas = torch.linspace(0, 1, n_interp)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        # Interpolate in noise space\n",
    "        z = slerp(z1, z2, alpha)\n",
    "        \n",
    "        # Denoise\n",
    "        x = z.clone()\n",
    "        step_size = scheduler.num_timesteps // num_steps\n",
    "        timesteps = list(reversed(range(0, scheduler.num_timesteps, step_size)))\n",
    "        \n",
    "        for i, t in enumerate(timesteps):\n",
    "            t_batch = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "            pred_noise = model(x, t_batch)\n",
    "            \n",
    "            alpha_cumprod_t = scheduler.alphas_cumprod[t]\n",
    "            alpha_cumprod_t_prev = scheduler.alphas_cumprod[timesteps[i+1]] if i+1 < len(timesteps) else torch.tensor(1.0)\n",
    "            \n",
    "            pred_x0 = (x - torch.sqrt(1 - alpha_cumprod_t) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "            pred_x0 = torch.clamp(pred_x0, -1, 1)\n",
    "            x = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + torch.sqrt(1 - alpha_cumprod_t_prev) * pred_noise\n",
    "        \n",
    "        images.append(torch.clamp(x, -1, 1))\n",
    "    \n",
    "    return torch.cat(images)\n",
    "\n",
    "# Generate interpolation\n",
    "interp_samples = interpolate_noise(model, scheduler, n_interp=10, num_steps=50)\n",
    "interp_samples = (interp_samples + 1) / 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 1.5))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(interp_samples[i, 0].cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Spherical Interpolation in Noise Space')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
