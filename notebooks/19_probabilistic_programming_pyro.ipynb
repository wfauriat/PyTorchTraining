{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 19: Probabilistic Programming with Pyro\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand Pyro's core primitives (`sample`, `param`, `plate`)\n",
    "2. Build probabilistic models with the model/guide pattern\n",
    "3. Perform variational inference with SVI\n",
    "4. Implement a proper Bayesian Neural Network\n",
    "5. Compare Pyro BNNs to MC Dropout and Ensembles\n",
    "\n",
    "**Prerequisites**: Notebook 18 (Uncertainty Quantification), basic probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pyro if needed\n",
    "# !pip install pyro-ppl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from pyro.optim import Adam\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "import pyro.infer.autoguide as autoguide\n",
    "\n",
    "# Clear param store (important for re-running cells)\n",
    "pyro.clear_param_store()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Pyro version: {pyro.__version__}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "pyro.set_rng_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Pyro Fundamentals\n",
    "\n",
    "Pyro extends PyTorch with probabilistic primitives:\n",
    "\n",
    "| Primitive | Purpose |\n",
    "|-----------|--------|\n",
    "| `pyro.sample` | Draw from a distribution (random variable) |\n",
    "| `pyro.param` | Learnable parameter (like `nn.Parameter`) |\n",
    "| `pyro.plate` | Declare independent observations (vectorization) |\n",
    "\n",
    "### The Model/Guide Pattern\n",
    "\n",
    "- **Model**: Defines the generative process (prior + likelihood)\n",
    "- **Guide**: Defines the approximate posterior (what we learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic example: sampling from distributions\n",
    "def simple_model():\n",
    "    # Sample from a prior\n",
    "    mu = pyro.sample(\"mu\", dist.Normal(0, 1))\n",
    "    sigma = pyro.sample(\"sigma\", dist.LogNormal(0, 1))\n",
    "    \n",
    "    # Sample observation\n",
    "    obs = pyro.sample(\"obs\", dist.Normal(mu, sigma))\n",
    "    return obs\n",
    "\n",
    "# Run the model (prior sampling)\n",
    "samples = [simple_model() for _ in range(1000)]\n",
    "print(f\"Mean of samples: {np.mean(samples):.3f}\")\n",
    "print(f\"Std of samples: {np.std(samples):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditioning on observations\n",
    "def model_with_obs(data):\n",
    "    mu = pyro.sample(\"mu\", dist.Normal(0, 10))  # Prior on mean\n",
    "    sigma = pyro.sample(\"sigma\", dist.LogNormal(0, 1))  # Prior on std\n",
    "    \n",
    "    # Plate for independent observations\n",
    "    with pyro.plate(\"data\", len(data)):\n",
    "        pyro.sample(\"obs\", dist.Normal(mu, sigma), obs=data)\n",
    "\n",
    "# Generate some data\n",
    "true_mu, true_sigma = 5.0, 2.0\n",
    "data = torch.randn(100) * true_sigma + true_mu\n",
    "\n",
    "print(f\"Data mean: {data.mean():.3f}, Data std: {data.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Bayesian Linear Regression\n",
    "\n",
    "Let's start with a simple example: inferring the slope and intercept of a line.\n",
    "\n",
    "$$y = wx + b + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "def generate_linear_data(n=100, w_true=2.5, b_true=-1.0, noise_std=0.5):\n",
    "    x = torch.linspace(-2, 2, n)\n",
    "    y = w_true * x + b_true + torch.randn(n) * noise_std\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = generate_linear_data(n=50)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_train, y_train, alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x, y=None):\n",
    "    \"\"\"Bayesian linear regression model.\"\"\"\n",
    "    # Priors\n",
    "    w = pyro.sample(\"w\", dist.Normal(0, 5))\n",
    "    b = pyro.sample(\"b\", dist.Normal(0, 5))\n",
    "    sigma = pyro.sample(\"sigma\", dist.LogNormal(0, 1))\n",
    "    \n",
    "    # Likelihood\n",
    "    mean = w * x + b\n",
    "    with pyro.plate(\"data\", len(x)):\n",
    "        pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n",
    "    \n",
    "    return mean\n",
    "\n",
    "\n",
    "def linear_guide(x, y=None):\n",
    "    \"\"\"Variational guide (approximate posterior).\"\"\"\n",
    "    # Learnable parameters for q(w), q(b), q(sigma)\n",
    "    w_loc = pyro.param(\"w_loc\", torch.tensor(0.0))\n",
    "    w_scale = pyro.param(\"w_scale\", torch.tensor(1.0), constraint=dist.constraints.positive)\n",
    "    \n",
    "    b_loc = pyro.param(\"b_loc\", torch.tensor(0.0))\n",
    "    b_scale = pyro.param(\"b_scale\", torch.tensor(1.0), constraint=dist.constraints.positive)\n",
    "    \n",
    "    sigma_loc = pyro.param(\"sigma_loc\", torch.tensor(0.0))\n",
    "    sigma_scale = pyro.param(\"sigma_scale\", torch.tensor(0.5), constraint=dist.constraints.positive)\n",
    "    \n",
    "    # Sample from approximate posterior\n",
    "    pyro.sample(\"w\", dist.Normal(w_loc, w_scale))\n",
    "    pyro.sample(\"b\", dist.Normal(b_loc, b_scale))\n",
    "    pyro.sample(\"sigma\", dist.LogNormal(sigma_loc, sigma_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Variational Inference (SVI)\n",
    "pyro.clear_param_store()\n",
    "\n",
    "optimizer = Adam({\"lr\": 0.05})\n",
    "svi = SVI(linear_model, linear_guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "losses = []\n",
    "n_steps = 2000\n",
    "\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(x_train, y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if (step + 1) % 500 == 0:\n",
    "        print(f\"Step {step+1}, Loss: {loss:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('ELBO Loss')\n",
    "plt.title('SVI Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine learned parameters\n",
    "print(\"Learned approximate posterior:\")\n",
    "print(f\"  w ~ N({pyro.param('w_loc').item():.3f}, {pyro.param('w_scale').item():.3f})\")\n",
    "print(f\"  b ~ N({pyro.param('b_loc').item():.3f}, {pyro.param('b_scale').item():.3f})\")\n",
    "print(f\"  sigma ~ LogN({pyro.param('sigma_loc').item():.3f}, {pyro.param('sigma_scale').item():.3f})\")\n",
    "print(f\"\\nTrue values: w=2.5, b=-1.0, sigma=0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive\n",
    "predictive = Predictive(linear_model, guide=linear_guide, num_samples=200)\n",
    "x_test = torch.linspace(-3, 3, 100)\n",
    "\n",
    "samples = predictive(x_test)\n",
    "obs_samples = samples[\"obs\"]  # [num_samples, num_points]\n",
    "\n",
    "mean_pred = obs_samples.mean(dim=0)\n",
    "std_pred = obs_samples.std(dim=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, alpha=0.7, label='Training data')\n",
    "plt.plot(x_test, mean_pred, 'r-', label='Posterior mean', linewidth=2)\n",
    "plt.fill_between(x_test, mean_pred - 2*std_pred, mean_pred + 2*std_pred,\n",
    "                 alpha=0.3, color='red', label='95% credible interval')\n",
    "plt.plot(x_test, 2.5 * x_test - 1.0, 'g--', label='True function', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Bayesian Linear Regression with Pyro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. AutoGuide: Automatic Guide Construction\n",
    "\n",
    "Writing guides manually is tedious. Pyro provides automatic guides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# AutoNormal: Mean-field Gaussian approximation\n",
    "auto_guide = autoguide.AutoNormal(linear_model)\n",
    "\n",
    "svi = SVI(linear_model, auto_guide, Adam({\"lr\": 0.05}), loss=Trace_ELBO())\n",
    "\n",
    "for step in range(2000):\n",
    "    svi.step(x_train, y_train)\n",
    "\n",
    "# Get posterior samples\n",
    "predictive = Predictive(linear_model, guide=auto_guide, num_samples=500)\n",
    "samples = predictive(x_test)\n",
    "\n",
    "print(\"Posterior statistics (AutoNormal guide):\")\n",
    "for name in [\"w\", \"b\", \"sigma\"]:\n",
    "    vals = samples[name].squeeze()\n",
    "    print(f\"  {name}: mean={vals.mean():.3f}, std={vals.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Bayesian Neural Network with Pyro\n",
    "\n",
    "Now let's build a proper BNN using `PyroModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate nonlinear regression data\n",
    "def generate_nonlinear_data(n=100):\n",
    "    x = torch.linspace(-2, 2, n).unsqueeze(1)\n",
    "    y = torch.sin(2 * x) + 0.3 * torch.randn(n, 1)\n",
    "    return x, y.squeeze()\n",
    "\n",
    "x_nl, y_nl = generate_nonlinear_data(80)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_nl, y_nl, alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Nonlinear Regression Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianMLP(PyroModule):\n",
    "    \"\"\"Bayesian MLP using PyroModule.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim=1, hidden_dim=20, out_dim=1, prior_scale=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First layer\n",
    "        self.fc1 = PyroModule[nn.Linear](in_dim, hidden_dim)\n",
    "        self.fc1.weight = PyroSample(\n",
    "            dist.Normal(0., prior_scale).expand([hidden_dim, in_dim]).to_event(2)\n",
    "        )\n",
    "        self.fc1.bias = PyroSample(\n",
    "            dist.Normal(0., prior_scale).expand([hidden_dim]).to_event(1)\n",
    "        )\n",
    "        \n",
    "        # Second layer\n",
    "        self.fc2 = PyroModule[nn.Linear](hidden_dim, out_dim)\n",
    "        self.fc2.weight = PyroSample(\n",
    "            dist.Normal(0., prior_scale).expand([out_dim, hidden_dim]).to_event(2)\n",
    "        )\n",
    "        self.fc2.bias = PyroSample(\n",
    "            dist.Normal(0., prior_scale).expand([out_dim]).to_event(1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        mu = self.fc2(x).squeeze(-1)\n",
    "        \n",
    "        # Observation noise\n",
    "        sigma = pyro.sample(\"sigma\", dist.LogNormal(0., 1.))\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            pyro.sample(\"obs\", dist.Normal(mu, sigma), obs=y)\n",
    "        \n",
    "        return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BNN\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BayesianMLP(in_dim=1, hidden_dim=20, out_dim=1, prior_scale=1.0)\n",
    "guide = autoguide.AutoDiagonalNormal(bnn)\n",
    "\n",
    "optimizer = Adam({\"lr\": 0.01})\n",
    "svi = SVI(bnn, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "losses = []\n",
    "n_steps = 3000\n",
    "\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(x_nl, y_nl)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if (step + 1) % 1000 == 0:\n",
    "        print(f\"Step {step+1}, Loss: {loss:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('ELBO Loss')\n",
    "plt.title('BNN Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive with uncertainty\n",
    "predictive = Predictive(bnn, guide=guide, num_samples=200)\n",
    "\n",
    "x_test_nl = torch.linspace(-3, 3, 100).unsqueeze(1)\n",
    "samples = predictive(x_test_nl)\n",
    "obs_samples = samples[\"obs\"]  # [num_samples, num_points]\n",
    "\n",
    "mean_pred = obs_samples.mean(dim=0)\n",
    "std_pred = obs_samples.std(dim=0)\n",
    "\n",
    "# Compute epistemic uncertainty (from weight uncertainty)\n",
    "# by looking at variance of the mean predictions\n",
    "mu_samples = []\n",
    "for i in range(200):\n",
    "    guide_trace = pyro.poutine.trace(guide).get_trace(x_test_nl)\n",
    "    model_trace = pyro.poutine.trace(\n",
    "        pyro.poutine.replay(bnn, trace=guide_trace)\n",
    "    ).get_trace(x_test_nl)\n",
    "    # Get the mean (before noise)\n",
    "    mu_samples.append(model_trace.nodes[\"_RETURN\"][\"value\"])\n",
    "\n",
    "mu_samples = torch.stack(mu_samples)\n",
    "epistemic_std = mu_samples.std(dim=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_nl, y_nl, alpha=0.7, label='Training data', zorder=5)\n",
    "plt.plot(x_test_nl, mean_pred, 'r-', label='Posterior mean', linewidth=2)\n",
    "plt.fill_between(x_test_nl.squeeze(), mean_pred - 2*std_pred, mean_pred + 2*std_pred,\n",
    "                 alpha=0.2, color='red', label='Total uncertainty (±2σ)')\n",
    "plt.fill_between(x_test_nl.squeeze(), mean_pred - 2*epistemic_std, mean_pred + 2*epistemic_std,\n",
    "                 alpha=0.3, color='blue', label='Epistemic uncertainty (±2σ)')\n",
    "plt.plot(x_test_nl, torch.sin(2 * x_test_nl), 'g--', label='True function', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Bayesian Neural Network with Pyro')\n",
    "plt.axvline(x=-2, color='gray', linestyle=':', alpha=0.5)\n",
    "plt.axvline(x=2, color='gray', linestyle=':', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Epistemic uncertainty increases outside training range [-2, 2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. BNN for Classification (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load a subset of MNIST for speed\n",
    "transform = transforms.ToTensor()\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# Use subset for faster training\n",
    "train_subset = torch.utils.data.Subset(mnist_train, range(5000))\n",
    "test_subset = torch.utils.data.Subset(mnist_test, range(1000))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianClassifier(PyroModule):\n",
    "    \"\"\"Bayesian classifier for MNIST.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim=784, hidden_dim=128, out_dim=10, prior_scale=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = PyroModule[nn.Linear](in_dim, hidden_dim)\n",
    "        self.fc1.weight = PyroSample(\n",
    "            dist.Normal(0., prior_scale).expand([hidden_dim, in_dim]).to_event(2)\n",
    "        )\n",
    "        self.fc1.bias = PyroSample(\n",
    "            dist.Normal(0., prior_scale).expand([hidden_dim]).to_event(1)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = PyroModule[nn.Linear](hidden_dim, out_dim)\n",
    "        self.fc2.weight = PyroSample(\n",
    "            dist.Normal(0., prior_scale).expand([out_dim, hidden_dim]).to_event(2)\n",
    "        )\n",
    "        self.fc2.bias = PyroSample(\n",
    "            dist.Normal(0., prior_scale).expand([out_dim]).to_event(1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            pyro.sample(\"obs\", dist.Categorical(logits=logits), obs=y)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bayesian classifier\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn_classifier = BayesianClassifier(prior_scale=0.5)\n",
    "guide = autoguide.AutoDiagonalNormal(bnn_classifier)\n",
    "\n",
    "optimizer = Adam({\"lr\": 0.005})\n",
    "svi = SVI(bnn_classifier, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        loss = svi.step(x, y)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with uncertainty\n",
    "def predict_with_uncertainty(model, guide, x, num_samples=30):\n",
    "    \"\"\"Get predictions with uncertainty from BNN.\"\"\"\n",
    "    predictive = Predictive(model, guide=guide, num_samples=num_samples, return_sites=[\"obs\", \"_RETURN\"])\n",
    "    samples = predictive(x)\n",
    "    \n",
    "    # Get logits from multiple forward passes\n",
    "    logits = samples[\"_RETURN\"]  # [num_samples, batch, classes]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    mean_probs = probs.mean(dim=0)\n",
    "    entropy = -(mean_probs * torch.log(mean_probs + 1e-10)).sum(dim=-1)\n",
    "    \n",
    "    return mean_probs, entropy\n",
    "\n",
    "# Evaluate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "all_entropy = []\n",
    "\n",
    "for x, y in test_loader:\n",
    "    probs, entropy = predict_with_uncertainty(bnn_classifier, guide, x, num_samples=30)\n",
    "    preds = probs.argmax(dim=-1)\n",
    "    correct += (preds == y).sum().item()\n",
    "    total += y.size(0)\n",
    "    all_entropy.extend(entropy.numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "print(f\"Mean entropy: {np.mean(all_entropy):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare uncertainty on ID vs OOD (random noise)\n",
    "x_id, _ = next(iter(test_loader))\n",
    "x_ood = torch.rand_like(x_id)  # Random noise\n",
    "\n",
    "_, entropy_id = predict_with_uncertainty(bnn_classifier, guide, x_id[:50], num_samples=50)\n",
    "_, entropy_ood = predict_with_uncertainty(bnn_classifier, guide, x_ood[:50], num_samples=50)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(entropy_id.numpy(), bins=30, alpha=0.5, label='MNIST (ID)', density=True)\n",
    "plt.hist(entropy_ood.numpy(), bins=30, alpha=0.5, label='Random noise (OOD)', density=True)\n",
    "plt.xlabel('Predictive Entropy')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Pyro BNN: Uncertainty on ID vs OOD Data')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean entropy - ID: {entropy_id.mean():.4f}, OOD: {entropy_ood.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Comparison: Pyro BNN vs MC Dropout vs Ensemble\n",
    "\n",
    "Let's compare uncertainty quality across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC Dropout baseline\n",
    "class MCDropoutNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def predict_with_uncertainty(self, x, n_samples=30):\n",
    "        self.train()  # Keep dropout on\n",
    "        preds = [F.softmax(self(x), dim=1) for _ in range(n_samples)]\n",
    "        preds = torch.stack(preds)\n",
    "        mean_pred = preds.mean(dim=0)\n",
    "        entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)\n",
    "        return mean_pred, entropy\n",
    "\n",
    "# Train MC Dropout\n",
    "mc_model = MCDropoutNet()\n",
    "optimizer = torch.optim.Adam(mc_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    mc_model.train()\n",
    "    for x, y in train_loader:\n",
    "        loss = F.cross_entropy(mc_model(x), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"MC Dropout trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ensemble\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "ensemble = [SimpleNet() for _ in range(5)]\n",
    "\n",
    "for i, model in enumerate(ensemble):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            loss = F.cross_entropy(model(x), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def ensemble_predict(models, x, n_samples=None):\n",
    "    preds = [F.softmax(m(x), dim=1) for m in models]\n",
    "    preds = torch.stack(preds)\n",
    "    mean_pred = preds.mean(dim=0)\n",
    "    entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)\n",
    "    return mean_pred, entropy\n",
    "\n",
    "print(\"Ensemble trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OOD detection\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_ood(predict_fn, id_data, ood_data, **kwargs):\n",
    "    _, ent_id = predict_fn(id_data, **kwargs)\n",
    "    _, ent_ood = predict_fn(ood_data, **kwargs)\n",
    "    \n",
    "    labels = np.concatenate([np.zeros(len(ent_id)), np.ones(len(ent_ood))])\n",
    "    scores = np.concatenate([ent_id.detach().numpy(), ent_ood.detach().numpy()])\n",
    "    \n",
    "    return roc_auc_score(labels, scores)\n",
    "\n",
    "# Get test data\n",
    "x_test_batch, _ = next(iter(test_loader))\n",
    "x_ood_batch = torch.rand_like(x_test_batch)\n",
    "\n",
    "# Evaluate each method\n",
    "auroc_bnn = evaluate_ood(\n",
    "    lambda x, **kw: predict_with_uncertainty(bnn_classifier, guide, x, num_samples=30),\n",
    "    x_test_batch, x_ood_batch\n",
    ")\n",
    "\n",
    "auroc_mc = evaluate_ood(\n",
    "    mc_model.predict_with_uncertainty,\n",
    "    x_test_batch, x_ood_batch,\n",
    "    n_samples=30\n",
    ")\n",
    "\n",
    "auroc_ens = evaluate_ood(\n",
    "    lambda x, **kw: ensemble_predict(ensemble, x),\n",
    "    x_test_batch, x_ood_batch\n",
    ")\n",
    "\n",
    "print(\"OOD Detection AUROC (higher is better):\")\n",
    "print(f\"  Pyro BNN:   {auroc_bnn:.4f}\")\n",
    "print(f\"  MC Dropout: {auroc_mc:.4f}\")\n",
    "print(f\"  Ensemble:   {auroc_ens:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary\n",
    "\n",
    "### Pyro Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| `pyro.sample` | Declare a random variable |\n",
    "| `pyro.param` | Declare a learnable parameter |\n",
    "| `pyro.plate` | Declare conditional independence |\n",
    "| **Model** | Generative process (prior × likelihood) |\n",
    "| **Guide** | Approximate posterior q(z) |\n",
    "| **SVI** | Stochastic Variational Inference |\n",
    "| **ELBO** | Evidence Lower Bound (optimization target) |\n",
    "\n",
    "### When to Use Pyro\n",
    "\n",
    "| Use Case | Recommendation |\n",
    "|----------|---------------|\n",
    "| Quick uncertainty estimate | MC Dropout |\n",
    "| Best uncertainty quality | Ensemble |\n",
    "| Principled Bayesian inference | Pyro |\n",
    "| Complex probabilistic models | Pyro |\n",
    "| Hierarchical models | Pyro |\n",
    "| Production with uncertainty | Ensemble or Pyro |\n",
    "\n",
    "### Pyro vs Notebook 18 Methods\n",
    "\n",
    "| Aspect | MC Dropout | Ensemble | Pyro BNN |\n",
    "|--------|-----------|----------|----------|\n",
    "| Theoretical grounding | Approximate | Heuristic | Principled |\n",
    "| Training cost | 1× | N× | 1× (but slower) |\n",
    "| Inference cost | N passes | N passes | N passes |\n",
    "| Flexibility | Low | Low | High |\n",
    "| Prior specification | No | No | Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Bayesian Logistic Regression\n",
    "Implement binary classification on a 2D dataset with uncertainty visualization.\n",
    "\n",
    "### Exercise 2: Heteroscedastic Regression\n",
    "Modify the BNN to learn input-dependent noise (aleatoric uncertainty).\n",
    "\n",
    "### Exercise 3: Prior Sensitivity\n",
    "Train BNNs with different prior scales and compare uncertainty estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Bayesian Logistic Regression\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate 2D data\n",
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def logistic_model(x, y=None):\n",
    "    w = pyro.sample(\"w\", dist.Normal(torch.zeros(2), torch.ones(2)).to_event(1))\n",
    "    b = pyro.sample(\"b\", dist.Normal(0., 1.))\n",
    "    \n",
    "    logits = (x * w).sum(dim=-1) + b\n",
    "    \n",
    "    with pyro.plate(\"data\", x.shape[0]):\n",
    "        pyro.sample(\"obs\", dist.Bernoulli(logits=logits), obs=y.float() if y is not None else None)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "pyro.clear_param_store()\n",
    "guide = autoguide.AutoNormal(logistic_model)\n",
    "svi = SVI(logistic_model, guide, Adam({\"lr\": 0.05}), loss=Trace_ELBO())\n",
    "\n",
    "for _ in range(2000):\n",
    "    svi.step(X, y)\n",
    "\n",
    "# Visualize decision boundary with uncertainty\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 3, 100), np.linspace(-1.5, 2, 100))\n",
    "X_grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "predictive = Predictive(logistic_model, guide=guide, num_samples=100)\n",
    "samples = predictive(X_grid)\n",
    "\n",
    "probs = torch.sigmoid(samples[\"obs\"]).mean(dim=0).numpy().reshape(xx.shape)\n",
    "uncertainty = torch.sigmoid(samples[\"obs\"]).std(dim=0).numpy().reshape(xx.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].contourf(xx, yy, probs, levels=20, cmap='RdBu_r', alpha=0.8)\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu_r', edgecolors='black')\n",
    "axes[0].set_title('Posterior Mean Probability')\n",
    "\n",
    "axes[1].contourf(xx, yy, uncertainty, levels=20, cmap='Oranges')\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu_r', edgecolors='black')\n",
    "axes[1].set_title('Posterior Uncertainty (std)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Heteroscedastic BNN\n",
    "class HeteroscedasticBNN(PyroModule):\n",
    "    \"\"\"BNN that predicts both mean and variance.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=20, prior_scale=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.fc1 = PyroModule[nn.Linear](1, hidden_dim)\n",
    "        self.fc1.weight = PyroSample(dist.Normal(0., prior_scale).expand([hidden_dim, 1]).to_event(2))\n",
    "        self.fc1.bias = PyroSample(dist.Normal(0., prior_scale).expand([hidden_dim]).to_event(1))\n",
    "        \n",
    "        # Mean head\n",
    "        self.fc_mean = PyroModule[nn.Linear](hidden_dim, 1)\n",
    "        self.fc_mean.weight = PyroSample(dist.Normal(0., prior_scale).expand([1, hidden_dim]).to_event(2))\n",
    "        self.fc_mean.bias = PyroSample(dist.Normal(0., prior_scale).expand([1]).to_event(1))\n",
    "        \n",
    "        # Variance head (predicts log variance)\n",
    "        self.fc_var = PyroModule[nn.Linear](hidden_dim, 1)\n",
    "        self.fc_var.weight = PyroSample(dist.Normal(0., prior_scale).expand([1, hidden_dim]).to_event(2))\n",
    "        self.fc_var.bias = PyroSample(dist.Normal(0., prior_scale).expand([1]).to_event(1))\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        h = torch.tanh(self.fc1(x))\n",
    "        mu = self.fc_mean(h).squeeze(-1)\n",
    "        log_var = self.fc_var(h).squeeze(-1)\n",
    "        sigma = torch.exp(0.5 * log_var) + 0.01  # Add small constant for stability\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            pyro.sample(\"obs\", dist.Normal(mu, sigma), obs=y)\n",
    "        \n",
    "        return mu, sigma\n",
    "\n",
    "# Generate heteroscedastic data\n",
    "x_het = torch.linspace(-2, 2, 100).unsqueeze(1)\n",
    "noise_scale = 0.1 + 0.5 * torch.abs(x_het)  # Noise increases with |x|\n",
    "y_het = torch.sin(2 * x_het) + torch.randn_like(x_het) * noise_scale\n",
    "y_het = y_het.squeeze()\n",
    "\n",
    "pyro.clear_param_store()\n",
    "het_bnn = HeteroscedasticBNN(hidden_dim=20)\n",
    "guide = autoguide.AutoDiagonalNormal(het_bnn)\n",
    "svi = SVI(het_bnn, guide, Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "\n",
    "for step in range(3000):\n",
    "    svi.step(x_het, y_het)\n",
    "\n",
    "# Predict\n",
    "x_test = torch.linspace(-3, 3, 100).unsqueeze(1)\n",
    "predictive = Predictive(het_bnn, guide=guide, num_samples=200, return_sites=[\"obs\", \"_RETURN\"])\n",
    "samples = predictive(x_test)\n",
    "\n",
    "obs = samples[\"obs\"]\n",
    "mean_pred = obs.mean(dim=0)\n",
    "std_pred = obs.std(dim=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_het, y_het, alpha=0.5, label='Data')\n",
    "plt.plot(x_test, mean_pred, 'r-', label='Mean prediction', linewidth=2)\n",
    "plt.fill_between(x_test.squeeze(), mean_pred - 2*std_pred, mean_pred + 2*std_pred,\n",
    "                 alpha=0.3, color='red', label='95% interval')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Heteroscedastic BNN: Learned Input-Dependent Noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Prior Sensitivity\n",
    "prior_scales = [0.1, 0.5, 1.0, 2.0]\n",
    "results = {}\n",
    "\n",
    "for scale in prior_scales:\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    model = BayesianMLP(prior_scale=scale)\n",
    "    guide = autoguide.AutoDiagonalNormal(model)\n",
    "    svi = SVI(model, guide, Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "    \n",
    "    for _ in range(2000):\n",
    "        svi.step(x_nl, y_nl)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictive = Predictive(model, guide=guide, num_samples=200)\n",
    "    samples = predictive(x_test_nl)\n",
    "    \n",
    "    results[scale] = {\n",
    "        'mean': samples['obs'].mean(dim=0),\n",
    "        'std': samples['obs'].std(dim=0)\n",
    "    }\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, len(prior_scales), figsize=(16, 4))\n",
    "\n",
    "for ax, scale in zip(axes, prior_scales):\n",
    "    mean = results[scale]['mean']\n",
    "    std = results[scale]['std']\n",
    "    \n",
    "    ax.scatter(x_nl, y_nl, alpha=0.5, s=10)\n",
    "    ax.plot(x_test_nl, mean, 'r-', linewidth=2)\n",
    "    ax.fill_between(x_test_nl.squeeze(), mean - 2*std, mean + 2*std, alpha=0.3, color='red')\n",
    "    ax.axvline(x=-2, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.axvline(x=2, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_title(f'Prior scale = {scale}')\n",
    "    ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.suptitle('Effect of Prior Scale on Uncertainty')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Smaller prior scale → tighter uncertainty, may underfit\")\n",
    "print(\"- Larger prior scale → wider uncertainty, especially OOD\")\n",
    "print(\"- Prior acts as regularization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
