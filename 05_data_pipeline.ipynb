{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Data Pipeline: Dataset, DataLoader, and Augmentation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand Dataset internals** - How `torch.utils.data.Dataset` works and when to use map-style vs iterable-style datasets\n",
    "2. **Master DataLoader mechanics** - Batching, shuffling, collation, and the interaction between components\n",
    "3. **Implement custom Samplers** - Control data ordering for stratified sampling, curriculum learning, etc.\n",
    "4. **Optimize data loading** - Multi-worker loading, memory pinning, and prefetching\n",
    "5. **Design augmentation pipelines** - Using torchvision.transforms and custom augmentations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    IterableDataset,\n",
    "    DataLoader, \n",
    "    Sampler,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    "    BatchSampler,\n",
    "    WeightedRandomSampler,\n",
    "    SubsetRandomSampler,\n",
    "    default_collate\n",
    ")\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Iterator, List, Tuple, Any, Optional\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Dataset Fundamentals\n",
    "\n",
    "PyTorch provides two types of datasets:\n",
    "\n",
    "1. **Map-style datasets** (`Dataset`): Index-based access via `__getitem__` and `__len__`\n",
    "2. **Iterable-style datasets** (`IterableDataset`): Streaming access via `__iter__`\n",
    "\n",
    "### 1.1 Map-Style Dataset Internals\n",
    "\n",
    "A map-style dataset must implement:\n",
    "- `__getitem__(index)`: Returns the sample at the given index\n",
    "- `__len__()`: Returns the total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple custom dataset example\n",
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"A dataset that returns (x, y) pairs where y = 2x + 1\"\"\"\n",
    "    \n",
    "    def __init__(self, size: int = 1000):\n",
    "        self.size = size\n",
    "        # Generate data once during initialization\n",
    "        self.x = torch.randn(size, 1)\n",
    "        self.y = 2 * self.x + 1 + 0.1 * torch.randn(size, 1)  # Add noise\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "\n",
    "# Create and inspect\n",
    "dataset = SimpleDataset(100)\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "print(f\"First sample: x={dataset[0][0].item():.3f}, y={dataset[0][1].item():.3f}\")\n",
    "print(f\"Type of sample: {type(dataset[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Lazy Loading vs Pre-loading\n",
    "\n",
    "A critical design decision is **when** to load data:\n",
    "\n",
    "- **Pre-loading**: Load everything into memory in `__init__` (fast access, high memory)\n",
    "- **Lazy loading**: Load each sample on-demand in `__getitem__` (slow access, low memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyImageDataset(Dataset):\n",
    "    \"\"\"Loads images from disk on-demand (lazy loading)\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths: List[str], labels: List[int], transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        # Load image from disk each time (lazy)\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "class PreloadedImageDataset(Dataset):\n",
    "    \"\"\"Loads all images into memory upfront (pre-loading)\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths: List[str], labels: List[int], transform=None):\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Load all images into memory\n",
    "        print(f\"Pre-loading {len(image_paths)} images...\")\n",
    "        self.images = [Image.open(p).convert('RGB') for p in image_paths]\n",
    "        print(\"Done!\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "print(\"LazyImageDataset: Low memory, disk I/O each access\")\n",
    "print(\"PreloadedImageDataset: High memory, fast access\")\n",
    "print(\"\\nChoose based on: dataset size vs available RAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Iterable-Style Datasets\n",
    "\n",
    "Use `IterableDataset` when:\n",
    "- Data comes from a stream (network, database)\n",
    "- Dataset is too large to index\n",
    "- Random access is expensive or impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingDataset(IterableDataset):\n",
    "    \"\"\"Simulates streaming data that can't be indexed\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples: int = 1000):\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def __iter__(self) -> Iterator[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # Generate samples on-the-fly\n",
    "        for _ in range(self.num_samples):\n",
    "            x = torch.randn(1)\n",
    "            y = 2 * x + 1 + 0.1 * torch.randn(1)\n",
    "            yield x, y\n",
    "\n",
    "\n",
    "# IterableDataset doesn't support __len__ or indexing\n",
    "stream_dataset = StreamingDataset(100)\n",
    "\n",
    "# Must iterate to access data\n",
    "for i, (x, y) in enumerate(stream_dataset):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"Sample {i}: x={x.item():.3f}, y={y.item():.3f}\")\n",
    "\n",
    "# This would fail:\n",
    "# stream_dataset[0]  # TypeError: 'StreamingDataset' object is not subscriptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Worker-Safe Iterable Datasets\n",
    "\n",
    "When using multiple workers with `IterableDataset`, each worker gets a copy. You must handle this to avoid duplicate data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerSafeStreamingDataset(IterableDataset):\n",
    "    \"\"\"Properly handles multi-worker data loading\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples: int = 1000):\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def __iter__(self) -> Iterator:\n",
    "        # Get worker info\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        \n",
    "        if worker_info is None:\n",
    "            # Single-process loading\n",
    "            start = 0\n",
    "            end = self.num_samples\n",
    "        else:\n",
    "            # Multi-process loading: split work among workers\n",
    "            per_worker = self.num_samples // worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            start = worker_id * per_worker\n",
    "            end = start + per_worker\n",
    "            if worker_id == worker_info.num_workers - 1:\n",
    "                end = self.num_samples  # Last worker handles remainder\n",
    "        \n",
    "        # Generate only this worker's portion\n",
    "        for i in range(start, end):\n",
    "            torch.manual_seed(i)  # Reproducible samples\n",
    "            x = torch.randn(1)\n",
    "            y = 2 * x + 1 + 0.1 * torch.randn(1)\n",
    "            yield x, y\n",
    "\n",
    "\n",
    "# Test with multiple workers\n",
    "safe_dataset = WorkerSafeStreamingDataset(20)\n",
    "loader = DataLoader(safe_dataset, batch_size=4, num_workers=2)\n",
    "\n",
    "all_samples = []\n",
    "for batch_x, batch_y in loader:\n",
    "    all_samples.extend(batch_x.tolist())\n",
    "\n",
    "print(f\"Total samples (with 2 workers): {len(all_samples)}\")\n",
    "print(f\"Unique samples: {len(set(tuple(s) for s in all_samples))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. DataLoader Deep Dive\n",
    "\n",
    "The `DataLoader` orchestrates:\n",
    "1. **Sampling**: Which indices to load (via `Sampler`)\n",
    "2. **Loading**: Getting samples from the dataset\n",
    "3. **Batching**: Grouping samples together (via `BatchSampler`)\n",
    "4. **Collation**: Combining samples into batches (via `collate_fn`)\n",
    "\n",
    "### 2.1 DataLoader Parameters Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for demonstration\n",
    "demo_dataset = SimpleDataset(100)\n",
    "\n",
    "# DataLoader with all major parameters\n",
    "loader = DataLoader(\n",
    "    demo_dataset,\n",
    "    \n",
    "    # Batching\n",
    "    batch_size=16,          # Samples per batch\n",
    "    drop_last=False,        # Keep incomplete final batch?\n",
    "    \n",
    "    # Sampling\n",
    "    shuffle=True,           # Randomize order each epoch\n",
    "    # sampler=None,         # Custom sampler (mutually exclusive with shuffle)\n",
    "    # batch_sampler=None,   # Custom batch sampler (overrides batch_size, shuffle, etc.)\n",
    "    \n",
    "    # Performance\n",
    "    num_workers=0,          # Parallel data loading processes\n",
    "    pin_memory=False,       # Pin memory for faster GPU transfer\n",
    "    prefetch_factor=2,      # Batches to prefetch per worker (when num_workers > 0)\n",
    "    persistent_workers=False,  # Keep workers alive between epochs\n",
    "    \n",
    "    # Collation\n",
    "    collate_fn=None,        # Custom function to merge samples\n",
    ")\n",
    "\n",
    "# Inspect a batch\n",
    "batch_x, batch_y = next(iter(loader))\n",
    "print(f\"Batch shape: x={batch_x.shape}, y={batch_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understanding Collation\n",
    "\n",
    "The `collate_fn` converts a list of samples into a batch. The default behavior:\n",
    "- Stacks tensors along a new dimension\n",
    "- Recursively processes tuples, lists, dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default collation behavior\n",
    "samples = [demo_dataset[i] for i in range(4)]\n",
    "print(\"Individual samples:\")\n",
    "for i, (x, y) in enumerate(samples):\n",
    "    print(f\"  Sample {i}: x.shape={x.shape}, y.shape={y.shape}\")\n",
    "\n",
    "# Default collate function stacks them\n",
    "batch = default_collate(samples)\n",
    "print(f\"\\nAfter collation:\")\n",
    "print(f\"  Batch x.shape={batch[0].shape}, y.shape={batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function for variable-length sequences\n",
    "class VariableLengthDataset(Dataset):\n",
    "    \"\"\"Dataset with variable-length sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, size: int = 100):\n",
    "        self.size = size\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        # Variable length sequence (5 to 15 elements)\n",
    "        length = 5 + index % 11\n",
    "        sequence = torch.randn(length)\n",
    "        label = index % 3\n",
    "        return sequence, label\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    \"\"\"Pad sequences to the longest in the batch\"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Find max length\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded = torch.zeros(len(sequences), max_len)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq)] = seq\n",
    "    \n",
    "    return padded, torch.tensor(labels), torch.tensor(lengths)\n",
    "\n",
    "\n",
    "var_dataset = VariableLengthDataset(20)\n",
    "var_loader = DataLoader(var_dataset, batch_size=4, collate_fn=pad_collate)\n",
    "\n",
    "for padded_seqs, labels, lengths in var_loader:\n",
    "    print(f\"Padded batch shape: {padded_seqs.shape}\")\n",
    "    print(f\"Original lengths: {lengths.tolist()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Using torch.nn.utils.rnn for Sequences\n",
    "\n",
    "PyTorch provides utilities for handling variable-length sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def advanced_pad_collate(batch):\n",
    "    \"\"\"Use PyTorch's pad_sequence for efficient padding\"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Sort by length (descending) - required for pack_padded_sequence\n",
    "    sorted_indices = sorted(range(len(sequences)), key=lambda i: len(sequences[i]), reverse=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = [labels[i] for i in sorted_indices]\n",
    "    \n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    \n",
    "    # pad_sequence expects list of tensors, pads to longest\n",
    "    padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded, torch.tensor(labels), lengths\n",
    "\n",
    "\n",
    "var_loader2 = DataLoader(var_dataset, batch_size=4, collate_fn=advanced_pad_collate)\n",
    "\n",
    "for padded_seqs, labels, lengths in var_loader2:\n",
    "    print(f\"Padded shape: {padded_seqs.shape}\")\n",
    "    print(f\"Lengths (sorted): {lengths.tolist()}\")\n",
    "    \n",
    "    # Can use with pack_padded_sequence for RNNs\n",
    "    packed = pack_padded_sequence(padded_seqs.unsqueeze(-1), lengths.cpu(), batch_first=True)\n",
    "    print(f\"Packed data shape: {packed.data.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Samplers: Controlling Data Order\n",
    "\n",
    "Samplers determine **which indices** are loaded and in **what order**.\n",
    "\n",
    "### 3.1 Built-in Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimpleDataset(20)\n",
    "\n",
    "# SequentialSampler: indices 0, 1, 2, ..., n-1\n",
    "seq_sampler = SequentialSampler(dataset)\n",
    "print(f\"Sequential: {list(seq_sampler)[:10]}...\")\n",
    "\n",
    "# RandomSampler: random permutation\n",
    "rand_sampler = RandomSampler(dataset)\n",
    "print(f\"Random: {list(rand_sampler)[:10]}...\")\n",
    "\n",
    "# RandomSampler with replacement (for oversampling)\n",
    "rand_replace_sampler = RandomSampler(dataset, replacement=True, num_samples=30)\n",
    "print(f\"Random w/ replacement (30 samples): {list(rand_replace_sampler)[:15]}...\")\n",
    "\n",
    "# SubsetRandomSampler: random sample from specific indices\n",
    "indices = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]  # Even indices only\n",
    "subset_sampler = SubsetRandomSampler(indices)\n",
    "print(f\"Subset (even indices): {list(subset_sampler)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 WeightedRandomSampler for Class Imbalance\n",
    "\n",
    "Handles imbalanced datasets by oversampling minority classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDataset(Dataset):\n",
    "    \"\"\"Dataset with class imbalance: 90% class 0, 10% class 1\"\"\"\n",
    "    \n",
    "    def __init__(self, size: int = 1000):\n",
    "        self.size = size\n",
    "        # Create imbalanced labels\n",
    "        self.labels = torch.zeros(size, dtype=torch.long)\n",
    "        self.labels[:size // 10] = 1  # 10% are class 1\n",
    "        # Shuffle\n",
    "        perm = torch.randperm(size)\n",
    "        self.labels = self.labels[perm]\n",
    "        self.data = torch.randn(size, 10)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "\n",
    "imb_dataset = ImbalancedDataset(1000)\n",
    "print(f\"Original class distribution: {Counter(imb_dataset.labels.tolist())}\")\n",
    "\n",
    "# Calculate sample weights (inverse class frequency)\n",
    "class_counts = Counter(imb_dataset.labels.tolist())\n",
    "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "sample_weights = [class_weights[label.item()] for label in imb_dataset.labels]\n",
    "\n",
    "# Create weighted sampler\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(imb_dataset),\n",
    "    replacement=True  # Must be True for oversampling\n",
    ")\n",
    "\n",
    "# Compare class distributions in batches\n",
    "regular_loader = DataLoader(imb_dataset, batch_size=100, shuffle=True)\n",
    "weighted_loader = DataLoader(imb_dataset, batch_size=100, sampler=weighted_sampler)\n",
    "\n",
    "# Check first epoch\n",
    "regular_labels = []\n",
    "weighted_labels = []\n",
    "\n",
    "for _, labels in regular_loader:\n",
    "    regular_labels.extend(labels.tolist())\n",
    "\n",
    "for _, labels in weighted_loader:\n",
    "    weighted_labels.extend(labels.tolist())\n",
    "\n",
    "print(f\"\\nRegular loader distribution: {Counter(regular_labels)}\")\n",
    "print(f\"Weighted loader distribution: {Counter(weighted_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Custom Sampler Implementation\n",
    "\n",
    "Create your own sampler by implementing `__iter__` and `__len__`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurriculumSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Curriculum learning sampler: starts with easier samples,\n",
    "    gradually introduces harder ones.\n",
    "    \n",
    "    Assumes dataset has a 'difficulty' attribute or method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, difficulties: List[float], epoch: int = 0, max_epochs: int = 10):\n",
    "        self.dataset = dataset\n",
    "        self.difficulties = difficulties\n",
    "        self.epoch = epoch\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        # Sort indices by difficulty\n",
    "        self.sorted_indices = sorted(range(len(difficulties)), key=lambda i: difficulties[i])\n",
    "    \n",
    "    def set_epoch(self, epoch: int):\n",
    "        \"\"\"Update epoch for curriculum progression\"\"\"\n",
    "        self.epoch = epoch\n",
    "    \n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        # Calculate how much of the dataset to use\n",
    "        progress = min(1.0, (self.epoch + 1) / self.max_epochs)\n",
    "        num_samples = int(progress * len(self.sorted_indices))\n",
    "        num_samples = max(num_samples, len(self.sorted_indices) // 10)  # At least 10%\n",
    "        \n",
    "        # Use easiest samples up to current progress\n",
    "        available_indices = self.sorted_indices[:num_samples]\n",
    "        \n",
    "        # Shuffle within available samples\n",
    "        perm = torch.randperm(len(available_indices)).tolist()\n",
    "        return iter([available_indices[i] for i in perm])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        progress = min(1.0, (self.epoch + 1) / self.max_epochs)\n",
    "        return max(int(progress * len(self.sorted_indices)), len(self.sorted_indices) // 10)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset = SimpleDataset(100)\n",
    "# Simulate difficulties (e.g., based on noise level)\n",
    "difficulties = torch.rand(100).tolist()\n",
    "\n",
    "curriculum_sampler = CurriculumSampler(dataset, difficulties, epoch=0, max_epochs=10)\n",
    "\n",
    "print(\"Curriculum learning progression:\")\n",
    "for epoch in [0, 2, 5, 9]:\n",
    "    curriculum_sampler.set_epoch(epoch)\n",
    "    indices = list(curriculum_sampler)\n",
    "    avg_difficulty = sum(difficulties[i] for i in indices) / len(indices)\n",
    "    print(f\"  Epoch {epoch}: {len(indices)} samples, avg difficulty: {avg_difficulty:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 BatchSampler: Custom Batching Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarLengthBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Groups sequences of similar length into batches.\n",
    "    Reduces padding waste in NLP tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lengths: List[int], batch_size: int, drop_last: bool = False):\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        \n",
    "        # Sort indices by length\n",
    "        self.sorted_indices = sorted(range(len(lengths)), key=lambda i: lengths[i])\n",
    "        self.lengths = lengths\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # Create batches of similar-length sequences\n",
    "        batches = []\n",
    "        for i in range(0, len(self.sorted_indices), self.batch_size):\n",
    "            batch = self.sorted_indices[i:i + self.batch_size]\n",
    "            if len(batch) == self.batch_size or not self.drop_last:\n",
    "                batches.append(batch)\n",
    "        \n",
    "        # Shuffle batch order (not within batches)\n",
    "        perm = torch.randperm(len(batches)).tolist()\n",
    "        for i in perm:\n",
    "            yield from batches[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return (len(self.sorted_indices) // self.batch_size) * self.batch_size\n",
    "        return len(self.sorted_indices)\n",
    "\n",
    "\n",
    "# Example with variable-length dataset\n",
    "var_dataset = VariableLengthDataset(100)\n",
    "lengths = [5 + i % 11 for i in range(100)]  # Match dataset's length pattern\n",
    "\n",
    "similar_sampler = SimilarLengthBatchSampler(lengths, batch_size=8)\n",
    "similar_loader = DataLoader(var_dataset, batch_size=1, sampler=similar_sampler, collate_fn=pad_collate)\n",
    "\n",
    "# The loader will yield individual samples in the sampler's order\n",
    "# We need to manually batch them or use BatchSampler\n",
    "print(\"Similar-length batching reduces padding waste!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Performance Optimization\n",
    "\n",
    "### 4.1 Multi-Worker Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlowDataset(Dataset):\n",
    "    \"\"\"Simulates a dataset with slow I/O (e.g., loading from disk)\"\"\"\n",
    "    \n",
    "    def __init__(self, size: int = 100, delay: float = 0.01):\n",
    "        self.size = size\n",
    "        self.delay = delay\n",
    "        self.data = torch.randn(size, 64, 64)\n",
    "        self.labels = torch.randint(0, 10, (size,))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        time.sleep(self.delay)  # Simulate I/O delay\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "\n",
    "def benchmark_loader(loader, num_batches: int = 10) -> float:\n",
    "    \"\"\"Measure time to load batches\"\"\"\n",
    "    start = time.time()\n",
    "    for i, (data, labels) in enumerate(loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        # Simulate some processing\n",
    "        _ = data.mean()\n",
    "    return time.time() - start\n",
    "\n",
    "\n",
    "slow_dataset = SlowDataset(200, delay=0.01)\n",
    "\n",
    "# Compare different num_workers\n",
    "print(\"Benchmarking multi-worker loading:\")\n",
    "print(\"(Note: First run may be slower due to worker startup)\")\n",
    "\n",
    "for num_workers in [0, 2, 4]:\n",
    "    loader = DataLoader(\n",
    "        slow_dataset, \n",
    "        batch_size=16, \n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=(num_workers > 0)\n",
    "    )\n",
    "    \n",
    "    # Warm up\n",
    "    _ = benchmark_loader(loader, 2)\n",
    "    \n",
    "    # Actual benchmark\n",
    "    elapsed = benchmark_loader(loader, 10)\n",
    "    print(f\"  num_workers={num_workers}: {elapsed:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Memory Pinning\n",
    "\n",
    "Pinned (page-locked) memory enables faster CPU-to-GPU transfers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Create larger dataset for meaningful benchmark\n",
    "    large_dataset = SimpleDataset(10000)\n",
    "    \n",
    "    def benchmark_transfer(loader, num_batches: int = 50):\n",
    "        \"\"\"Measure time to transfer batches to GPU\"\"\"\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            # Transfer to GPU with non_blocking when using pinned memory\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        return time.time() - start\n",
    "    \n",
    "    # Without memory pinning\n",
    "    loader_no_pin = DataLoader(large_dataset, batch_size=256, pin_memory=False)\n",
    "    time_no_pin = benchmark_transfer(loader_no_pin)\n",
    "    \n",
    "    # With memory pinning\n",
    "    loader_pinned = DataLoader(large_dataset, batch_size=256, pin_memory=True)\n",
    "    time_pinned = benchmark_transfer(loader_pinned)\n",
    "    \n",
    "    print(f\"Without pin_memory: {time_no_pin:.4f}s\")\n",
    "    print(f\"With pin_memory: {time_pinned:.4f}s\")\n",
    "    print(f\"Speedup: {time_no_pin / time_pinned:.2f}x\")\n",
    "else:\n",
    "    print(\"GPU not available - pin_memory has no effect on CPU-only training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Prefetching and Persistent Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices for DataLoader configuration\n",
    "def create_optimized_loader(\n",
    "    dataset,\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 4,\n",
    "    pin_memory: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an optimized DataLoader with best practices.\n",
    "    \"\"\"\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        \n",
    "        # Performance settings\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory and torch.cuda.is_available(),\n",
    "        \n",
    "        # Prefetch batches while GPU is processing\n",
    "        prefetch_factor=2 if num_workers > 0 else None,\n",
    "        \n",
    "        # Keep workers alive between epochs (saves startup time)\n",
    "        persistent_workers=num_workers > 0,\n",
    "        \n",
    "        # Drop incomplete batches for consistent batch sizes\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Optimal DataLoader settings:\")\n",
    "print(\"  - num_workers: 2-4x CPU cores (start with 4, tune based on I/O)\")\n",
    "print(\"  - pin_memory: True for GPU training\")\n",
    "print(\"  - prefetch_factor: 2 (default) is usually good\")\n",
    "print(\"  - persistent_workers: True to avoid worker restart overhead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Augmentation\n",
    "\n",
    "Data augmentation increases training data diversity without collecting more data.\n",
    "\n",
    "### 5.1 torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST for augmentation examples\n",
    "data_dir = '../data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Basic transforms\n",
    "basic_transform = T.Compose([\n",
    "    T.ToTensor(),  # PIL Image -> Tensor, scales to [0, 1]\n",
    "    T.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transform = T.Compose([\n",
    "    T.RandomRotation(15),              # Rotate up to 15 degrees\n",
    "    T.RandomAffine(                    # Random affine transformation\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),          # Shift up to 10%\n",
    "        scale=(0.9, 1.1),              # Scale between 90% and 110%\n",
    "    ),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.1307,), (0.3081,)),\n",
    "    T.RandomErasing(p=0.2),            # Randomly erase patches\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = MNIST(data_dir, train=True, download=True, transform=train_transform)\n",
    "test_dataset = MNIST(data_dir, train=False, download=True, transform=basic_transform)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmentations\n",
    "def show_augmentations(dataset, index: int = 0, num_versions: int = 8):\n",
    "    \"\"\"Show multiple augmented versions of the same image\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        img, label = dataset[index]  # Each call applies random augmentation\n",
    "        ax.imshow(img.squeeze(), cmap='gray')\n",
    "        ax.set_title(f'Version {i+1} (Label: {label})')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Same image with different random augmentations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_augmentations(train_dataset, index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Advanced Transforms for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common augmentation pipeline for color images\n",
    "imagenet_train_transform = T.Compose([\n",
    "    # Spatial transformations\n",
    "    T.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop and resize\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(15),\n",
    "    \n",
    "    # Color transformations\n",
    "    T.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.1\n",
    "    ),\n",
    "    T.RandomGrayscale(p=0.1),\n",
    "    \n",
    "    # Convert to tensor\n",
    "    T.ToTensor(),\n",
    "    \n",
    "    # Normalize (ImageNet stats)\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    \n",
    "    # Regularization\n",
    "    T.RandomErasing(p=0.2),\n",
    "])\n",
    "\n",
    "# Validation/test transform (no augmentation)\n",
    "imagenet_val_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "print(\"ImageNet-style augmentation pipeline created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Custom Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise:\n",
    "    \"\"\"Add Gaussian noise to tensor images\"\"\"\n",
    "    \n",
    "    def __init__(self, mean: float = 0.0, std: float = 0.1):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        noise = torch.randn_like(tensor) * self.std + self.mean\n",
    "        return tensor + noise\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
    "\n",
    "\n",
    "class Cutout:\n",
    "    \"\"\"Randomly mask out rectangular regions (cutout regularization)\"\"\"\n",
    "    \n",
    "    def __init__(self, n_holes: int = 1, length: int = 8):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "    \n",
    "    def __call__(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        h, w = img.shape[-2:]\n",
    "        mask = torch.ones_like(img)\n",
    "        \n",
    "        for _ in range(self.n_holes):\n",
    "            y = torch.randint(0, h, (1,)).item()\n",
    "            x = torch.randint(0, w, (1,)).item()\n",
    "            \n",
    "            y1 = max(0, y - self.length // 2)\n",
    "            y2 = min(h, y + self.length // 2)\n",
    "            x1 = max(0, x - self.length // 2)\n",
    "            x2 = min(w, x + self.length // 2)\n",
    "            \n",
    "            mask[..., y1:y2, x1:x2] = 0\n",
    "        \n",
    "        return img * mask\n",
    "\n",
    "\n",
    "# Use in a transform pipeline\n",
    "custom_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.1307,), (0.3081,)),\n",
    "    AddGaussianNoise(std=0.1),\n",
    "    Cutout(n_holes=2, length=6),\n",
    "])\n",
    "\n",
    "# Test custom transforms\n",
    "custom_dataset = MNIST(data_dir, train=True, download=True, transform=custom_transform)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = custom_dataset[i]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Custom augmented (Label: {label})')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 torchvision.transforms.v2 (Modern API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "# v2 transforms work on both images AND bounding boxes, masks, etc.\n",
    "modern_transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(224, scale=(0.8, 1.0), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    v2.ToImage(),  # Convert to TVTensor (enables efficient GPU augmentations)\n",
    "    v2.ToDtype(torch.float32, scale=True),  # Normalize to [0, 1]\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "print(\"v2 transforms offer:\")\n",
    "print(\"  - Consistent transforms across images, boxes, masks\")\n",
    "print(\"  - GPU-accelerated augmentations\")\n",
    "print(\"  - Better performance with TVTensor format\")\n",
    "print(\"  - Compound transforms (MixUp, CutMix)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 MixUp and CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 0.2):\n",
    "    \"\"\"\n",
    "    MixUp: Creates convex combinations of training examples.\n",
    "    \n",
    "    x_mixed = lambda * x_i + (1 - lambda) * x_j\n",
    "    y_mixed = lambda * y_i + (1 - lambda) * y_j\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"MixUp loss: weighted combination of losses\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "# Demonstrate MixUp\n",
    "batch_x, batch_y = next(iter(DataLoader(train_dataset, batch_size=4)))\n",
    "mixed_x, y_a, y_b, lam = mixup_data(batch_x, batch_y, alpha=0.4)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i in range(4):\n",
    "    axes[0, i].imshow(batch_x[i].squeeze(), cmap='gray')\n",
    "    axes[0, i].set_title(f'Original (y={batch_y[i].item()})')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(mixed_x[i].squeeze(), cmap='gray')\n",
    "    axes[1, i].set_title(f'Mixed ({lam:.2f}*{y_a[i].item()} + {1-lam:.2f}*{y_b[i].item()})')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(f'MixUp with lambda={lam:.2f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 1.0):\n",
    "    \"\"\"\n",
    "    CutMix: Cuts a patch from one image and pastes it onto another.\n",
    "    Labels are mixed proportionally to the area.\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    _, _, h, w = x.shape\n",
    "    \n",
    "    # Calculate cut size\n",
    "    cut_ratio = np.sqrt(1 - lam)\n",
    "    cut_h = int(h * cut_ratio)\n",
    "    cut_w = int(w * cut_ratio)\n",
    "    \n",
    "    # Random position for cut\n",
    "    cy = np.random.randint(h)\n",
    "    cx = np.random.randint(w)\n",
    "    \n",
    "    # Bound the cut region\n",
    "    y1 = np.clip(cy - cut_h // 2, 0, h)\n",
    "    y2 = np.clip(cy + cut_h // 2, 0, h)\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, w)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, w)\n",
    "    \n",
    "    # Apply CutMix\n",
    "    mixed_x = x.clone()\n",
    "    mixed_x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]\n",
    "    \n",
    "    # Adjust lambda based on actual cut area\n",
    "    lam = 1 - ((y2 - y1) * (x2 - x1) / (h * w))\n",
    "    \n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "# Demonstrate CutMix\n",
    "batch_x, batch_y = next(iter(DataLoader(train_dataset, batch_size=4)))\n",
    "cutmix_x, y_a, y_b, lam = cutmix_data(batch_x, batch_y, alpha=1.0)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i in range(4):\n",
    "    axes[0, i].imshow(batch_x[i].squeeze(), cmap='gray')\n",
    "    axes[0, i].set_title(f'Original (y={batch_y[i].item()})')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(cutmix_x[i].squeeze(), cmap='gray')\n",
    "    axes[1, i].set_title(f'CutMix ({lam:.2f}*{y_a[i].item()} + {1-lam:.2f}*{y_b[i].item()})')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(f'CutMix with effective lambda={lam:.2f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together\n",
    "\n",
    "Complete example with a real training pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteTrainingPipeline:\n",
    "    \"\"\"\n",
    "    A complete data pipeline demonstrating all concepts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = '../data',\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 2,\n",
    "        use_augmentation: bool = True,\n",
    "        use_mixup: bool = False,\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mixup = use_mixup\n",
    "        \n",
    "        # Define transforms\n",
    "        if use_augmentation:\n",
    "            self.train_transform = T.Compose([\n",
    "                T.RandomRotation(10),\n",
    "                T.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.1307,), (0.3081,)),\n",
    "            ])\n",
    "        else:\n",
    "            self.train_transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.1307,), (0.3081,)),\n",
    "            ])\n",
    "        \n",
    "        self.val_transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "        \n",
    "        # Load datasets\n",
    "        full_train = MNIST(data_dir, train=True, download=True, transform=self.train_transform)\n",
    "        self.test_dataset = MNIST(data_dir, train=False, download=True, transform=self.val_transform)\n",
    "        \n",
    "        # Split training into train/val\n",
    "        train_size = int(0.9 * len(full_train))\n",
    "        val_size = len(full_train) - train_size\n",
    "        \n",
    "        # Note: Using random_split with different transforms requires more care\n",
    "        # For simplicity, we use the same transform here\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(\n",
    "            full_train, [train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            persistent_workers=num_workers > 0,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            persistent_workers=num_workers > 0,\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "    \n",
    "    def get_batch(self, split: str = 'train'):\n",
    "        \"\"\"Get a single batch for debugging/visualization\"\"\"\n",
    "        loader = getattr(self, f'{split}_loader')\n",
    "        return next(iter(loader))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"CompleteTrainingPipeline(\\n\"\n",
    "            f\"  train_samples={len(self.train_dataset)},\\n\"\n",
    "            f\"  val_samples={len(self.val_dataset)},\\n\"\n",
    "            f\"  test_samples={len(self.test_dataset)},\\n\"\n",
    "            f\"  batch_size={self.batch_size},\\n\"\n",
    "            f\"  use_mixup={self.use_mixup}\\n\"\n",
    "            f\")\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = CompleteTrainingPipeline(\n",
    "    data_dir='../data',\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    use_augmentation=True,\n",
    "    use_mixup=False,\n",
    ")\n",
    "\n",
    "print(pipeline)\n",
    "\n",
    "# Get a batch\n",
    "batch_x, batch_y = pipeline.get_batch('train')\n",
    "print(f\"\\nBatch shape: {batch_x.shape}\")\n",
    "print(f\"Labels shape: {batch_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Stratified Train/Val Split\n",
    "\n",
    "Create a dataset wrapper that ensures train and validation splits have the same class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement stratified splitting\n",
    "\n",
    "def stratified_split(dataset, labels: List[int], val_ratio: float = 0.1):\n",
    "    \"\"\"\n",
    "    Split a dataset into train/val sets with the same class distribution.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to split\n",
    "        labels: List of labels for each sample\n",
    "        val_ratio: Fraction of data for validation\n",
    "    \n",
    "    Returns:\n",
    "        train_indices, val_indices: Lists of indices for each split\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Group indices by class, then sample from each class\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "# labels = [dataset[i][1] for i in range(len(dataset))]\n",
    "# train_idx, val_idx = stratified_split(dataset, labels, val_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a Dynamic Batch Sampler\n",
    "\n",
    "Create a sampler that adjusts batch size based on sequence length to maintain roughly constant memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Implement dynamic batching\n",
    "\n",
    "class DynamicBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Sampler that creates batches with approximately equal total tokens.\n",
    "    Longer sequences get smaller batches, shorter sequences get larger batches.\n",
    "    \n",
    "    Args:\n",
    "        lengths: List of sequence lengths\n",
    "        max_tokens: Maximum total tokens per batch\n",
    "        shuffle: Whether to shuffle the data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lengths: List[int], max_tokens: int = 1000, shuffle: bool = True):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: Sort by length, then create batches that don't exceed max_tokens\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Create a Multi-Dataset Loader\n",
    "\n",
    "Implement a loader that samples from multiple datasets with specified mixing ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Multi-dataset sampling\n",
    "\n",
    "class MultiDatasetLoader:\n",
    "    \"\"\"\n",
    "    Samples from multiple datasets according to specified ratios.\n",
    "    \n",
    "    Args:\n",
    "        datasets: List of datasets\n",
    "        ratios: Sampling ratios for each dataset (will be normalized)\n",
    "        batch_size: Batch size\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, datasets: List[Dataset], ratios: List[float], batch_size: int = 32):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: Create samplers for each dataset and interleave according to ratios\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Stratified Split\n",
    "\n",
    "def stratified_split(dataset, labels: List[int], val_ratio: float = 0.1):\n",
    "    \"\"\"\n",
    "    Split a dataset into train/val sets with the same class distribution.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Group indices by class\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    \n",
    "    # Sample from each class\n",
    "    for cls, indices in class_indices.items():\n",
    "        # Shuffle indices for this class\n",
    "        perm = torch.randperm(len(indices)).tolist()\n",
    "        shuffled = [indices[i] for i in perm]\n",
    "        \n",
    "        # Split\n",
    "        val_count = int(len(shuffled) * val_ratio)\n",
    "        val_indices.extend(shuffled[:val_count])\n",
    "        train_indices.extend(shuffled[val_count:])\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "\n",
    "\n",
    "# Test\n",
    "test_dataset = MNIST('../data', train=True, download=True)\n",
    "labels = [test_dataset[i][1] for i in range(len(test_dataset))]\n",
    "train_idx, val_idx = stratified_split(test_dataset, labels, val_ratio=0.2)\n",
    "\n",
    "print(f\"Train size: {len(train_idx)}, Val size: {len(val_idx)}\")\n",
    "print(f\"Train class dist: {Counter([labels[i] for i in train_idx])}\")\n",
    "print(f\"Val class dist: {Counter([labels[i] for i in val_idx])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Dynamic Batch Sampler\n",
    "\n",
    "class DynamicBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Sampler that creates batches with approximately equal total tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lengths: List[int], max_tokens: int = 1000, shuffle: bool = True):\n",
    "        self.lengths = lengths\n",
    "        self.max_tokens = max_tokens\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Pre-compute batches\n",
    "        self._create_batches()\n",
    "    \n",
    "    def _create_batches(self):\n",
    "        # Sort indices by length\n",
    "        sorted_indices = sorted(range(len(self.lengths)), key=lambda i: self.lengths[i])\n",
    "        \n",
    "        self.batches = []\n",
    "        current_batch = []\n",
    "        current_max_len = 0\n",
    "        \n",
    "        for idx in sorted_indices:\n",
    "            seq_len = self.lengths[idx]\n",
    "            new_max_len = max(current_max_len, seq_len)\n",
    "            new_batch_tokens = new_max_len * (len(current_batch) + 1)\n",
    "            \n",
    "            if new_batch_tokens > self.max_tokens and current_batch:\n",
    "                # Start new batch\n",
    "                self.batches.append(current_batch)\n",
    "                current_batch = [idx]\n",
    "                current_max_len = seq_len\n",
    "            else:\n",
    "                current_batch.append(idx)\n",
    "                current_max_len = new_max_len\n",
    "        \n",
    "        if current_batch:\n",
    "            self.batches.append(current_batch)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            perm = torch.randperm(len(self.batches)).tolist()\n",
    "            for i in perm:\n",
    "                yield from self.batches[i]\n",
    "        else:\n",
    "            for batch in self.batches:\n",
    "                yield from batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(len(b) for b in self.batches)\n",
    "\n",
    "\n",
    "# Test\n",
    "lengths = [10, 50, 15, 100, 25, 30, 80, 12, 45, 60]\n",
    "dyn_sampler = DynamicBatchSampler(lengths, max_tokens=150, shuffle=False)\n",
    "\n",
    "print(f\"Batches created: {len(dyn_sampler.batches)}\")\n",
    "for i, batch in enumerate(dyn_sampler.batches):\n",
    "    batch_lens = [lengths[j] for j in batch]\n",
    "    total_tokens = max(batch_lens) * len(batch)\n",
    "    print(f\"  Batch {i}: size={len(batch)}, lengths={batch_lens}, total_tokens={total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Multi-Dataset Loader\n",
    "\n",
    "class MultiDatasetLoader:\n",
    "    \"\"\"\n",
    "    Samples from multiple datasets according to specified ratios.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, datasets: List[Dataset], ratios: List[float], batch_size: int = 32):\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Normalize ratios\n",
    "        total = sum(ratios)\n",
    "        self.ratios = [r / total for r in ratios]\n",
    "        \n",
    "        # Calculate samples per dataset for one epoch\n",
    "        # Use the largest dataset as reference\n",
    "        max_size = max(len(d) for d in datasets)\n",
    "        self.samples_per_dataset = [\n",
    "            int(max_size * ratio) for ratio in self.ratios\n",
    "        ]\n",
    "        \n",
    "        # Create weighted sampler for dataset selection\n",
    "        self.total_samples = sum(self.samples_per_dataset)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # Create iterator for each dataset\n",
    "        samplers = [\n",
    "            iter(RandomSampler(d, replacement=True, num_samples=n))\n",
    "            for d, n in zip(self.datasets, self.samples_per_dataset)\n",
    "        ]\n",
    "        \n",
    "        # Create dataset selection weights\n",
    "        remaining = list(self.samples_per_dataset)\n",
    "        \n",
    "        batch = []\n",
    "        while sum(remaining) > 0:\n",
    "            # Select dataset proportionally to remaining samples\n",
    "            weights = [r / sum(remaining) if sum(remaining) > 0 else 0 for r in remaining]\n",
    "            dataset_idx = np.random.choice(len(self.datasets), p=weights)\n",
    "            \n",
    "            # Get sample\n",
    "            try:\n",
    "                sample_idx = next(samplers[dataset_idx])\n",
    "                batch.append(self.datasets[dataset_idx][sample_idx])\n",
    "                remaining[dataset_idx] -= 1\n",
    "                \n",
    "                if len(batch) == self.batch_size:\n",
    "                    yield default_collate(batch)\n",
    "                    batch = []\n",
    "            except StopIteration:\n",
    "                remaining[dataset_idx] = 0\n",
    "        \n",
    "        # Yield remaining\n",
    "        if batch:\n",
    "            yield default_collate(batch)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (self.total_samples + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "# Test with MNIST and FashionMNIST\n",
    "transform = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])\n",
    "mnist = MNIST('../data', train=True, download=True, transform=transform)\n",
    "fashion = FashionMNIST('../data', train=True, download=True, transform=transform)\n",
    "\n",
    "multi_loader = MultiDatasetLoader(\n",
    "    datasets=[mnist, fashion],\n",
    "    ratios=[0.7, 0.3],  # 70% MNIST, 30% FashionMNIST\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"Total batches: {len(multi_loader)}\")\n",
    "print(f\"Samples per dataset: {multi_loader.samples_per_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Dataset Types**:\n",
    "   - `Dataset` (map-style): Index-based, use for most cases\n",
    "   - `IterableDataset`: For streaming data, requires worker-aware splitting\n",
    "\n",
    "2. **DataLoader Components**:\n",
    "   - Sampler determines **which** indices to load\n",
    "   - Collate function determines **how** to batch samples\n",
    "   - Multi-worker loading parallelizes I/O\n",
    "\n",
    "3. **Performance Optimization**:\n",
    "   - Use `num_workers > 0` for disk-bound loading\n",
    "   - Enable `pin_memory=True` for GPU training\n",
    "   - Set `persistent_workers=True` to avoid startup overhead\n",
    "\n",
    "4. **Class Imbalance**:\n",
    "   - `WeightedRandomSampler` for oversampling minority classes\n",
    "   - Custom samplers for curriculum learning, etc.\n",
    "\n",
    "5. **Data Augmentation**:\n",
    "   - Use `torchvision.transforms` for standard augmentations\n",
    "   - MixUp/CutMix improve generalization\n",
    "   - Augment training data only, not validation/test\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "- Forgetting to handle multi-worker splitting in `IterableDataset`\n",
    "- Using `shuffle=True` with custom sampler (mutually exclusive)\n",
    "- Not accounting for variable-length sequences in collation\n",
    "- Applying augmentation to validation data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
