{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.3: nn.Module Architecture\n",
    "\n",
    "`nn.Module` is the base class for all neural network components in PyTorch. Understanding its internals is essential for:\n",
    "- Building custom layers and architectures\n",
    "- Debugging model behavior\n",
    "- Properly managing model state (training vs evaluation)\n",
    "- Implementing advanced patterns (hooks, custom initialization)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how `nn.Module` tracks parameters and submodules\n",
    "- Master parameter registration and access patterns\n",
    "- Use forward and backward hooks effectively\n",
    "- Properly handle train/eval modes and their implications\n",
    "- Implement custom layers with learnable parameters\n",
    "- Apply proper weight initialization strategies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The nn.Module Basics\n",
    "\n",
    "Every neural network in PyTorch inherits from `nn.Module`. Let's understand what it provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest possible module\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # ALWAYS call super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * 2\n",
    "\n",
    "model = SimpleModule()\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = model(x)  # Calls forward() via __call__\n",
    "print(f\"Output: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why use __call__ instead of forward directly?\n",
    "# __call__ does more than just forward():\n",
    "# 1. Runs registered hooks\n",
    "# 2. Handles autograd properly\n",
    "# 3. Manages module state\n",
    "\n",
    "# NEVER call forward() directly - always use model(x)\n",
    "print(\"Always use model(x), not model.forward(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What nn.Module Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module maintains several internal dictionaries\n",
    "class DemoModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 5)  # Submodule\n",
    "        self.my_param = nn.Parameter(torch.randn(3))  # Parameter\n",
    "        self.register_buffer('my_buffer', torch.zeros(4))  # Buffer\n",
    "        self.some_value = 42  # Regular Python attribute\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = DemoModule()\n",
    "\n",
    "print(\"Tracked by nn.Module:\")\n",
    "print(f\"  _modules: {list(model._modules.keys())}\")\n",
    "print(f\"  _parameters: {list(model._parameters.keys())}\")\n",
    "print(f\"  _buffers: {list(model._buffers.keys())}\")\n",
    "print(f\"\\nRegular attribute (not tracked): some_value = {model.some_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Parameters\n",
    "\n",
    "Parameters are tensors that require gradients and are registered with the module for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Parameter is a Tensor subclass that auto-registers with the module\n",
    "param = nn.Parameter(torch.randn(3, 4))\n",
    "\n",
    "print(f\"Is a tensor: {isinstance(param, torch.Tensor)}\")\n",
    "print(f\"requires_grad: {param.requires_grad}\")  # True by default\n",
    "print(f\"Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters are automatically registered when assigned as attributes\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # These get registered automatically\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.weight.T + self.bias\n",
    "\n",
    "layer = MyLayer(10, 5)\n",
    "print(\"Registered parameters:\")\n",
    "for name, param in layer.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular tensors are NOT registered\n",
    "class BrokenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # This is a regular tensor, NOT a parameter!\n",
    "        self.weight = torch.randn(5, 10, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.weight.T\n",
    "\n",
    "broken = BrokenLayer()\n",
    "print(f\"Number of parameters: {sum(1 for _ in broken.parameters())}\")\n",
    "print(\"The weight won't be optimized or saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Accessing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a more complex model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 20)\n",
    "        self.layer2 = nn.Linear(20, 15)\n",
    "        self.layer3 = nn.Linear(15, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters() - iterator over all parameters (recursive)\n",
    "print(\"All parameters:\")\n",
    "for i, param in enumerate(model.parameters()):\n",
    "    print(f\"  {i}: {param.shape}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal: {total_params} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named_parameters() - includes the name\n",
    "print(\"Named parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access specific parameter by name\n",
    "print(f\"layer1.weight shape: {model.layer1.weight.shape}\")\n",
    "print(f\"layer1.bias shape: {model.layer1.bias.shape}\")\n",
    "\n",
    "# Or use get_parameter (more programmatic)\n",
    "param = model.get_parameter('layer2.weight')\n",
    "print(f\"\\nget_parameter('layer2.weight'): {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering parameters (e.g., for different learning rates)\n",
    "def get_layer_groups(model):\n",
    "    \"\"\"Group parameters by layer for different learning rates.\"\"\"\n",
    "    early_layers = []\n",
    "    late_layers = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'layer1' in name or 'layer2' in name:\n",
    "            early_layers.append(param)\n",
    "        else:\n",
    "            late_layers.append(param)\n",
    "    \n",
    "    return early_layers, late_layers\n",
    "\n",
    "early, late = get_layer_groups(model)\n",
    "print(f\"Early layers: {len(early)} tensors\")\n",
    "print(f\"Late layers: {len(late)} tensors\")\n",
    "\n",
    "# Use with optimizer for differential learning rates:\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': early, 'lr': 1e-4},\n",
    "#     {'params': late, 'lr': 1e-3}\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Freezing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze parameters by setting requires_grad = False\n",
    "model = MLP()\n",
    "\n",
    "# Freeze layer1\n",
    "for param in model.layer1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"After freezing layer1:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: requires_grad = {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only pass trainable parameters to optimizer\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=0.001)\n",
    "\n",
    "# Or use list comprehension\n",
    "trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"Trainable parameters: {len(trainable)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Submodules\n",
    "\n",
    "Modules can contain other modules, creating a tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submodules are automatically registered when assigned as attributes\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.norm(F.relu(self.linear(x)) + x)  # Residual\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, dim, num_blocks):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(10, dim)\n",
    "        self.blocks = nn.ModuleList([Block(dim) for _ in range(num_blocks)])\n",
    "        self.head = nn.Linear(dim, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.head(x)\n",
    "\n",
    "model = Network(dim=64, num_blocks=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over submodules\n",
    "print(\"Direct children (modules()):\")\n",
    "for name, module in model.named_modules():\n",
    "    print(f\"  {name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# children() vs modules()\n",
    "print(\"children() - direct children only:\")\n",
    "for name, child in model.named_children():\n",
    "    print(f\"  {name}: {child.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nmodules() - all modules recursively (shown above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ModuleList vs Python List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList properly registers modules\n",
    "class GoodModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(10, 10) for _ in range(3)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Python list does NOT register modules!\n",
    "class BadModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(10, 10) for _ in range(3)]  # Regular list!\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "good = GoodModel()\n",
    "bad = BadModel()\n",
    "\n",
    "print(f\"GoodModel parameters: {sum(p.numel() for p in good.parameters())}\")\n",
    "print(f\"BadModel parameters: {sum(p.numel() for p in bad.parameters())}\")\n",
    "print(\"\\nBadModel layers won't be saved, loaded to GPU, or optimized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleDict for named access\n",
    "class MultiHeadModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Linear(10, 20)\n",
    "        self.heads = nn.ModuleDict({\n",
    "            'classification': nn.Linear(20, 10),\n",
    "            'regression': nn.Linear(20, 1),\n",
    "            'embedding': nn.Linear(20, 64)\n",
    "        })\n",
    "    \n",
    "    def forward(self, x, head_name):\n",
    "        x = F.relu(self.backbone(x))\n",
    "        return self.heads[head_name](x)\n",
    "\n",
    "model = MultiHeadModel()\n",
    "x = torch.randn(5, 10)\n",
    "print(f\"Classification output: {model(x, 'classification').shape}\")\n",
    "print(f\"Regression output: {model(x, 'regression').shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Buffers\n",
    "\n",
    "Buffers are tensors that should be part of the module state but don't require gradients (not optimized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cases for buffers:\n",
    "# - Running statistics (BatchNorm)\n",
    "# - Position encodings\n",
    "# - Masks that should move with the model\n",
    "\n",
    "class BatchNormLike(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        # Learnable parameters\n",
    "        self.weight = nn.Parameter(torch.ones(num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        \n",
    "        # Buffers - saved with model, move with .to(device), but not optimized\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "        self.register_buffer('num_batches_tracked', torch.tensor(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=0)\n",
    "            var = x.var(dim=0)\n",
    "            # Update running stats\n",
    "            self.running_mean = 0.9 * self.running_mean + 0.1 * mean\n",
    "            self.running_var = 0.9 * self.running_var + 0.1 * var\n",
    "            self.num_batches_tracked += 1\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        \n",
    "        x_norm = (x - mean) / (var + 1e-5).sqrt()\n",
    "        return self.weight * x_norm + self.bias\n",
    "\n",
    "layer = BatchNormLike(10)\n",
    "print(\"Parameters (optimized):\")\n",
    "for name, param in layer.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "print(\"\\nBuffers (not optimized):\")\n",
    "for name, buf in layer.named_buffers():\n",
    "    print(f\"  {name}: {buf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffers move with the model\n",
    "print(f\"Before: running_mean device = {layer.running_mean.device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    layer = layer.cuda()\n",
    "    print(f\"After .cuda(): running_mean device = {layer.running_mean.device}\")\n",
    "    layer = layer.cpu()\n",
    "\n",
    "# Buffers are saved with state_dict\n",
    "state = layer.state_dict()\n",
    "print(f\"\\nState dict keys: {list(state.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistent vs non-persistent buffers\n",
    "class LayerWithBuffers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Persistent (default) - saved in state_dict\n",
    "        self.register_buffer('saved_buffer', torch.zeros(5))\n",
    "        \n",
    "        # Non-persistent - NOT saved in state_dict\n",
    "        self.register_buffer('temp_buffer', torch.zeros(5), persistent=False)\n",
    "\n",
    "layer = LayerWithBuffers()\n",
    "print(f\"state_dict keys: {list(layer.state_dict().keys())}\")\n",
    "print(\"Note: temp_buffer is not in state_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Train vs Eval Mode\n",
    "\n",
    "Modules can behave differently during training vs inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.BatchNorm1d(20),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "print(f\"Default training mode: {model.training}\")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(f\"After eval(): {model.training}\")\n",
    "\n",
    "model.train()  # Set back to training mode\n",
    "print(f\"After train(): {model.training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()/eval() propagates to all submodules\n",
    "model.eval()\n",
    "for name, module in model.named_modules():\n",
    "    if name:\n",
    "        print(f\"{name}: training = {module.training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Modules Affected by train/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout: disabled in eval mode\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "x = torch.ones(10)\n",
    "\n",
    "dropout.train()\n",
    "print(f\"Training mode: {dropout(x)}\")\n",
    "\n",
    "dropout.eval()\n",
    "print(f\"Eval mode: {dropout(x)}\")  # No dropout applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm: uses batch stats (train) vs running stats (eval)\n",
    "bn = nn.BatchNorm1d(3)\n",
    "\n",
    "# Fake training to build running stats\n",
    "bn.train()\n",
    "for _ in range(100):\n",
    "    x = torch.randn(32, 3) * 2 + 5  # Mean ~5, Std ~2\n",
    "    _ = bn(x)\n",
    "\n",
    "print(f\"Running mean: {bn.running_mean}\")\n",
    "print(f\"Running var: {bn.running_var}\")\n",
    "\n",
    "# In eval mode, uses these stats instead of batch stats\n",
    "bn.eval()\n",
    "x_test = torch.randn(1, 3) * 2 + 5\n",
    "y = bn(x_test)\n",
    "print(f\"\\nEval output mean (should be ~0): {y.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common pattern: combine eval() with no_grad()\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "def inference(model, x):\n",
    "    model.eval()  # Disable dropout, use running stats\n",
    "    with torch.no_grad():  # Don't compute gradients\n",
    "        output = model(x)\n",
    "    model.train()  # Set back if continuing training\n",
    "    return output\n",
    "\n",
    "x = torch.randn(5, 10)\n",
    "predictions = inference(model, x)\n",
    "print(f\"Predictions shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Hooks\n",
    "\n",
    "Hooks let you execute custom code during forward or backward passes without modifying the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Forward Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward hook signature: hook(module, input, output) -> None or modified output\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "# Storage for activations\n",
    "activations = {}\n",
    "\n",
    "def save_activation(name):\n",
    "    def hook(module, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks\n",
    "model[0].register_forward_hook(save_activation('linear1'))\n",
    "model[1].register_forward_hook(save_activation('relu'))\n",
    "model[2].register_forward_hook(save_activation('linear2'))\n",
    "\n",
    "# Run forward pass\n",
    "x = torch.randn(5, 10)\n",
    "output = model(x)\n",
    "\n",
    "print(\"Captured activations:\")\n",
    "for name, act in activations.items():\n",
    "    print(f\"  {name}: {act.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward hook that modifies output\n",
    "def add_noise_hook(module, input, output):\n",
    "    noise = torch.randn_like(output) * 0.1\n",
    "    return output + noise  # Return modified output\n",
    "\n",
    "model = nn.Linear(10, 5)\n",
    "handle = model.register_forward_hook(add_noise_hook)\n",
    "\n",
    "x = torch.randn(3, 10)\n",
    "y1 = model(x)\n",
    "y2 = model(x)\n",
    "print(f\"Outputs differ due to noise: {not torch.allclose(y1, y2)}\")\n",
    "\n",
    "# Remove the hook\n",
    "handle.remove()\n",
    "y3 = model(x)\n",
    "y4 = model(x)\n",
    "print(f\"After removing hook, outputs same: {torch.allclose(y3, y4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Backward Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full backward hook signature: hook(module, grad_input, grad_output) -> tuple or None\n",
    "\n",
    "gradients = {}\n",
    "\n",
    "def save_gradients(name):\n",
    "    def hook(module, grad_input, grad_output):\n",
    "        gradients[name] = {\n",
    "            'input': [g.detach() if g is not None else None for g in grad_input],\n",
    "            'output': [g.detach() for g in grad_output]\n",
    "        }\n",
    "    return hook\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "# Register backward hooks\n",
    "model[0].register_full_backward_hook(save_gradients('linear1'))\n",
    "model[2].register_full_backward_hook(save_gradients('linear2'))\n",
    "\n",
    "# Forward and backward\n",
    "x = torch.randn(5, 10)\n",
    "output = model(x)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"Captured gradients:\")\n",
    "for name, grads in gradients.items():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    grad_output: {[g.shape for g in grads['output']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use case: Gradient clipping per layer\n",
    "def clip_gradient_hook(max_norm):\n",
    "    def hook(module, grad_input, grad_output):\n",
    "        clipped = []\n",
    "        for g in grad_input:\n",
    "            if g is not None:\n",
    "                norm = g.norm()\n",
    "                if norm > max_norm:\n",
    "                    g = g * max_norm / norm\n",
    "            clipped.append(g)\n",
    "        return tuple(clipped)\n",
    "    return hook\n",
    "\n",
    "model = nn.Linear(10, 5)\n",
    "model.register_full_backward_hook(clip_gradient_hook(max_norm=1.0))\n",
    "\n",
    "x = torch.randn(3, 10) * 100  # Large input -> large gradients\n",
    "y = model(x)\n",
    "y.sum().backward()\n",
    "\n",
    "# Gradients should be clipped\n",
    "print(f\"Weight gradient norm: {model.weight.grad.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Forward Pre-Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-hook runs before forward, can modify inputs\n",
    "# Signature: hook(module, input) -> None or modified input\n",
    "\n",
    "def normalize_input_hook(module, input):\n",
    "    x = input[0]  # input is a tuple\n",
    "    normalized = (x - x.mean()) / (x.std() + 1e-5)\n",
    "    return (normalized,)  # Return tuple\n",
    "\n",
    "model = nn.Linear(10, 5)\n",
    "model.register_forward_pre_hook(normalize_input_hook)\n",
    "\n",
    "x = torch.randn(3, 10) * 100 + 50  # Unnormalized\n",
    "y = model(x)  # Input will be normalized before forward\n",
    "\n",
    "print(f\"Original input mean: {x.mean():.2f}, std: {x.std():.2f}\")\n",
    "print(\"Input was normalized by pre-hook before linear layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Weight Initialization\n",
    "\n",
    "Proper initialization is crucial for training deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in initializers\n",
    "linear = nn.Linear(100, 50)\n",
    "\n",
    "# Xavier/Glorot initialization (good for tanh/sigmoid)\n",
    "nn.init.xavier_uniform_(linear.weight)\n",
    "print(f\"Xavier uniform std: {linear.weight.std():.4f}\")\n",
    "\n",
    "nn.init.xavier_normal_(linear.weight)\n",
    "print(f\"Xavier normal std: {linear.weight.std():.4f}\")\n",
    "\n",
    "# Kaiming/He initialization (good for ReLU)\n",
    "nn.init.kaiming_uniform_(linear.weight, mode='fan_in', nonlinearity='relu')\n",
    "print(f\"Kaiming uniform std: {linear.weight.std():.4f}\")\n",
    "\n",
    "nn.init.kaiming_normal_(linear.weight, mode='fan_in', nonlinearity='relu')\n",
    "print(f\"Kaiming normal std: {linear.weight.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other common initializers\n",
    "linear = nn.Linear(100, 50)\n",
    "\n",
    "nn.init.zeros_(linear.bias)           # All zeros\n",
    "nn.init.ones_(linear.weight)          # All ones\n",
    "nn.init.constant_(linear.bias, 0.1)   # Constant value\n",
    "nn.init.normal_(linear.weight, mean=0, std=0.02)  # Normal distribution\n",
    "nn.init.uniform_(linear.weight, a=-0.1, b=0.1)    # Uniform distribution\n",
    "nn.init.orthogonal_(linear.weight)    # Orthogonal matrix\n",
    "\n",
    "print(\"Various initializations applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize entire network with apply()\n",
    "def init_weights(module):\n",
    "    \"\"\"Custom initialization function.\"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.BatchNorm1d):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.BatchNorm1d(20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "# Apply to all submodules\n",
    "model.apply(init_weights)\n",
    "\n",
    "print(\"Initialization applied to all layers:\")\n",
    "print(f\"  Linear weight std: {model[0].weight.std():.4f}\")\n",
    "print(f\"  Linear bias: {model[0].bias[:3]}\")\n",
    "print(f\"  BatchNorm weight: {model[1].weight[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Saving and Loading\n",
    "\n",
    "Understanding state_dict is essential for model persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.BatchNorm1d(20),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "# state_dict contains all parameters and buffers\n",
    "state = model.state_dict()\n",
    "print(\"State dict keys:\")\n",
    "for key in state.keys():\n",
    "    print(f\"  {key}: {state[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create temp file\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.pt') as f:\n",
    "    path = f.name\n",
    "\n",
    "# Save state dict\n",
    "torch.save(model.state_dict(), path)\n",
    "print(f\"Saved to {path}\")\n",
    "\n",
    "# Create new model and load\n",
    "model2 = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.BatchNorm1d(20),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "model2.load_state_dict(torch.load(path))\n",
    "print(\"Loaded successfully\")\n",
    "\n",
    "# Verify\n",
    "print(f\"Weights match: {torch.allclose(model[0].weight, model2[0].weight)}\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial loading with strict=False\n",
    "# Useful for transfer learning\n",
    "\n",
    "# Original model\n",
    "model1 = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "# New model with extra layer\n",
    "model2 = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.Linear(20, 15),  # Different!\n",
    "    nn.Linear(15, 5)    # Extra layer\n",
    ")\n",
    "\n",
    "# This would fail with strict=True\n",
    "missing, unexpected = model2.load_state_dict(model1.state_dict(), strict=False)\n",
    "print(f\"Missing keys: {missing}\")\n",
    "print(f\"Unexpected keys: {unexpected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Custom Linear Layer\n",
    "\n",
    "Implement a custom linear layer from scratch with proper parameter registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement a linear layer: output = input @ weight.T + bias\n",
    "    \n",
    "    Args:\n",
    "        in_features: size of input\n",
    "        out_features: size of output\n",
    "        bias: whether to include bias term\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE:\n",
    "        # 1. Create weight as nn.Parameter with shape (out_features, in_features)\n",
    "        # 2. Create bias as nn.Parameter with shape (out_features,) if bias=True, else None\n",
    "        # 3. Initialize weights using Kaiming uniform\n",
    "        # 4. Initialize bias to zeros\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# layer = CustomLinear(10, 5)\n",
    "# x = torch.randn(3, 10)\n",
    "# y = layer(x)\n",
    "# print(f\"Output shape: {y.shape}\")  # Should be (3, 5)\n",
    "# print(f\"Parameters: {list(layer.named_parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature Extraction with Hooks\n",
    "\n",
    "Create a feature extractor that captures intermediate activations from a pretrained-style model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract features from specified layers of a model.\n",
    "    \n",
    "    Usage:\n",
    "        model = create_model()\n",
    "        extractor = FeatureExtractor(model, ['layer1', 'layer2.relu'])\n",
    "        output = model(x)\n",
    "        features = extractor.get_features()  # Dict of layer_name -> activation\n",
    "    \"\"\"\n",
    "    def __init__(self, model, layer_names):\n",
    "        self.model = model\n",
    "        self.layer_names = layer_names\n",
    "        self.features = {}\n",
    "        self.handles = []\n",
    "        \n",
    "        # YOUR CODE HERE:\n",
    "        # 1. For each layer_name, get the corresponding module\n",
    "        # 2. Register a forward hook that saves the output\n",
    "        # 3. Store the handle so we can remove it later\n",
    "        pass\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "\n",
    "# Test\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(10, 20),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(20, 5)\n",
    "# )\n",
    "# extractor = FeatureExtractor(model, ['0', '1'])\n",
    "# output = model(torch.randn(3, 10))\n",
    "# features = extractor.get_features()\n",
    "# print(f\"Extracted features: {list(features.keys())}\")\n",
    "# extractor.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Layer with Learned Normalization\n",
    "\n",
    "Implement a layer that learns to normalize its input differently for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    A layer that learns per-feature normalization.\n",
    "    \n",
    "    For each feature i:\n",
    "        output[..., i] = (input[..., i] - learned_mean[i]) / (learned_std[i] + eps)\n",
    "    \n",
    "    Both mean and std are learnable parameters.\n",
    "    \n",
    "    Args:\n",
    "        num_features: number of features\n",
    "        eps: small constant for numerical stability\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # YOUR CODE HERE:\n",
    "        # 1. Create learned_mean as Parameter, initialized to zeros\n",
    "        # 2. Create learned_std as Parameter, initialized to ones\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        # Normalize: (x - mean) / (std + eps)\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# layer = LearnedNorm(10)\n",
    "# x = torch.randn(5, 10) * 2 + 3  # Offset distribution\n",
    "# y = layer(x)\n",
    "# print(f\"Output mean (before training): {y.mean():.4f}\")\n",
    "# print(f\"Learnable parameters: {[n for n, p in layer.named_parameters()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "class CustomLinearSolution(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in = self.in_features\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = x @ self.weight.T\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "\n",
    "print(\"Exercise 1 Solution:\")\n",
    "layer = CustomLinearSolution(10, 5)\n",
    "x = torch.randn(3, 10)\n",
    "y = layer(x)\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Parameters: {[n for n, p in layer.named_parameters()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "class FeatureExtractorSolution:\n",
    "    def __init__(self, model, layer_names):\n",
    "        self.model = model\n",
    "        self.layer_names = layer_names\n",
    "        self.features = {}\n",
    "        self.handles = []\n",
    "        \n",
    "        for name in layer_names:\n",
    "            # Navigate to the module\n",
    "            module = model\n",
    "            for part in name.split('.'):\n",
    "                if part.isdigit():\n",
    "                    module = module[int(part)]\n",
    "                else:\n",
    "                    module = getattr(module, part)\n",
    "            \n",
    "            # Register hook\n",
    "            handle = module.register_forward_hook(self._make_hook(name))\n",
    "            self.handles.append(handle)\n",
    "    \n",
    "    def _make_hook(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.features[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "\n",
    "print(\"\\nExercise 2 Solution:\")\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "extractor = FeatureExtractorSolution(model, ['0', '1'])\n",
    "output = model(torch.randn(3, 10))\n",
    "features = extractor.get_features()\n",
    "print(f\"Extracted features: {list(features.keys())}\")\n",
    "for name, feat in features.items():\n",
    "    print(f\"  {name}: {feat.shape}\")\n",
    "extractor.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution\n",
    "class LearnedNormSolution(nn.Module):\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.learned_mean = nn.Parameter(torch.zeros(num_features))\n",
    "        self.learned_std = nn.Parameter(torch.ones(num_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (x - self.learned_mean) / (self.learned_std + self.eps)\n",
    "\n",
    "print(\"\\nExercise 3 Solution:\")\n",
    "layer = LearnedNormSolution(10)\n",
    "x = torch.randn(5, 10) * 2 + 3\n",
    "y = layer(x)\n",
    "print(f\"Input mean: {x.mean():.4f}, std: {x.std():.4f}\")\n",
    "print(f\"Output mean: {y.mean():.4f}, std: {y.std():.4f}\")\n",
    "print(f\"Learnable parameters: {[n for n, p in layer.named_parameters()]}\")\n",
    "\n",
    "# The layer can learn to shift/scale appropriately during training\n",
    "# to normalize inputs to a better range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Key takeaways from this notebook:\n",
    "\n",
    "1. **nn.Module** is the base class for all neural network components\n",
    "2. **Parameters** are registered automatically when assigned as `nn.Parameter`\n",
    "3. **Submodules** must use `nn.ModuleList`/`nn.ModuleDict`, not Python lists/dicts\n",
    "4. **Buffers** are for non-learnable state that should be saved and moved with the model\n",
    "5. **train()/eval()** affects Dropout, BatchNorm, and other mode-dependent layers\n",
    "6. **Hooks** enable inspection and modification of forward/backward passes\n",
    "7. **Proper initialization** is crucial for training deep networks\n",
    "8. **state_dict** enables saving, loading, and transfer learning\n",
    "\n",
    "---\n",
    "*Next: Module 2.1 - The Training Loop Deconstructed*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
