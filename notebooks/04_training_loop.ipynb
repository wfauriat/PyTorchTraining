{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2.1: The Training Loop Deconstructed\n",
    "\n",
    "The training loop is where deep learning happens. Understanding each component deeply enables you to:\n",
    "- Debug training issues effectively\n",
    "- Implement custom training procedures\n",
    "- Optimize for speed and memory\n",
    "- Adapt to different training paradigms\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand each step of the training loop and why it matters\n",
    "- Master loss functions and implement custom ones\n",
    "- Understand optimizers: SGD, Adam, AdamW and their differences\n",
    "- Use learning rate schedulers effectively\n",
    "- Apply gradient clipping for stability\n",
    "- Handle numerical issues in training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Anatomy of the Training Loop\n",
    "\n",
    "Every training loop follows this pattern:\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # 1. Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 3. Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 4. Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "Let's understand each step deeply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset\n",
    "def create_synthetic_data(n_samples=1000, n_features=10, n_classes=3):\n",
    "    \"\"\"Create a synthetic classification dataset.\"\"\"\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    # Create linearly separable classes\n",
    "    true_weights = torch.randn(n_features, n_classes)\n",
    "    logits = X @ true_weights\n",
    "    y = logits.argmax(dim=1)\n",
    "    return X, y\n",
    "\n",
    "X, y = create_synthetic_data()\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Batch count: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Zero Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(10, 3)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Without zeroing, gradients accumulate\n",
    "for i in range(3):\n",
    "    x = torch.randn(5, 10)\n",
    "    y = model(x).sum()\n",
    "    y.backward()\n",
    "    print(f\"Iteration {i+1}, weight grad norm: {model.weight.grad.norm():.4f}\")\n",
    "\n",
    "print(\"\\nGradients accumulated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper pattern: zero gradients before backward\n",
    "model = nn.Linear(10, 3)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(3):\n",
    "    optimizer.zero_grad()  # or model.zero_grad()\n",
    "    x = torch.randn(5, 10)\n",
    "    y = model(x).sum()\n",
    "    y.backward()\n",
    "    print(f\"Iteration {i+1}, weight grad norm: {model.weight.grad.norm():.4f}\")\n",
    "\n",
    "print(\"\\nGradients are fresh each iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_to_none=True can be slightly faster (avoids zero fill)\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# Now gradients are None instead of zeros\n",
    "print(f\"After set_to_none=True: grad = {model.weight.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass builds the computation graph\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 3)\n",
    ")\n",
    "\n",
    "x = torch.randn(5, 10)\n",
    "output = model(x)  # Graph is built during this call\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output has grad_fn: {output.grad_fn is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function reduces outputs to a scalar\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "targets = torch.randint(0, 3, (5,))  # Class labels\n",
    "loss = criterion(output, targets)\n",
    "\n",
    "print(f\"Loss value: {loss.item():.4f}\")\n",
    "print(f\"Loss is scalar: {loss.shape == torch.Size([])}\")\n",
    "print(f\"Loss requires grad: {loss.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward computes gradients for all parameters\n",
    "print(\"Before backward:\")\n",
    "print(f\"  model[0].weight.grad: {model[0].weight.grad}\")\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nAfter backward:\")\n",
    "print(f\"  model[0].weight.grad shape: {model[0].weight.grad.shape}\")\n",
    "print(f\"  model[0].weight.grad norm: {model[0].weight.grad.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Update Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.step() updates parameters using computed gradients\n",
    "print(\"Before step:\")\n",
    "print(f\"  model[0].weight[0,:3]: {model[0].weight.data[0,:3]}\")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\nAfter step:\")\n",
    "print(f\"  model[0].weight[0,:3]: {model[0].weight.data[0,:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # 1. Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 3. Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 4. Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return total_loss / total, 100. * correct / total\n",
    "\n",
    "# Initialize\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 3)\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train\n",
    "print(\"Training:\")\n",
    "for epoch in range(10):\n",
    "    loss, acc = train_epoch(model, dataloader, criterion, optimizer, device)\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Loss = {loss:.4f}, Accuracy = {acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Loss Functions\n",
    "\n",
    "Loss functions measure how far predictions are from targets. Understanding them helps you choose the right one and create custom losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Common Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression losses\n",
    "predictions = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "targets = torch.tensor([1.1, 2.2, 2.8, 4.5])\n",
    "\n",
    "# Mean Squared Error (L2)\n",
    "mse = nn.MSELoss()\n",
    "print(f\"MSE: {mse(predictions, targets):.4f}\")\n",
    "print(f\"Manual: {((predictions - targets) ** 2).mean():.4f}\")\n",
    "\n",
    "# Mean Absolute Error (L1)\n",
    "mae = nn.L1Loss()\n",
    "print(f\"\\nMAE: {mae(predictions, targets):.4f}\")\n",
    "print(f\"Manual: {(predictions - targets).abs().mean():.4f}\")\n",
    "\n",
    "# Smooth L1 (Huber) - combines L1 and L2\n",
    "smooth_l1 = nn.SmoothL1Loss()\n",
    "print(f\"\\nSmooth L1: {smooth_l1(predictions, targets):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification losses\n",
    "\n",
    "# Binary Cross Entropy (for binary classification)\n",
    "# Input should be probabilities (after sigmoid)\n",
    "probs = torch.tensor([0.9, 0.1, 0.8, 0.3])\n",
    "binary_targets = torch.tensor([1.0, 0.0, 1.0, 0.0])\n",
    "\n",
    "bce = nn.BCELoss()\n",
    "print(f\"BCE: {bce(probs, binary_targets):.4f}\")\n",
    "\n",
    "# BCE with logits (more numerically stable)\n",
    "logits = torch.tensor([2.0, -2.0, 1.5, -1.0])\n",
    "bce_logits = nn.BCEWithLogitsLoss()\n",
    "print(f\"BCE with logits: {bce_logits(logits, binary_targets):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy for multi-class\n",
    "# Input: (N, C) logits, Targets: (N,) class indices\n",
    "logits = torch.tensor([\n",
    "    [2.0, 1.0, 0.1],   # Predicts class 0\n",
    "    [0.5, 2.5, 0.3],   # Predicts class 1\n",
    "    [0.1, 0.2, 3.0],   # Predicts class 2\n",
    "])\n",
    "targets = torch.tensor([0, 1, 2])  # All correct\n",
    "\n",
    "ce = nn.CrossEntropyLoss()\n",
    "print(f\"Cross Entropy (all correct): {ce(logits, targets):.4f}\")\n",
    "\n",
    "# Wrong predictions\n",
    "wrong_targets = torch.tensor([2, 0, 1])\n",
    "print(f\"Cross Entropy (all wrong): {ce(logits, wrong_targets):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy = LogSoftmax + NLLLoss\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "nll_loss = nn.NLLLoss()\n",
    "\n",
    "log_probs = log_softmax(logits)\n",
    "manual_ce = nll_loss(log_probs, targets)\n",
    "\n",
    "print(f\"CrossEntropyLoss: {ce(logits, targets):.4f}\")\n",
    "print(f\"LogSoftmax + NLLLoss: {manual_ce:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loss Function Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction modes\n",
    "predictions = torch.tensor([1.0, 2.0, 3.0])\n",
    "targets = torch.tensor([1.5, 2.5, 3.5])\n",
    "\n",
    "# Mean (default)\n",
    "mse_mean = nn.MSELoss(reduction='mean')\n",
    "print(f\"Mean: {mse_mean(predictions, targets):.4f}\")\n",
    "\n",
    "# Sum\n",
    "mse_sum = nn.MSELoss(reduction='sum')\n",
    "print(f\"Sum: {mse_sum(predictions, targets):.4f}\")\n",
    "\n",
    "# None (per-element)\n",
    "mse_none = nn.MSELoss(reduction='none')\n",
    "print(f\"None: {mse_none(predictions, targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights for imbalanced data\n",
    "# If class 0 is rare, give it higher weight\n",
    "weights = torch.tensor([2.0, 1.0, 1.0])  # Class 0 counts double\n",
    "ce_weighted = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "logits = torch.randn(10, 3)\n",
    "targets = torch.randint(0, 3, (10,))\n",
    "\n",
    "print(f\"Unweighted loss: {ce(logits, targets):.4f}\")\n",
    "print(f\"Weighted loss: {ce_weighted(logits, targets):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label smoothing (regularization technique)\n",
    "# Instead of hard targets [0, 1, 0], use soft [0.1, 0.8, 0.1]\n",
    "ce_smooth = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "logits = torch.tensor([[3.0, 0.5, 0.1]])  # Very confident\n",
    "targets = torch.tensor([0])\n",
    "\n",
    "print(f\"Without smoothing: {ce(logits, targets):.4f}\")\n",
    "print(f\"With smoothing: {ce_smooth(logits, targets):.4f}\")\n",
    "print(\"\\nSmoothing penalizes overconfidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Simple function\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Focal Loss: down-weights easy examples, focuses on hard ones.\n",
    "    FL = -alpha * (1 - p)^gamma * log(p)\n",
    "    \"\"\"\n",
    "    ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)  # probability of correct class\n",
    "    focal_weight = alpha * (1 - pt) ** gamma\n",
    "    return (focal_weight * ce_loss).mean()\n",
    "\n",
    "logits = torch.randn(10, 3)\n",
    "targets = torch.randint(0, 3, (10,))\n",
    "\n",
    "print(f\"Cross Entropy: {F.cross_entropy(logits, targets):.4f}\")\n",
    "print(f\"Focal Loss: {focal_loss(logits, targets):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: nn.Module class (for learnable parameters)\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss for similarity learning.\n",
    "    For similar pairs: loss = distance^2\n",
    "    For dissimilar pairs: loss = max(0, margin - distance)^2\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, embeddings1, embeddings2, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings1, embeddings2: (N, D) embeddings\n",
    "            labels: (N,) 1 if similar, 0 if dissimilar\n",
    "        \"\"\"\n",
    "        distances = F.pairwise_distance(embeddings1, embeddings2)\n",
    "        \n",
    "        similar_loss = labels * distances.pow(2)\n",
    "        dissimilar_loss = (1 - labels) * F.relu(self.margin - distances).pow(2)\n",
    "        \n",
    "        return (similar_loss + dissimilar_loss).mean()\n",
    "\n",
    "# Test\n",
    "criterion = ContrastiveLoss(margin=2.0)\n",
    "emb1 = torch.randn(5, 64)\n",
    "emb2 = torch.randn(5, 64)\n",
    "labels = torch.tensor([1, 1, 0, 0, 1], dtype=torch.float32)\n",
    "\n",
    "loss = criterion(emb1, emb2, labels)\n",
    "print(f\"Contrastive loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining multiple losses\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, class_logits, regression_pred, class_targets, regression_targets):\n",
    "        ce_loss = self.ce(class_logits, class_targets)\n",
    "        mse_loss = self.mse(regression_pred, regression_targets)\n",
    "        return self.alpha * ce_loss + (1 - self.alpha) * mse_loss\n",
    "\n",
    "criterion = CombinedLoss(alpha=0.7)\n",
    "print(\"Multi-task loss defined with weighted combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Optimizers\n",
    "\n",
    "Optimizers update model parameters based on computed gradients. Different optimizers have different update rules and properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SGD (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SGD: w = w - lr * grad\n",
    "model = nn.Linear(10, 5)\n",
    "sgd = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "x = torch.randn(3, 10)\n",
    "y = model(x).sum()\n",
    "y.backward()\n",
    "\n",
    "# Manual update matches SGD\n",
    "w_before = model.weight.data.clone()\n",
    "sgd.step()\n",
    "w_after = model.weight.data\n",
    "\n",
    "expected = w_before - 0.01 * model.weight.grad\n",
    "print(f\"SGD update correct: {torch.allclose(w_after, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with momentum\n",
    "# v = momentum * v + grad\n",
    "# w = w - lr * v\n",
    "sgd_momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Momentum accumulates gradient direction over time,\n",
    "# helping escape local minima and smooth noisy gradients\n",
    "print(\"Momentum accelerates consistent gradient directions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with weight decay (L2 regularization)\n",
    "# Penalizes large weights: loss = original_loss + (weight_decay/2) * ||w||^2\n",
    "# Gradient becomes: grad + weight_decay * w\n",
    "sgd_wd = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "\n",
    "print(\"Weight decay prevents overfitting by keeping weights small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adam (Adaptive Moment Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam maintains per-parameter adaptive learning rates\n",
    "# using first moment (mean) and second moment (variance) of gradients\n",
    "\n",
    "# m = beta1 * m + (1 - beta1) * grad          # First moment\n",
    "# v = beta2 * v + (1 - beta2) * grad^2        # Second moment  \n",
    "# m_hat = m / (1 - beta1^t)                   # Bias correction\n",
    "# v_hat = v / (1 - beta2^t)                   # Bias correction\n",
    "# w = w - lr * m_hat / (sqrt(v_hat) + eps)    # Update\n",
    "\n",
    "model = nn.Linear(10, 5)\n",
    "adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "print(\"Adam adapts learning rate for each parameter based on gradient history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam state inspection\n",
    "# After some training steps, Adam stores momentum for each parameter\n",
    "for _ in range(10):\n",
    "    adam.zero_grad()\n",
    "    x = torch.randn(3, 10)\n",
    "    model(x).sum().backward()\n",
    "    adam.step()\n",
    "\n",
    "# Inspect state\n",
    "for name, param in model.named_parameters():\n",
    "    if param in adam.state:\n",
    "        state = adam.state[param]\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  step: {state['step']}\")\n",
    "        print(f\"  exp_avg (m) shape: {state['exp_avg'].shape}\")\n",
    "        print(f\"  exp_avg_sq (v) shape: {state['exp_avg_sq'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 AdamW (Adam with Decoupled Weight Decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW fixes a subtle bug in Adam's weight decay implementation\n",
    "#\n",
    "# Adam with weight_decay: adds weight_decay * w to gradient BEFORE adaptive scaling\n",
    "# AdamW: applies weight decay AFTER the Adam update (decoupled)\n",
    "#\n",
    "# Adam:  w = w - lr * (m_hat / sqrt(v_hat) + weight_decay * w)\n",
    "# AdamW: w = w - lr * m_hat / sqrt(v_hat) - lr * weight_decay * w\n",
    "\n",
    "model = nn.Linear(10, 5)\n",
    "adamw = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "print(\"AdamW is generally preferred over Adam with weight_decay\")\n",
    "print(\"It's the default in many modern architectures (transformers, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comparing Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_optimizer(optimizer_class, optimizer_kwargs, epochs=50):\n",
    "    \"\"\"Train a model with given optimizer and return loss history.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(10, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 3)\n",
    "    )\n",
    "    optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(dataloader))\n",
    "    return losses\n",
    "\n",
    "# Compare optimizers\n",
    "optimizers = {\n",
    "    'SGD': (optim.SGD, {'lr': 0.1}),\n",
    "    'SGD+Momentum': (optim.SGD, {'lr': 0.1, 'momentum': 0.9}),\n",
    "    'Adam': (optim.Adam, {'lr': 0.01}),\n",
    "    'AdamW': (optim.AdamW, {'lr': 0.01, 'weight_decay': 0.01}),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, (opt_class, kwargs) in optimizers.items():\n",
    "    results[name] = train_with_optimizer(opt_class, kwargs)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "for name, losses in results.items():\n",
    "    plt.plot(losses, label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Optimizer Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Parameter Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different learning rates for different layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),  # Early layer - lower LR\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)    # Later layer - higher LR\n",
    ")\n",
    "\n",
    "# Define parameter groups\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model[0].parameters(), 'lr': 1e-4},   # Early layer\n",
    "    {'params': model[2].parameters(), 'lr': 1e-3},   # Later layer\n",
    "], lr=1e-3)  # Default LR (not used here since all groups specify lr)\n",
    "\n",
    "print(\"Parameter groups:\")\n",
    "for i, group in enumerate(optimizer.param_groups):\n",
    "    print(f\"  Group {i}: lr = {group['lr']}, params = {len(group['params'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying learning rates during training\n",
    "for group in optimizer.param_groups:\n",
    "    group['lr'] *= 0.1  # Reduce all LRs by 10x\n",
    "\n",
    "print(\"After LR reduction:\")\n",
    "for i, group in enumerate(optimizer.param_groups):\n",
    "    print(f\"  Group {i}: lr = {group['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Learning Rate Schedulers\n",
    "\n",
    "Schedulers adjust the learning rate during training, often leading to better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StepLR: Decay LR by gamma every step_size epochs\n",
    "model = nn.Linear(10, 5)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "lrs = []\n",
    "for epoch in range(50):\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    # train_epoch(...)\n",
    "    scheduler.step()  # Call after each epoch\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lrs)\n",
    "plt.title('StepLR (step=10, gamma=0.5)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CosineAnnealingLR: Smooth cosine decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "lrs = []\n",
    "for epoch in range(50):\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lrs)\n",
    "plt.title('CosineAnnealingLR (T_max=50)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneCycleLR: Popular for fast training (1cycle policy)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.1,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=len(dataloader)  # Called every batch, not epoch!\n",
    ")\n",
    "\n",
    "lrs = []\n",
    "for epoch in range(50):\n",
    "    for batch in dataloader:\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        scheduler.step()  # Called every batch!\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(lrs)\n",
    "plt.title('OneCycleLR (per-batch updates)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReduceLROnPlateau: Reduce when metric stops improving\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',      # Reduce when metric stops decreasing\n",
    "    factor=0.5,      # Multiply LR by this factor\n",
    "    patience=5,      # Wait this many epochs before reducing\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Usage:\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss = train(...)\n",
    "#     val_loss = validate(...)\n",
    "#     scheduler.step(val_loss)  # Pass the metric to monitor!\n",
    "\n",
    "print(\"ReduceLROnPlateau requires passing the metric to step()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup + Decay combination\n",
    "def get_warmup_scheduler(optimizer, warmup_epochs, total_epochs):\n",
    "    \"\"\"Linear warmup followed by cosine decay.\"\"\"\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return epoch / warmup_epochs\n",
    "        else:\n",
    "            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "            return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = get_warmup_scheduler(optimizer, warmup_epochs=5, total_epochs=50)\n",
    "\n",
    "lrs = []\n",
    "for epoch in range(50):\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(lrs)\n",
    "plt.axvline(x=5, color='r', linestyle='--', label='End of warmup')\n",
    "plt.title('Warmup + Cosine Decay')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Gradient Clipping\n",
    "\n",
    "Gradient clipping prevents exploding gradients by limiting gradient magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient explosion example\n",
    "model = nn.Sequential(*[nn.Linear(10, 10) for _ in range(50)])  # Deep network\n",
    "\n",
    "# Initialize with values that cause explosion\n",
    "for layer in model:\n",
    "    if hasattr(layer, 'weight'):\n",
    "        nn.init.uniform_(layer.weight, 1.0, 1.1)\n",
    "\n",
    "x = torch.randn(1, 10)\n",
    "y = model(x).sum()\n",
    "y.backward()\n",
    "\n",
    "# Check gradient norms\n",
    "for i, layer in enumerate(model[:5]):\n",
    "    if hasattr(layer, 'weight'):\n",
    "        print(f\"Layer {i} gradient norm: {layer.weight.grad.norm():.2e}\")\n",
    "\n",
    "print(\"\\nGradients exploded in early layers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_grad_norm_: Clip by global norm\n",
    "# If ||grad|| > max_norm, scale all gradients so ||grad|| = max_norm\n",
    "\n",
    "model = nn.Linear(10, 5)\n",
    "x = torch.randn(3, 10) * 100  # Large input -> large gradients\n",
    "model(x).sum().backward()\n",
    "\n",
    "print(f\"Before clipping: {model.weight.grad.norm():.4f}\")\n",
    "\n",
    "# Clip\n",
    "max_norm = 1.0\n",
    "total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "print(f\"Total norm was: {total_norm:.4f}\")\n",
    "print(f\"After clipping: {model.weight.grad.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_grad_value_: Clip by value\n",
    "# Clamps each gradient element to [-clip_value, clip_value]\n",
    "\n",
    "model = nn.Linear(10, 5)\n",
    "x = torch.randn(3, 10) * 100\n",
    "model(x).sum().backward()\n",
    "\n",
    "print(f\"Before clipping:\")\n",
    "print(f\"  Max: {model.weight.grad.max():.4f}\")\n",
    "print(f\"  Min: {model.weight.grad.min():.4f}\")\n",
    "\n",
    "torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)\n",
    "\n",
    "print(f\"\\nAfter clipping:\")\n",
    "print(f\"  Max: {model.weight.grad.max():.4f}\")\n",
    "print(f\"  Min: {model.weight.grad.min():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with gradient clipping\n",
    "def train_with_clipping(model, dataloader, criterion, optimizer, max_grad_norm=1.0):\n",
    "    model.train()\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients BEFORE optimizer step\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Gradient clipping is especially important for RNNs and transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Numerical Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Overflow in softmax\n",
    "logits = torch.tensor([1000.0, 1001.0, 1002.0])\n",
    "try:\n",
    "    naive_softmax = torch.exp(logits) / torch.exp(logits).sum()\n",
    "    print(f\"Naive softmax: {naive_softmax}\")\n",
    "except:\n",
    "    print(\"Overflow!\")\n",
    "\n",
    "# Solution: Subtract max before exp\n",
    "stable_softmax = F.softmax(logits, dim=0)\n",
    "print(f\"Stable softmax: {stable_softmax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Log of small probabilities\n",
    "probs = torch.tensor([0.99, 0.009, 0.001, 1e-10])\n",
    "log_probs = torch.log(probs)\n",
    "print(f\"Log probs: {log_probs}\")\n",
    "print(\"Note: log(1e-10) = -23, which can cause issues\")\n",
    "\n",
    "# Solution: Use log_softmax instead of softmax + log\n",
    "logits = torch.randn(4)\n",
    "log_probs_stable = F.log_softmax(logits, dim=0)\n",
    "print(f\"\\nStable log_softmax: {log_probs_stable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting NaN and Inf\n",
    "def check_for_nan(model, loss):\n",
    "    \"\"\"Check for NaN in loss or gradients.\"\"\"\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "        print(\"Warning: Loss is NaN or Inf!\")\n",
    "        return True\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(f\"Warning: NaN gradient in {name}\")\n",
    "                return True\n",
    "            if torch.isinf(param.grad).any():\n",
    "                print(f\"Warning: Inf gradient in {name}\")\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Usage in training loop:\n",
    "# if check_for_nan(model, loss):\n",
    "#     print(\"Skipping batch due to numerical issues\")\n",
    "#     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use anomaly detection during debugging\n",
    "# (SLOW - only use for debugging)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "x = torch.tensor([0.0], requires_grad=True)\n",
    "try:\n",
    "    y = torch.log(x)  # log(0) = -inf\n",
    "    z = y * 2\n",
    "    z.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"Anomaly detected: {str(e)[:100]}...\")\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement SGD from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySGD:\n",
    "    \"\"\"\n",
    "    Implement SGD with momentum from scratch.\n",
    "    \n",
    "    Update rule:\n",
    "    if momentum > 0:\n",
    "        v = momentum * v + grad\n",
    "        param = param - lr * v\n",
    "    else:\n",
    "        param = param - lr * grad\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters, lr, momentum=0):\n",
    "        self.parameters = list(parameters)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # YOUR CODE: Initialize velocity buffers if momentum > 0\n",
    "        self.velocities = None\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        # YOUR CODE: Set all gradients to zero\n",
    "        pass\n",
    "    \n",
    "    def step(self):\n",
    "        # YOUR CODE: Update parameters using SGD with momentum\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# model = nn.Linear(10, 5)\n",
    "# my_sgd = MySGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# x = torch.randn(3, 10)\n",
    "# model(x).sum().backward()\n",
    "# my_sgd.step()\n",
    "# print(\"Custom SGD step completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR:\n",
    "    \"\"\"\n",
    "    Implement triangular cyclic learning rate.\n",
    "    \n",
    "    LR oscillates between base_lr and max_lr over step_size*2 iterations.\n",
    "    \n",
    "    Args:\n",
    "        optimizer: The optimizer\n",
    "        base_lr: Minimum learning rate\n",
    "        max_lr: Maximum learning rate\n",
    "        step_size: Number of iterations to go from base to max\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, base_lr, max_lr, step_size):\n",
    "        self.optimizer = optimizer\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.iteration = 0\n",
    "    \n",
    "    def step(self):\n",
    "        # YOUR CODE: Calculate and set new learning rate\n",
    "        # Cycle position goes from 0 to 1 and back to 0\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# model = nn.Linear(10, 5)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size=100)\n",
    "# lrs = []\n",
    "# for i in range(400):\n",
    "#     lrs.append(optimizer.param_groups[0]['lr'])\n",
    "#     scheduler.step()\n",
    "# plt.plot(lrs)\n",
    "# plt.title('Cyclic LR')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Training with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to prevent overfitting.\n",
    "    \n",
    "    Stop training if validation loss doesn't improve for `patience` epochs.\n",
    "    Save the best model weights.\n",
    "    \n",
    "    Args:\n",
    "        patience: Number of epochs to wait before stopping\n",
    "        min_delta: Minimum change to consider as improvement\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.best_weights = None\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Call after each validation.\n",
    "        Returns True if training should stop.\n",
    "        \"\"\"\n",
    "        # YOUR CODE:\n",
    "        # 1. Check if this is the first call or if val_loss improved\n",
    "        # 2. If improved, reset counter and save best weights\n",
    "        # 3. If not improved, increment counter\n",
    "        # 4. Set should_stop = True if counter >= patience\n",
    "        pass\n",
    "    \n",
    "    def load_best_weights(self, model):\n",
    "        \"\"\"Restore the best weights.\"\"\"\n",
    "        # YOUR CODE\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "# early_stopping = EarlyStopping(patience=5)\n",
    "# for epoch in range(100):\n",
    "#     train_loss = train_epoch(...)\n",
    "#     val_loss = validate_epoch(...)\n",
    "#     if early_stopping(val_loss, model):\n",
    "#         print(f\"Early stopping at epoch {epoch}\")\n",
    "#         break\n",
    "# early_stopping.load_best_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "class MySGDSolution:\n",
    "    def __init__(self, parameters, lr, momentum=0):\n",
    "        self.parameters = list(parameters)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        if momentum > 0:\n",
    "            self.velocities = [torch.zeros_like(p) for p in self.parameters]\n",
    "        else:\n",
    "            self.velocities = None\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for i, param in enumerate(self.parameters):\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                if self.momentum > 0:\n",
    "                    self.velocities[i] = self.momentum * self.velocities[i] + param.grad\n",
    "                    param -= self.lr * self.velocities[i]\n",
    "                else:\n",
    "                    param -= self.lr * param.grad\n",
    "\n",
    "print(\"Exercise 1 Solution:\")\n",
    "model = nn.Linear(10, 5)\n",
    "my_sgd = MySGDSolution(model.parameters(), lr=0.01, momentum=0.9)\n",
    "x = torch.randn(3, 10)\n",
    "model(x).sum().backward()\n",
    "my_sgd.step()\n",
    "print(\"Custom SGD step completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "class CyclicLRSolution:\n",
    "    def __init__(self, optimizer, base_lr, max_lr, step_size):\n",
    "        self.optimizer = optimizer\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.iteration = 0\n",
    "    \n",
    "    def step(self):\n",
    "        cycle = self.iteration // (2 * self.step_size)\n",
    "        x = abs(self.iteration / self.step_size - 2 * cycle - 1)\n",
    "        lr = self.base_lr + (self.max_lr - self.base_lr) * max(0, 1 - x)\n",
    "        \n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = lr\n",
    "        \n",
    "        self.iteration += 1\n",
    "\n",
    "print(\"\\nExercise 2 Solution:\")\n",
    "model = nn.Linear(10, 5)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = CyclicLRSolution(optimizer, base_lr=0.001, max_lr=0.1, step_size=100)\n",
    "lrs = []\n",
    "for i in range(400):\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(lrs)\n",
    "plt.title('Cyclic LR (Triangular)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution\n",
    "import copy\n",
    "\n",
    "class EarlyStoppingSolution:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.best_weights = None\n",
    "        self.should_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_weights = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        \n",
    "        return self.should_stop\n",
    "    \n",
    "    def load_best_weights(self, model):\n",
    "        if self.best_weights is not None:\n",
    "            model.load_state_dict(self.best_weights)\n",
    "\n",
    "print(\"\\nExercise 3 Solution:\")\n",
    "# Simulate training with early stopping\n",
    "early_stopping = EarlyStoppingSolution(patience=3)\n",
    "model = nn.Linear(10, 5)\n",
    "\n",
    "# Simulate validation losses that plateau\n",
    "fake_val_losses = [1.0, 0.8, 0.6, 0.5, 0.5, 0.51, 0.52, 0.53]\n",
    "\n",
    "for epoch, val_loss in enumerate(fake_val_losses):\n",
    "    stop = early_stopping(val_loss, model)\n",
    "    print(f\"Epoch {epoch}: val_loss={val_loss:.2f}, counter={early_stopping.counter}, stop={stop}\")\n",
    "    if stop:\n",
    "        print(f\"\\nStopped at epoch {epoch}. Best loss: {early_stopping.best_loss:.2f}\")\n",
    "        break\n",
    "\n",
    "early_stopping.load_best_weights(model)\n",
    "print(\"Best weights restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Key takeaways from this notebook:\n",
    "\n",
    "1. **Training Loop Steps**: zero_grad → forward → loss → backward → step\n",
    "2. **Loss Functions**: Choose based on task; use numerically stable versions\n",
    "3. **Optimizers**: SGD with momentum for simplicity, Adam/AdamW for adaptive learning\n",
    "4. **Learning Rate Schedulers**: Warmup + decay, cosine annealing, or reduce-on-plateau\n",
    "5. **Gradient Clipping**: Essential for RNNs and transformers; use `clip_grad_norm_`\n",
    "6. **Numerical Stability**: Use `log_softmax`, avoid log(0), detect anomalies\n",
    "\n",
    "---\n",
    "*Next: Module 2.2 - Data Pipeline Mastery*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
