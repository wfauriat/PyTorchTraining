{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Convolutional Neural Networks from Scratch\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand convolution operations** - How filters slide over images, padding, stride, dilation\n",
    "2. **Master pooling layers** - Max pooling, average pooling, global pooling, adaptive pooling\n",
    "3. **Build CNN architectures** - From simple ConvNets to ResNet with skip connections\n",
    "4. **Implement key components** - Batch normalization, dropout, residual blocks\n",
    "5. **Train CNNs effectively** - Data augmentation, learning rate schedules, regularization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional\n",
    "import math\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Understanding Convolutions\n",
    "\n",
    "### 1.1 What is a Convolution?\n",
    "\n",
    "A convolution slides a small filter (kernel) across an image, computing dot products at each position. This creates a feature map that detects specific patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual 2D convolution to understand the operation\n",
    "\n",
    "def manual_conv2d(image: torch.Tensor, kernel: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Manual implementation of 2D convolution (no padding, stride=1).\n",
    "    \n",
    "    Args:\n",
    "        image: (H, W) tensor\n",
    "        kernel: (kH, kW) tensor\n",
    "    \n",
    "    Returns:\n",
    "        (H-kH+1, W-kW+1) tensor\n",
    "    \"\"\"\n",
    "    h, w = image.shape\n",
    "    kh, kw = kernel.shape\n",
    "    \n",
    "    out_h = h - kh + 1\n",
    "    out_w = w - kw + 1\n",
    "    \n",
    "    output = torch.zeros(out_h, out_w)\n",
    "    \n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            # Extract patch and compute dot product with kernel\n",
    "            patch = image[i:i+kh, j:j+kw]\n",
    "            output[i, j] = (patch * kernel).sum()\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Create a simple image and kernel\n",
    "image = torch.tensor([\n",
    "    [1, 2, 3, 0, 1],\n",
    "    [0, 1, 2, 3, 1],\n",
    "    [1, 2, 1, 0, 0],\n",
    "    [2, 1, 0, 1, 1],\n",
    "    [1, 0, 1, 2, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Edge detection kernel (Sobel-like)\n",
    "kernel = torch.tensor([\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "output = manual_conv2d(image, kernel)\n",
    "print(f\"Input shape: {image.shape}\")\n",
    "print(f\"Kernel shape: {kernel.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nOutput:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with PyTorch's conv2d\n",
    "# F.conv2d expects (N, C, H, W) input and (out_channels, in_channels, kH, kW) kernel\n",
    "\n",
    "image_4d = image.unsqueeze(0).unsqueeze(0)  # (1, 1, 5, 5)\n",
    "kernel_4d = kernel.unsqueeze(0).unsqueeze(0)  # (1, 1, 3, 3)\n",
    "\n",
    "pytorch_output = F.conv2d(image_4d, kernel_4d)\n",
    "print(f\"PyTorch output:\\n{pytorch_output.squeeze()}\")\n",
    "print(f\"\\nMatches manual: {torch.allclose(output, pytorch_output.squeeze())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Padding, Stride, and Dilation\n",
    "\n",
    "These parameters control the convolution's behavior:\n",
    "\n",
    "- **Padding**: Add zeros around the input to preserve spatial dimensions\n",
    "- **Stride**: How many pixels the kernel moves at each step\n",
    "- **Dilation**: Spacing between kernel elements (for larger receptive fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output_size(input_size: int, kernel_size: int, padding: int = 0, \n",
    "                     stride: int = 1, dilation: int = 1) -> int:\n",
    "    \"\"\"\n",
    "    Calculate output size of a convolution.\n",
    "    \n",
    "    Formula: out = floor((in + 2*pad - dil*(k-1) - 1) / stride + 1)\n",
    "    \"\"\"\n",
    "    effective_kernel = dilation * (kernel_size - 1) + 1\n",
    "    return (input_size + 2 * padding - effective_kernel) // stride + 1\n",
    "\n",
    "\n",
    "# Examples\n",
    "print(\"Output size calculations:\")\n",
    "print(f\"Input=32, kernel=3, padding=0, stride=1: {calc_output_size(32, 3, 0, 1)}\")\n",
    "print(f\"Input=32, kernel=3, padding=1, stride=1: {calc_output_size(32, 3, 1, 1)} (same padding)\")\n",
    "print(f\"Input=32, kernel=3, padding=1, stride=2: {calc_output_size(32, 3, 1, 2)} (downsampling)\")\n",
    "print(f\"Input=32, kernel=3, padding=2, stride=1, dilation=2: {calc_output_size(32, 3, 2, 1, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of different parameters\n",
    "\n",
    "def visualize_conv_params():\n",
    "    x = torch.randn(1, 1, 8, 8)\n",
    "    \n",
    "    configs = [\n",
    "        {'kernel_size': 3, 'padding': 0, 'stride': 1, 'dilation': 1},\n",
    "        {'kernel_size': 3, 'padding': 1, 'stride': 1, 'dilation': 1},  # Same padding\n",
    "        {'kernel_size': 3, 'padding': 1, 'stride': 2, 'dilation': 1},  # Downsampling\n",
    "        {'kernel_size': 3, 'padding': 2, 'stride': 1, 'dilation': 2},  # Dilated\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(configs) + 1, figsize=(15, 3))\n",
    "    \n",
    "    axes[0].imshow(x.squeeze(), cmap='viridis')\n",
    "    axes[0].set_title(f'Input\\n{tuple(x.shape[2:])}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    for i, cfg in enumerate(configs):\n",
    "        conv = nn.Conv2d(1, 1, **cfg)\n",
    "        out = conv(x)\n",
    "        axes[i+1].imshow(out.detach().squeeze(), cmap='viridis')\n",
    "        title = f\"k={cfg['kernel_size']}, p={cfg['padding']}\\ns={cfg['stride']}, d={cfg['dilation']}\\n{tuple(out.shape[2:])}\"\n",
    "        axes[i+1].set_title(title)\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_conv_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Multiple Channels\n",
    "\n",
    "Real convolutions work with multiple input and output channels:\n",
    "- **Input channels**: e.g., RGB = 3 channels\n",
    "- **Output channels**: Each output channel is a different learned filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding multi-channel convolutions\n",
    "\n",
    "# Conv2d with 3 input channels (RGB) and 16 output channels\n",
    "conv = nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=16,\n",
    "    kernel_size=3,\n",
    "    padding=1\n",
    ")\n",
    "\n",
    "print(f\"Weight shape: {conv.weight.shape}\")  # (out_ch, in_ch, kH, kW)\n",
    "print(f\"Bias shape: {conv.bias.shape}\")  # (out_ch,)\n",
    "\n",
    "# Total parameters\n",
    "num_params = conv.weight.numel() + conv.bias.numel()\n",
    "print(f\"Total parameters: {num_params}\")\n",
    "print(f\"  = {conv.out_channels} * ({conv.in_channels} * {conv.kernel_size[0]} * {conv.kernel_size[1]} + 1)\")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(4, 3, 32, 32)  # (batch, channels, height, width)\n",
    "output = conv(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what happens in multi-channel convolution\n",
    "\n",
    "def visualize_multichannel_conv():\n",
    "    \"\"\"Show how each output channel is computed\"\"\"\n",
    "    \n",
    "    # Simple 2 in, 3 out convolution\n",
    "    conv = nn.Conv2d(2, 3, kernel_size=3, padding=1, bias=False)\n",
    "    \n",
    "    # Input: 2 channels\n",
    "    x = torch.randn(1, 2, 8, 8)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "    \n",
    "    # For each output channel\n",
    "    for out_ch in range(3):\n",
    "        # Show the kernel for each input channel\n",
    "        for in_ch in range(2):\n",
    "            kernel = conv.weight[out_ch, in_ch].detach()\n",
    "            axes[out_ch, in_ch].imshow(kernel, cmap='RdBu', vmin=-1, vmax=1)\n",
    "            axes[out_ch, in_ch].set_title(f'Kernel[{out_ch},{in_ch}]')\n",
    "            axes[out_ch, in_ch].axis('off')\n",
    "        \n",
    "        # Show the sum (what conv actually computes)\n",
    "        axes[out_ch, 2].text(0.5, 0.5, 'â†’', fontsize=30, ha='center', va='center')\n",
    "        axes[out_ch, 2].axis('off')\n",
    "        \n",
    "        # Output channel\n",
    "        output = conv(x)\n",
    "        axes[out_ch, 3].imshow(output[0, out_ch].detach(), cmap='viridis')\n",
    "        axes[out_ch, 3].set_title(f'Output[{out_ch}]')\n",
    "        axes[out_ch, 3].axis('off')\n",
    "    \n",
    "    plt.suptitle('Multi-channel Convolution: Each output = sum of (input_ch * kernel)', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_multichannel_conv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Receptive Field\n",
    "\n",
    "The **receptive field** is the region in the input that influences a particular output pixel. Deeper layers have larger receptive fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_receptive_field(layers: List[dict]) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the receptive field of a stack of conv layers.\n",
    "    \n",
    "    Each layer dict should have: kernel_size, stride, dilation\n",
    "    \"\"\"\n",
    "    rf = 1  # Start with 1 pixel\n",
    "    stride_product = 1\n",
    "    \n",
    "    for layer in layers:\n",
    "        k = layer.get('kernel_size', 1)\n",
    "        s = layer.get('stride', 1)\n",
    "        d = layer.get('dilation', 1)\n",
    "        \n",
    "        # Effective kernel size with dilation\n",
    "        effective_k = d * (k - 1) + 1\n",
    "        \n",
    "        # Update receptive field\n",
    "        rf = rf + (effective_k - 1) * stride_product\n",
    "        stride_product *= s\n",
    "    \n",
    "    return rf\n",
    "\n",
    "\n",
    "# Example: VGG-like stack\n",
    "vgg_layers = [\n",
    "    {'kernel_size': 3, 'stride': 1},  # 3x3 conv\n",
    "    {'kernel_size': 3, 'stride': 1},  # 3x3 conv\n",
    "    {'kernel_size': 2, 'stride': 2},  # 2x2 pool\n",
    "    {'kernel_size': 3, 'stride': 1},  # 3x3 conv\n",
    "    {'kernel_size': 3, 'stride': 1},  # 3x3 conv\n",
    "    {'kernel_size': 2, 'stride': 2},  # 2x2 pool\n",
    "]\n",
    "\n",
    "print(f\"VGG-like receptive field: {calculate_receptive_field(vgg_layers)}\")\n",
    "\n",
    "# With dilated convolutions\n",
    "dilated_layers = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'dilation': 1},\n",
    "    {'kernel_size': 3, 'stride': 1, 'dilation': 2},\n",
    "    {'kernel_size': 3, 'stride': 1, 'dilation': 4},\n",
    "]\n",
    "\n",
    "print(f\"Dilated conv receptive field: {calculate_receptive_field(dilated_layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pooling Layers\n",
    "\n",
    "Pooling reduces spatial dimensions while maintaining important features.\n",
    "\n",
    "### 2.1 Types of Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample feature map\n",
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12],\n",
    "    [13, 14, 15, 16]\n",
    "], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(f\"Input:\\n{x.squeeze()}\\n\")\n",
    "\n",
    "# Max Pooling - takes maximum value in each window\n",
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "print(f\"Max Pooling (2x2, stride 2):\\n{max_pool(x).squeeze()}\\n\")\n",
    "\n",
    "# Average Pooling - takes mean value in each window\n",
    "avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "print(f\"Average Pooling (2x2, stride 2):\\n{avg_pool(x).squeeze()}\\n\")\n",
    "\n",
    "# Global Average Pooling - reduces to single value per channel\n",
    "global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "print(f\"Global Average Pooling:\\n{global_avg_pool(x).squeeze()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive pooling - specify output size, not kernel size\n",
    "\n",
    "x = torch.randn(1, 3, 37, 41)  # Odd dimensions\n",
    "\n",
    "# AdaptiveAvgPool2d automatically calculates kernel size and stride\n",
    "adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))  # Always outputs 7x7\n",
    "output = adaptive_pool(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\nAdaptive pooling is essential for handling variable input sizes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Max Pooling with Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPool2d can return indices for unpooling (used in autoencoders)\n",
    "\n",
    "max_pool_idx = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "\n",
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12],\n",
    "    [13, 14, 15, 16]\n",
    "], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Pool and get indices\n",
    "pooled, indices = max_pool_idx(x)\n",
    "print(f\"Input:\\n{x.squeeze()}\\n\")\n",
    "print(f\"Pooled:\\n{pooled.squeeze()}\\n\")\n",
    "print(f\"Indices (flattened positions):\\n{indices.squeeze()}\\n\")\n",
    "\n",
    "# Unpool using indices\n",
    "unpooled = max_unpool(pooled, indices)\n",
    "print(f\"Unpooled:\\n{unpooled.squeeze()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Building CNN Architectures\n",
    "\n",
    "### 3.1 Simple CNN (LeNet-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN architecture similar to LeNet.\n",
    "    Architecture: Conv -> Pool -> Conv -> Pool -> FC -> FC -> Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # After 3 pools of 2x2 on 32x32 input: 32 -> 16 -> 8 -> 4\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
    "        \n",
    "        # Conv block 3\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        # Fully connected\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test\n",
    "model = SimpleCNN(num_classes=10)\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "output = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 VGG-style CNN with Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Conv -> BatchNorm -> ReLU block\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class VGGStyleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG-style CNN with batch normalization.\n",
    "    Uses repeated 3x3 convolutions with pooling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Block 1: 2 convs, pool\n",
    "        self.block1 = nn.Sequential(\n",
    "            ConvBlock(3, 64),\n",
    "            ConvBlock(64, 64),\n",
    "            nn.MaxPool2d(2, 2)  # 32 -> 16\n",
    "        )\n",
    "        \n",
    "        # Block 2: 2 convs, pool\n",
    "        self.block2 = nn.Sequential(\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 128),\n",
    "            nn.MaxPool2d(2, 2)  # 16 -> 8\n",
    "        )\n",
    "        \n",
    "        # Block 3: 3 convs, pool\n",
    "        self.block3 = nn.Sequential(\n",
    "            ConvBlock(128, 256),\n",
    "            ConvBlock(256, 256),\n",
    "            ConvBlock(256, 256),\n",
    "            nn.MaxPool2d(2, 2)  # 8 -> 4\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # Global average pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = VGGStyleCNN(num_classes=10)\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "output = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ResNet: Residual Connections\n",
    "\n",
    "The key innovation of ResNet is the **skip connection** (residual connection), which helps train very deep networks by allowing gradients to flow directly through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic ResNet block with skip connection.\n",
    "    \n",
    "    Structure:\n",
    "        x -> Conv -> BN -> ReLU -> Conv -> BN -> (+x) -> ReLU\n",
    "    \"\"\"\n",
    "    \n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Main path\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection (identity or projection)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # Need to match dimensions\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Add skip connection\n",
    "        out += self.shortcut(identity)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Demonstrate the skip connection\n",
    "block = BasicBlock(64, 64)\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "out = block(x)\n",
    "print(f\"BasicBlock: {x.shape} -> {out.shape}\")\n",
    "\n",
    "# With downsampling\n",
    "block_down = BasicBlock(64, 128, stride=2)\n",
    "out_down = block_down(x)\n",
    "print(f\"BasicBlock (stride=2): {x.shape} -> {out_down.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottleneck block for deeper ResNets (50, 101, 152).\n",
    "    \n",
    "    Structure:\n",
    "        x -> 1x1 Conv -> 3x3 Conv -> 1x1 Conv -> (+x) -> ReLU\n",
    "    \n",
    "    The 1x1 convs reduce then expand channels (bottleneck).\n",
    "    \"\"\"\n",
    "    \n",
    "    expansion = 4  # Output channels = out_channels * 4\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x1 conv to reduce channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # 3x3 conv (main processing)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # 1x1 conv to expand channels\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        \n",
    "        # Skip connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        \n",
    "        out += self.shortcut(identity)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Demonstrate bottleneck\n",
    "bottleneck = Bottleneck(64, 64)\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "out = bottleneck(x)\n",
    "print(f\"Bottleneck: {x.shape} -> {out.shape}\")\n",
    "print(f\"Note: output channels = 64 * 4 = 256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet implementation.\n",
    "    \n",
    "    Configurations:\n",
    "        ResNet-18: [2, 2, 2, 2] with BasicBlock\n",
    "        ResNet-34: [3, 4, 6, 3] with BasicBlock\n",
    "        ResNet-50: [3, 4, 6, 3] with Bottleneck\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, block, num_blocks: List[int], num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        # Classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_layer(self, block, out_channels: int, num_blocks: int, stride: int):\n",
    "        \"\"\"Create a layer with multiple residual blocks\"\"\"\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        \n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using Kaiming initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial conv\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Residual layers\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.avgpool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "def ResNet34(num_classes=10):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "# Test ResNet-18\n",
    "model = ResNet18(num_classes=10)\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "output = model(x)\n",
    "print(f\"ResNet-18 Input shape: {x.shape}\")\n",
    "print(f\"ResNet-18 Output shape: {output.shape}\")\n",
    "print(f\"ResNet-18 Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Training a CNN on CIFAR-10\n",
    "\n",
    "### 4.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 transforms\n",
    "train_transform = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CIFAR10('../data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = CIFAR10('../data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Class names\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "\n",
    "def show_samples(dataset, num_samples=16):\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        img, label = dataset[i]\n",
    "        img = img * std + mean  # Denormalize\n",
    "        img = img.permute(1, 2, 0).clamp(0, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(classes[label])\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_samples(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ResNet-18 on CIFAR-10\n",
    "\n",
    "model = ResNet18(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# Training history\n",
    "history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "num_epochs = 20\n",
    "print(f\"Training ResNet-18 for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.3f} Acc: {train_acc:.1f}% | \"\n",
    "          f\"Test Loss: {test_loss:.3f} Acc: {test_acc:.1f}% | \"\n",
    "          f\"LR: {scheduler.get_last_lr()[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history['train_loss'], label='Train')\n",
    "ax1.plot(history['test_loss'], label='Test')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Curves')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(history['train_acc'], label='Train')\n",
    "ax2.plot(history['test_acc'], label='Test')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Accuracy Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize Learned Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first layer filters\n",
    "\n",
    "def visualize_filters(model, layer_name='conv1'):\n",
    "    \"\"\"Visualize the learned filters of a conv layer\"\"\"\n",
    "    # Get the layer\n",
    "    conv_layer = getattr(model, layer_name)\n",
    "    filters = conv_layer.weight.data.cpu()\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    filters = filters - filters.min()\n",
    "    filters = filters / filters.max()\n",
    "    \n",
    "    # Plot\n",
    "    n_filters = min(64, filters.shape[0])\n",
    "    n_cols = 8\n",
    "    n_rows = (n_filters + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 1.5))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < n_filters:\n",
    "            # For RGB filters, show as color image\n",
    "            if filters.shape[1] == 3:\n",
    "                img = filters[i].permute(1, 2, 0)\n",
    "                ax.imshow(img)\n",
    "            else:\n",
    "                ax.imshow(filters[i, 0], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Learned Filters in {layer_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_filters(model, 'conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps\n",
    "\n",
    "def visualize_feature_maps(model, image, layer_names=['conv1', 'layer1', 'layer2', 'layer3']):\n",
    "    \"\"\"Visualize feature maps at different layers\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Hook to capture activations\n",
    "    activations = {}\n",
    "    \n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    hooks = []\n",
    "    for name in layer_names:\n",
    "        layer = getattr(model, name)\n",
    "        hooks.append(layer.register_forward_hook(get_activation(name)))\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(image.unsqueeze(0).to(device))\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, len(layer_names) + 1, figsize=(15, 3))\n",
    "    \n",
    "    # Original image\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1)\n",
    "    img = image.cpu() * std + mean\n",
    "    axes[0].imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[0].set_title('Input')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Feature maps (show mean across channels)\n",
    "    for i, name in enumerate(layer_names):\n",
    "        act = activations[name].squeeze().cpu()\n",
    "        # Mean across channels\n",
    "        act_mean = act.mean(dim=0)\n",
    "        axes[i+1].imshow(act_mean, cmap='viridis')\n",
    "        axes[i+1].set_title(f'{name}\\n{tuple(act.shape)}')\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get a sample image\n",
    "sample_image, _ = test_dataset[0]\n",
    "visualize_feature_maps(model, sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Advanced CNN Techniques\n",
    "\n",
    "### 5.1 1x1 Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1x1 convolutions for channel mixing and dimensionality reduction\n",
    "\n",
    "class ChannelMixer(nn.Module):\n",
    "    \"\"\"Use 1x1 conv to mix/reduce channels\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# Example: Reduce 256 channels to 64\n",
    "mixer = ChannelMixer(256, 64)\n",
    "x = torch.randn(1, 256, 32, 32)\n",
    "out = mixer(x)\n",
    "print(f\"Channel reduction: {x.shape} -> {out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in mixer.parameters())}\")\n",
    "print(f\"  vs 3x3 conv: {256 * 64 * 3 * 3 + 64}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Depthwise Separable Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise separable convolution (MobileNet style).\n",
    "    \n",
    "    Splits convolution into:\n",
    "    1. Depthwise: One filter per input channel\n",
    "    2. Pointwise: 1x1 conv to mix channels\n",
    "    \n",
    "    Much fewer parameters than standard conv.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, stride: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Depthwise convolution: groups=in_channels means each channel has its own filter\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size,\n",
    "            stride=stride, padding=kernel_size//2,\n",
    "            groups=in_channels, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        # Pointwise convolution: 1x1 to mix channels\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.depthwise(x)))\n",
    "        x = F.relu(self.bn2(self.pointwise(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Compare parameters\n",
    "in_ch, out_ch, k = 64, 128, 3\n",
    "\n",
    "standard_conv = nn.Conv2d(in_ch, out_ch, k, padding=1)\n",
    "dw_sep_conv = DepthwiseSeparableConv(in_ch, out_ch, k)\n",
    "\n",
    "print(f\"Standard 3x3 conv parameters: {sum(p.numel() for p in standard_conv.parameters()):,}\")\n",
    "print(f\"Depthwise separable parameters: {sum(p.numel() for p in dw_sep_conv.parameters()):,}\")\n",
    "print(f\"Reduction factor: {sum(p.numel() for p in standard_conv.parameters()) / sum(p.numel() for p in dw_sep_conv.parameters()):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Squeeze-and-Excitation (SE) Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation block.\n",
    "    \n",
    "    Learns to recalibrate channel-wise feature responses by:\n",
    "    1. Squeeze: Global average pooling\n",
    "    2. Excitation: FC layers to learn channel importance\n",
    "    3. Scale: Multiply input by learned weights\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        \n",
    "        # Squeeze\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        \n",
    "        # Excitation\n",
    "        y = self.excitation(y).view(b, c, 1, 1)\n",
    "        \n",
    "        # Scale\n",
    "        return x * y\n",
    "\n",
    "\n",
    "# Test SE block\n",
    "se = SEBlock(64, reduction=16)\n",
    "x = torch.randn(4, 64, 32, 32)\n",
    "out = se(x)\n",
    "print(f\"SE Block: {x.shape} -> {out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in se.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE-ResNet Block\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    \"\"\"BasicBlock with SE attention\"\"\"\n",
    "    \n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # SE block\n",
    "        self.se = SEBlock(out_channels, reduction)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Apply SE attention\n",
    "        out = self.se(out)\n",
    "        \n",
    "        out += self.shortcut(identity)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "print(\"SE-ResNet block defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement a Custom Conv Block\n",
    "\n",
    "Create a conv block that uses:\n",
    "- Depthwise separable convolution\n",
    "- SE attention\n",
    "- Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement MBConv (MobileNet V2 style block)\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Bottleneck Conv block.\n",
    "    \n",
    "    Structure:\n",
    "    1. Expand: 1x1 conv to expand channels\n",
    "    2. Depthwise: 3x3 depthwise conv\n",
    "    3. SE: Squeeze-and-excitation\n",
    "    4. Project: 1x1 conv to project back\n",
    "    5. Residual: Add input if dimensions match\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Input channels\n",
    "        out_channels: Output channels\n",
    "        expansion: Channel expansion factor\n",
    "        stride: Stride for depthwise conv\n",
    "        se_ratio: SE reduction ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, expansion: int = 4, \n",
    "                 stride: int = 1, se_ratio: int = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Visualize Receptive Fields\n",
    "\n",
    "Write a function that visualizes which input pixels affect a given output pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Visualize receptive field using gradients\n",
    "\n",
    "def visualize_receptive_field(model, input_size=(32, 32), output_position=(16, 16)):\n",
    "    \"\"\"\n",
    "    Visualize the receptive field of a CNN by computing gradients.\n",
    "    \n",
    "    Args:\n",
    "        model: CNN model (should output feature maps, not class scores)\n",
    "        input_size: (H, W) of input\n",
    "        output_position: (y, x) position in output to analyze\n",
    "    \n",
    "    Returns:\n",
    "        receptive_field: (H, W) tensor showing which input pixels affect the output\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Create input with requires_grad=True, do forward pass,\n",
    "    # select one output pixel, backward, and look at input gradients\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement DenseNet Block\n",
    "\n",
    "In DenseNet, each layer receives feature maps from all preceding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement DenseNet block\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single layer in a dense block.\n",
    "    \n",
    "    Structure: BN -> ReLU -> 1x1 Conv -> BN -> ReLU -> 3x3 Conv\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Input channels\n",
    "        growth_rate: Number of new channels to add (k in paper)\n",
    "        bn_size: Bottleneck size multiplier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, growth_rate: int, bn_size: int = 4):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Dense block containing multiple dense layers.\n",
    "    Each layer receives all previous feature maps concatenated.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers: int, in_channels: int, growth_rate: int):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: MBConv\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    \"\"\"Mobile Inverted Bottleneck Conv block\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, expansion: int = 4,\n",
    "                 stride: int = 1, se_ratio: int = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_residual = (stride == 1 and in_channels == out_channels)\n",
    "        hidden_dim = in_channels * expansion\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Expansion phase (skip if expansion == 1)\n",
    "        if expansion != 1:\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_channels, hidden_dim, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6()\n",
    "            ])\n",
    "        \n",
    "        # Depthwise conv\n",
    "        layers.extend([\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride=stride, padding=1,\n",
    "                     groups=hidden_dim, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6()\n",
    "        ])\n",
    "        \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "        # SE block\n",
    "        self.se = SEBlock(hidden_dim, reduction=se_ratio)\n",
    "        \n",
    "        # Projection\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv(x)\n",
    "        out = self.se(out)\n",
    "        out = self.project(out)\n",
    "        \n",
    "        if self.use_residual:\n",
    "            out = out + identity\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Test\n",
    "mbconv = MBConv(32, 32, expansion=4, stride=1)\n",
    "x = torch.randn(4, 32, 16, 16)\n",
    "out = mbconv(x)\n",
    "print(f\"MBConv: {x.shape} -> {out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in mbconv.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Visualize Receptive Field\n",
    "\n",
    "def visualize_receptive_field(model, input_size=(32, 32), output_position=None):\n",
    "    \"\"\"\n",
    "    Visualize the receptive field of a CNN using gradients.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create input that requires gradients\n",
    "    x = torch.zeros(1, 3, *input_size, requires_grad=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    # Get output size\n",
    "    if len(output.shape) == 4:  # Feature map\n",
    "        _, c, h, w = output.shape\n",
    "        if output_position is None:\n",
    "            output_position = (h // 2, w // 2)\n",
    "        \n",
    "        # Select center pixel, sum across channels\n",
    "        target = output[0, :, output_position[0], output_position[1]].sum()\n",
    "    else:  # Classifier output\n",
    "        target = output[0, 0]\n",
    "    \n",
    "    # Backward\n",
    "    target.backward()\n",
    "    \n",
    "    # Get gradient magnitude\n",
    "    grad = x.grad[0].abs().sum(dim=0)  # Sum across RGB\n",
    "    \n",
    "    # Normalize\n",
    "    grad = grad / grad.max()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(grad.detach().numpy(), cmap='hot')\n",
    "    plt.colorbar()\n",
    "    plt.title(f'Receptive Field\\nOutput position: {output_position}')\n",
    "    plt.show()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "# Test with a simple conv stack\n",
    "simple_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(64, 128, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "rf = visualize_receptive_field(simple_model, input_size=(32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: DenseNet Block\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    \"\"\"Single layer in a dense block\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, growth_rate: int, bn_size: int = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Bottleneck: BN -> ReLU -> 1x1 Conv\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, bn_size * growth_rate, 1, bias=False)\n",
    "        \n",
    "        # Main conv: BN -> ReLU -> 3x3 Conv\n",
    "        self.bn2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
    "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, 3, padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"Dense block with feature concatenation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers: int, in_channels: int, growth_rate: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            layer = DenseLayer(in_channels + i * growth_rate, growth_rate)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        self.out_channels = in_channels + num_layers * growth_rate\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            # Concatenate all previous features\n",
    "            concat_features = torch.cat(features, dim=1)\n",
    "            # Apply dense layer\n",
    "            new_features = layer(concat_features)\n",
    "            features.append(new_features)\n",
    "        \n",
    "        # Return concatenation of all features\n",
    "        return torch.cat(features, dim=1)\n",
    "\n",
    "\n",
    "# Test\n",
    "dense_block = DenseBlock(num_layers=4, in_channels=64, growth_rate=32)\n",
    "x = torch.randn(4, 64, 16, 16)\n",
    "out = dense_block(x)\n",
    "print(f\"DenseBlock: {x.shape} -> {out.shape}\")\n",
    "print(f\"Output channels = 64 + 4*32 = {dense_block.out_channels}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in dense_block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Convolution Fundamentals**:\n",
    "   - Convolutions extract local features with learned filters\n",
    "   - Padding controls output size, stride controls downsampling\n",
    "   - Dilation increases receptive field without more parameters\n",
    "\n",
    "2. **Pooling**:\n",
    "   - Max pooling keeps strongest activations\n",
    "   - Average pooling smooths features\n",
    "   - Global average pooling replaces fully connected layers\n",
    "\n",
    "3. **Architecture Patterns**:\n",
    "   - VGG: Stack 3x3 convs with max pooling\n",
    "   - ResNet: Skip connections enable very deep networks\n",
    "   - DenseNet: Concatenate all previous features\n",
    "\n",
    "4. **Efficiency Techniques**:\n",
    "   - 1x1 convolutions for channel mixing\n",
    "   - Depthwise separable convolutions reduce parameters\n",
    "   - SE blocks add channel attention\n",
    "\n",
    "5. **Training CNNs**:\n",
    "   - Use batch normalization for stable training\n",
    "   - Data augmentation is crucial for generalization\n",
    "   - Learning rate schedules improve convergence\n",
    "\n",
    "### Output Size Formula\n",
    "\n",
    "```\n",
    "out_size = floor((in_size + 2*padding - dilation*(kernel-1) - 1) / stride + 1)\n",
    "```\n",
    "\n",
    "### Parameter Count\n",
    "\n",
    "```\n",
    "Conv2d params = out_channels * (in_channels * kernel_h * kernel_w + 1)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
