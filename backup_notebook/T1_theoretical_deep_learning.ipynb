{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Deep Learning: Foundations of Why Neural Networks Work\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Universal Approximation Theory**\n",
    "   - Why neural networks can approximate any function\n",
    "   - The role of depth vs width\n",
    "   - Connection to Neural Tangent Kernels\n",
    "\n",
    "2. **Loss Landscape Geometry**\n",
    "   - Why high-dimensional optimization isn't as hard as it seems\n",
    "   - Mode connectivity and loss surface structure\n",
    "   - Why SGD finds good solutions\n",
    "\n",
    "3. **Implicit Regularization**\n",
    "   - The implicit bias of gradient descent\n",
    "   - Double descent phenomenon\n",
    "   - Why overparameterized networks generalize\n",
    "\n",
    "**Note**: This notebook is theory-focused with visualizations and empirical demonstrations rather than traditional exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "from typing import Callable, List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Universal Approximation Theory\n",
    "\n",
    "### 1.1 The Universal Approximation Theorem\n",
    "\n",
    "**Theorem (Cybenko, 1989; Hornik et al., 1989)**: \n",
    "A feedforward neural network with:\n",
    "- A single hidden layer\n",
    "- Finite number of neurons\n",
    "- Non-polynomial activation function\n",
    "\n",
    "can approximate any continuous function on a compact subset of $\\mathbb{R}^n$ to arbitrary accuracy.\n",
    "\n",
    "**Mathematical Statement**:\n",
    "For any continuous function $f: [0,1]^n \\rightarrow \\mathbb{R}$ and any $\\epsilon > 0$, there exists a neural network $N$ such that:\n",
    "\n",
    "$$|N(x) - f(x)| < \\epsilon \\quad \\forall x \\in [0,1]^n$$\n",
    "\n",
    "**Key Insights**:\n",
    "1. This tells us neural networks *can* approximate functions (existence)\n",
    "2. It doesn't tell us *how many* neurons we need (efficiency)\n",
    "3. It doesn't tell us *how to find* the weights (optimization)\n",
    "4. It doesn't address *generalization* (learning from samples)\n",
    "\n",
    "### 1.2 Intuition: Building Functions from Bumps\n",
    "\n",
    "The proof works by showing that sigmoid/ReLU neurons can create \"bump\" functions, and any continuous function can be approximated by summing many bumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_bump_function(center: float, width: float, height: float, \n",
    "                        activation: str = 'sigmoid') -> Callable:\n",
    "    \"\"\"Create a bump function using neural network building blocks.\"\"\"\n",
    "    def bump(x):\n",
    "        if activation == 'sigmoid':\n",
    "            # Two sigmoids create a bump: σ(w(x-c+w)) - σ(w(x-c-w))\n",
    "            w = 1.0 / width\n",
    "            return height * (torch.sigmoid(w * (x - center + width)) - \n",
    "                           torch.sigmoid(w * (x - center - width)))\n",
    "        else:  # ReLU\n",
    "            # ReLU bump: ReLU(x-c+w) - 2*ReLU(x-c) + ReLU(x-c-w)\n",
    "            return height * (F.relu(x - center + width) - \n",
    "                           2 * F.relu(x - center) + \n",
    "                           F.relu(x - center - width)) / width\n",
    "    return bump\n",
    "\n",
    "# Demonstrate approximating a function with bumps\n",
    "x = torch.linspace(-1, 1, 1000)\n",
    "\n",
    "# Target function: a wiggly function\n",
    "target = torch.sin(5 * x) * torch.exp(-x**2)\n",
    "\n",
    "# Approximate with increasing numbers of bumps\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "\n",
    "for idx, n_bumps in enumerate([5, 10, 20, 50]):\n",
    "    # Create bumps at regular intervals\n",
    "    centers = torch.linspace(-1, 1, n_bumps)\n",
    "    width = 2.0 / n_bumps\n",
    "    \n",
    "    # Approximate: fit heights to match target\n",
    "    approximation = torch.zeros_like(x)\n",
    "    for center in centers:\n",
    "        bump = create_bump_function(center, width, 1.0, 'sigmoid')\n",
    "        # Simple fitting: height = target value at center\n",
    "        center_idx = (torch.abs(x - center)).argmin()\n",
    "        height = target[center_idx]\n",
    "        bump_with_height = create_bump_function(center, width, height, 'sigmoid')\n",
    "        approximation += bump_with_height(x)\n",
    "    \n",
    "    axes[idx].plot(x, target, 'b-', label='Target', linewidth=2, alpha=0.7)\n",
    "    axes[idx].plot(x, approximation, 'r--', label='Approximation', linewidth=1.5)\n",
    "    axes[idx].set_title(f'{n_bumps} Bumps\\nMSE: {F.mse_loss(approximation, target):.4f}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation: More bumps (neurons) → better approximation\")\n",
    "print(\"But the number of neurons needed grows exponentially with dimension!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Width vs Depth: The Expressivity Trade-off\n",
    "\n",
    "**Width** (neurons per layer):\n",
    "- Universal approximation with 1 hidden layer\n",
    "- May need exponentially many neurons\n",
    "- Suffers from curse of dimensionality\n",
    "\n",
    "**Depth** (number of layers):\n",
    "- Can represent functions more efficiently\n",
    "- Hierarchical feature composition\n",
    "- Some functions require exponentially fewer parameters with depth\n",
    "\n",
    "**Example**: Representing a Boolean function with $n$ inputs:\n",
    "- Shallow network: $O(2^n)$ neurons needed\n",
    "- Deep network: $O(n)$ neurons suffice\n",
    "\n",
    "Let's demonstrate this with a compositional function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class WideNetwork(nn.Module):\n",
    "    \"\"\"Single hidden layer with many neurons.\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    \"\"\"Multiple hidden layers with fewer neurons each.\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create a compositional function: f(x) = cos(sin(relu(x)))\n",
    "# This naturally has a hierarchical structure\n",
    "def compositional_function(x):\n",
    "    return torch.cos(torch.sin(F.relu(x.sum(dim=1, keepdim=True))))\n",
    "\n",
    "# Generate data\n",
    "n_samples = 5000\n",
    "input_dim = 10\n",
    "X = torch.randn(n_samples, input_dim)\n",
    "y = compositional_function(X)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Compare wide vs deep with same parameter budget\n",
    "param_budget = 5000  # Approximately\n",
    "\n",
    "# Wide network: 1 hidden layer\n",
    "# params ≈ input_dim * hidden + hidden * output = 10 * h + h * 1\n",
    "wide_hidden = param_budget // (input_dim + 1)  # ≈ 454\n",
    "wide_net = WideNetwork(input_dim, wide_hidden, 1).to(device)\n",
    "\n",
    "# Deep network: 5 hidden layers\n",
    "# params ≈ input_dim * h + 4 * h * h + h * 1\n",
    "# Solve: 10h + 4h² + h ≈ 5000 → h ≈ 31\n",
    "deep_hidden = 31\n",
    "deep_net = DeepNetwork(input_dim, deep_hidden, 5, 1).to(device)\n",
    "\n",
    "print(f\"Wide Network: {sum(p.numel() for p in wide_net.parameters()):,} parameters\")\n",
    "print(f\"Deep Network: {sum(p.numel() for p in deep_net.parameters()):,} parameters\")\n",
    "\n",
    "# Train both networks\n",
    "def train_network(model, loader, epochs=50):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_batch)\n",
    "            loss = F.mse_loss(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        losses.append(epoch_loss / len(loader))\n",
    "    \n",
    "    return losses\n",
    "\n",
    "print(\"\\nTraining wide network...\")\n",
    "wide_losses = train_network(wide_net, loader)\n",
    "\n",
    "print(\"Training deep network...\")\n",
    "deep_losses = train_network(deep_net, loader)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(wide_losses, label='Wide (1 layer, 454 neurons)', linewidth=2)\n",
    "plt.plot(deep_losses, label='Deep (5 layers, 31 neurons each)', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss (MSE)')\n",
    "plt.title('Width vs Depth: Same Parameter Budget')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal losses:\")\n",
    "print(f\"Wide network: {wide_losses[-1]:.6f}\")\n",
    "print(f\"Deep network: {deep_losses[-1]:.6f}\")\n",
    "print(f\"\\nDeep network achieves {wide_losses[-1]/deep_losses[-1]:.2f}x lower loss!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Neural Tangent Kernel (NTK) Perspective\n",
    "\n",
    "**Key Idea**: In the limit of infinite width, neural network training behaves like kernel regression!\n",
    "\n",
    "**The Neural Tangent Kernel**:\n",
    "$$K_{\\text{NTK}}(x, x') = \\mathbb{E}_{\\theta \\sim \\text{init}}\\left[\\left\\langle \\frac{\\partial f(x; \\theta)}{\\partial \\theta}, \\frac{\\partial f(x'; \\theta)}{\\partial \\theta} \\right\\rangle\\right]$$\n",
    "\n",
    "**Why it matters**:\n",
    "1. Explains why gradient descent finds global minima\n",
    "2. Connects to classical kernel methods\n",
    "3. In infinite width limit, the kernel is *constant* during training\n",
    "\n",
    "**Training dynamics** become:\n",
    "$$\\frac{df(x; \\theta(t))}{dt} = -\\eta \\sum_{i} K_{\\text{NTK}}(x, x_i)(f(x_i; \\theta(t)) - y_i)$$\n",
    "\n",
    "This is a *linear* ODE in function space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_ntk(model: nn.Module, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute the Neural Tangent Kernel empirically.\n",
    "    \n",
    "    NTK(x1, x2) = <∂f(x1)/∂θ, ∂f(x2)/∂θ>\n",
    "    \"\"\"\n",
    "    n1, n2 = x1.shape[0], x2.shape[0]\n",
    "    kernel = torch.zeros(n1, n2)\n",
    "    \n",
    "    for i in range(n1):\n",
    "        # Compute gradient of f(x1[i]) w.r.t. parameters\n",
    "        model.zero_grad()\n",
    "        out1 = model(x1[i:i+1])\n",
    "        grad1 = torch.autograd.grad(out1, model.parameters(), \n",
    "                                    create_graph=False, retain_graph=False)\n",
    "        grad1_flat = torch.cat([g.flatten() for g in grad1])\n",
    "        \n",
    "        for j in range(n2):\n",
    "            # Compute gradient of f(x2[j]) w.r.t. parameters\n",
    "            model.zero_grad()\n",
    "            out2 = model(x2[j:j+1])\n",
    "            grad2 = torch.autograd.grad(out2, model.parameters(), \n",
    "                                       create_graph=False, retain_graph=False)\n",
    "            grad2_flat = torch.cat([g.flatten() for g in grad2])\n",
    "            \n",
    "            # Inner product of gradients\n",
    "            kernel[i, j] = torch.dot(grad1_flat, grad2_flat).item()\n",
    "    \n",
    "    return kernel\n",
    "\n",
    "# Demonstrate NTK behavior with different widths\n",
    "input_dim = 5\n",
    "n_points = 20\n",
    "X_test = torch.randn(n_points, input_dim)\n",
    "\n",
    "widths = [10, 50, 200, 1000]\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for idx, width in enumerate(widths):\n",
    "    # Create network\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_dim, width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(width, 1)\n",
    "    )\n",
    "    \n",
    "    # Initialize with scaled initialization (important for NTK regime)\n",
    "    with torch.no_grad():\n",
    "        for layer in model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data *= 1.0 / np.sqrt(width)\n",
    "    \n",
    "    # Compute NTK at initialization\n",
    "    with torch.no_grad():\n",
    "        ntk_init = compute_ntk(model, X_test, X_test)\n",
    "    \n",
    "    # Train for a few steps\n",
    "    y_dummy = torch.randn(n_points, 1)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.mse_loss(model(X_test), y_dummy)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Compute NTK after training\n",
    "    with torch.no_grad():\n",
    "        ntk_trained = compute_ntk(model, X_test, X_test)\n",
    "    \n",
    "    # Plot NTKs\n",
    "    im1 = axes[0, idx].imshow(ntk_init.numpy(), cmap='viridis')\n",
    "    axes[0, idx].set_title(f'Width={width}\\nNTK at Init')\n",
    "    plt.colorbar(im1, ax=axes[0, idx])\n",
    "    \n",
    "    im2 = axes[1, idx].imshow(ntk_trained.numpy(), cmap='viridis')\n",
    "    axes[1, idx].set_title('NTK after Training')\n",
    "    plt.colorbar(im2, ax=axes[1, idx])\n",
    "    \n",
    "    # Compute relative change\n",
    "    rel_change = torch.norm(ntk_trained - ntk_init) / torch.norm(ntk_init)\n",
    "    print(f\"Width {width}: Relative NTK change = {rel_change:.4f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation: As width increases, NTK changes less during training.\")\n",
    "print(\"In the infinite width limit, NTK is constant → training becomes kernel regression!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Loss Landscape Geometry\n",
    "\n",
    "### 2.1 The Mystery of High-Dimensional Optimization\n",
    "\n",
    "**The Paradox**: \n",
    "- Neural networks have millions of parameters\n",
    "- Loss landscape is non-convex with many local minima\n",
    "- Yet gradient descent reliably finds good solutions\n",
    "\n",
    "**Why?**\n",
    "\n",
    "1. **Most critical points are saddle points, not local minima**\n",
    "   - In high dimensions, most critical points (∇L = 0) have both positive and negative curvature\n",
    "   - True local minima are exponentially rare\n",
    "   - Gradient descent can escape saddle points\n",
    "\n",
    "2. **Local minima are all roughly equally good**\n",
    "   - Empirically, different local minima achieve similar test loss\n",
    "   - This is not true for all non-convex problems!\n",
    "\n",
    "3. **Mode connectivity**\n",
    "   - Different minima are connected by paths of nearly constant loss\n",
    "   - The loss landscape has a \"flat\" basin structure\n",
    "\n",
    "### 2.2 Visualizing Loss Landscapes\n",
    "\n",
    "We can visualize 2D slices of the loss landscape using random directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_random_direction(model: nn.Module, normalize: bool = True) -> List[torch.Tensor]:\n",
    "    \"\"\"Generate a random direction in parameter space.\"\"\"\n",
    "    direction = []\n",
    "    for param in model.parameters():\n",
    "        if normalize:\n",
    "            # Filter-wise normalization (better for visualization)\n",
    "            random_dir = torch.randn_like(param)\n",
    "            random_dir = random_dir / (torch.norm(random_dir) + 1e-10)\n",
    "            random_dir = random_dir * torch.norm(param)\n",
    "        else:\n",
    "            random_dir = torch.randn_like(param)\n",
    "        direction.append(random_dir)\n",
    "    return direction\n",
    "\n",
    "def set_weights(model: nn.Module, base_weights: List[torch.Tensor], \n",
    "                direction1: List[torch.Tensor], direction2: List[torch.Tensor],\n",
    "                alpha: float, beta: float) -> None:\n",
    "    \"\"\"Set model weights to: base + alpha * dir1 + beta * dir2.\"\"\"\n",
    "    for param, base, d1, d2 in zip(model.parameters(), base_weights, direction1, direction2):\n",
    "        param.data = base + alpha * d1 + beta * d2\n",
    "\n",
    "def evaluate_loss_landscape(model: nn.Module, loader: DataLoader,\n",
    "                           base_weights: List[torch.Tensor],\n",
    "                           direction1: List[torch.Tensor], \n",
    "                           direction2: List[torch.Tensor],\n",
    "                           range_val: float = 1.0, \n",
    "                           resolution: int = 25) -> np.ndarray:\n",
    "    \"\"\"Evaluate loss on a 2D grid around base_weights.\"\"\"\n",
    "    alphas = np.linspace(-range_val, range_val, resolution)\n",
    "    betas = np.linspace(-range_val, range_val, resolution)\n",
    "    \n",
    "    loss_surface = np.zeros((resolution, resolution))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            for j, beta in enumerate(betas):\n",
    "                set_weights(model, base_weights, direction1, direction2, alpha, beta)\n",
    "                \n",
    "                total_loss = 0\n",
    "                for X_batch, y_batch in loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    pred = model(X_batch)\n",
    "                    loss = F.mse_loss(pred, y_batch)\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                loss_surface[i, j] = total_loss / len(loader)\n",
    "    \n",
    "    # Restore base weights\n",
    "    set_weights(model, base_weights, direction1, direction2, 0, 0)\n",
    "    \n",
    "    return loss_surface, alphas, betas\n",
    "\n",
    "# Create a simple dataset\n",
    "X_data = torch.randn(1000, 5)\n",
    "y_data = torch.sin(X_data.sum(dim=1, keepdim=True))\n",
    "dataset = TensorDataset(X_data, y_data)\n",
    "loader = DataLoader(dataset, batch_size=100)\n",
    "\n",
    "# Create and train a small network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training model...\")\n",
    "for epoch in range(100):\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.mse_loss(model(X_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save trained weights\n",
    "trained_weights = [p.data.clone() for p in model.parameters()]\n",
    "\n",
    "# Generate random directions\n",
    "dir1 = get_random_direction(model)\n",
    "dir2 = get_random_direction(model)\n",
    "\n",
    "# Evaluate loss landscape around trained weights\n",
    "print(\"Evaluating loss landscape (this may take a minute)...\")\n",
    "loss_surface, alphas, betas = evaluate_loss_landscape(\n",
    "    model, loader, trained_weights, dir1, dir2, range_val=0.5, resolution=20\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "A, B = np.meshgrid(alphas, betas)\n",
    "surf = ax1.plot_surface(A, B, loss_surface.T, cmap=cm.viridis, alpha=0.8)\n",
    "ax1.set_xlabel('Direction 1')\n",
    "ax1.set_ylabel('Direction 2')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('Loss Landscape (3D)')\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contourf(A, B, loss_surface.T, levels=20, cmap='viridis')\n",
    "ax2.plot(0, 0, 'r*', markersize=15, label='Trained weights')\n",
    "ax2.set_xlabel('Direction 1')\n",
    "ax2.set_ylabel('Direction 2')\n",
    "ax2.set_title('Loss Landscape (Contour)')\n",
    "ax2.legend()\n",
    "fig.colorbar(contour, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMinimum loss in landscape: {loss_surface.min():.6f}\")\n",
    "print(f\"Loss at trained weights (center): {loss_surface[len(alphas)//2, len(betas)//2]:.6f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mode Connectivity\n",
    "\n",
    "**Observation (Garipov et al., 2018)**: Different SGD runs find different minima, but these minima are connected by paths of nearly constant low loss.\n",
    "\n",
    "**Linear mode connectivity**: For some pairs of minima θ₁ and θ₂:\n",
    "$$L(\\alpha \\theta_1 + (1-\\alpha) \\theta_2) \\approx L(\\theta_1) \\approx L(\\theta_2) \\quad \\forall \\alpha \\in [0,1]$$\n",
    "\n",
    "**Why it matters**:\n",
    "- Suggests the loss landscape has wide, flat basins\n",
    "- Different solutions found by SGD are in the same \"mode\"\n",
    "- Explains why ensembling and learning rate decay work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_independent_model(dataset: TensorDataset, architecture: nn.Module, \n",
    "                          epochs: int = 50) -> nn.Module:\n",
    "    \"\"\"Train a model from random initialization.\"\"\"\n",
    "    model = architecture.to(device)\n",
    "    loader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.mse_loss(model(X_batch), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def interpolate_models(model1: nn.Module, model2: nn.Module, \n",
    "                      alpha: float) -> nn.Module:\n",
    "    \"\"\"Create a model with weights: alpha * w1 + (1-alpha) * w2.\"\"\"\n",
    "    interpolated = type(model1)().to(device)\n",
    "    \n",
    "    for p_interp, p1, p2 in zip(interpolated.parameters(), \n",
    "                                 model1.parameters(), \n",
    "                                 model2.parameters()):\n",
    "        p_interp.data = alpha * p1.data + (1 - alpha) * p2.data\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "def evaluate_model(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"Evaluate model loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            loss = F.mse_loss(pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Create dataset\n",
    "n_samples = 2000\n",
    "X = torch.randn(n_samples, 10)\n",
    "y = torch.sin(X[:, :3].sum(dim=1, keepdim=True)) + 0.1 * torch.randn(n_samples, 1)\n",
    "dataset = TensorDataset(X, y)\n",
    "eval_loader = DataLoader(dataset, batch_size=100)\n",
    "\n",
    "# Define architecture\n",
    "def create_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(10, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, 1)\n",
    "    )\n",
    "\n",
    "# Train two independent models\n",
    "print(\"Training Model 1...\")\n",
    "model1 = train_independent_model(dataset, create_model(), epochs=100)\n",
    "\n",
    "print(\"Training Model 2...\")\n",
    "model2 = train_independent_model(dataset, create_model(), epochs=100)\n",
    "\n",
    "# Evaluate along the linear path between them\n",
    "alphas = np.linspace(0, 1, 21)\n",
    "losses = []\n",
    "\n",
    "print(\"Evaluating mode connectivity...\")\n",
    "for alpha in alphas:\n",
    "    interpolated = interpolate_models(model1, model2, alpha)\n",
    "    loss = evaluate_model(interpolated, eval_loader)\n",
    "    losses.append(loss)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(alphas, losses, 'o-', linewidth=2, markersize=6)\n",
    "plt.axhline(y=losses[0], color='r', linestyle='--', alpha=0.5, label='Model 1 loss')\n",
    "plt.axhline(y=losses[-1], color='b', linestyle='--', alpha=0.5, label='Model 2 loss')\n",
    "plt.xlabel('Interpolation parameter α')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Linear Mode Connectivity\\n(α=0: Model 1, α=1: Model 2)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Compute barrier height\n",
    "barrier_height = max(losses) - min(losses[0], losses[-1])\n",
    "print(f\"\\nModel 1 loss: {losses[0]:.6f}\")\n",
    "print(f\"Model 2 loss: {losses[-1]:.6f}\")\n",
    "print(f\"Maximum loss on path: {max(losses):.6f}\")\n",
    "print(f\"Barrier height: {barrier_height:.6f}\")\n",
    "\n",
    "if barrier_height < 0.01:\n",
    "    print(\"\\n✓ Strong mode connectivity! Models are in the same basin.\")\n",
    "else:\n",
    "    print(\"\\n⚠ Weak mode connectivity. Models may be in different basins.\")\n",
    "    print(\"  (Try training longer or with different hyperparameters)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Why SGD Finds Good Minima\n",
    "\n",
    "**Key insights**:\n",
    "\n",
    "1. **Stochasticity helps escape sharp minima**\n",
    "   - Sharp minima → high curvature → gradient noise is amplified\n",
    "   - Flat minima → low curvature → gradient noise has less effect\n",
    "   - SGD naturally prefers flat minima\n",
    "\n",
    "2. **Flat minima generalize better**\n",
    "   - Sharp minima are sensitive to perturbations\n",
    "   - Flat minima are robust → better test performance\n",
    "\n",
    "3. **Implicit regularization**\n",
    "   - SGD implicitly regularizes toward simpler solutions\n",
    "   - Related to the trajectory through parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_sharpness(model: nn.Module, loader: DataLoader, epsilon: float = 0.01) -> float:\n",
    "    \"\"\"Compute sharpness: max loss increase when perturbing weights by epsilon.\n",
    "    \n",
    "    Sharpness measures the maximum eigenvalue of the Hessian (approximately).\n",
    "    \"\"\"\n",
    "    original_loss = evaluate_model(model, loader)\n",
    "    \n",
    "    # Perturb in random directions and measure worst-case loss increase\n",
    "    max_loss = original_loss\n",
    "    \n",
    "    for _ in range(10):  # Try 10 random directions\n",
    "        # Save original weights\n",
    "        original_weights = [p.data.clone() for p in model.parameters()]\n",
    "        \n",
    "        # Perturb\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters():\n",
    "                perturbation = torch.randn_like(p) * epsilon\n",
    "                p.data.add_(perturbation)\n",
    "        \n",
    "        # Evaluate\n",
    "        perturbed_loss = evaluate_model(model, loader)\n",
    "        max_loss = max(max_loss, perturbed_loss)\n",
    "        \n",
    "        # Restore\n",
    "        with torch.no_grad():\n",
    "            for p, orig in zip(model.parameters(), original_weights):\n",
    "                p.data.copy_(orig)\n",
    "    \n",
    "    sharpness = (max_loss - original_loss) / epsilon\n",
    "    return sharpness\n",
    "\n",
    "# Train models with SGD vs full-batch GD\n",
    "def train_with_optimizer(dataset: TensorDataset, batch_size: int, \n",
    "                        lr: float, epochs: int = 100) -> Tuple[nn.Module, List[float]]:\n",
    "    \"\"\"Train a model and return it with training losses.\"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(10, 30),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(30, 1)\n",
    "    ).to(device)\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.mse_loss(model(X_batch), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(loader))\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "# Create train and test sets\n",
    "X_train = torch.randn(1000, 10)\n",
    "y_train = torch.sin(X_train[:, :3].sum(dim=1, keepdim=True)) + 0.2 * torch.randn(1000, 1)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "X_test = torch.randn(500, 10)\n",
    "y_test = torch.sin(X_test[:, :3].sum(dim=1, keepdim=True)) + 0.2 * torch.randn(500, 1)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100)\n",
    "\n",
    "# Train with different batch sizes\n",
    "print(\"Training with full-batch GD (batch_size=1000)...\")\n",
    "model_gd, losses_gd = train_with_optimizer(train_dataset, batch_size=1000, lr=0.1)\n",
    "\n",
    "print(\"Training with mini-batch SGD (batch_size=32)...\")\n",
    "model_sgd, losses_sgd = train_with_optimizer(train_dataset, batch_size=32, lr=0.01)\n",
    "\n",
    "# Evaluate test performance\n",
    "test_loss_gd = evaluate_model(model_gd, test_loader)\n",
    "test_loss_sgd = evaluate_model(model_sgd, test_loader)\n",
    "\n",
    "# Compute sharpness\n",
    "print(\"\\nComputing sharpness (this may take a moment)...\")\n",
    "sharpness_gd = compute_sharpness(model_gd, test_loader)\n",
    "sharpness_sgd = compute_sharpness(model_sgd, test_loader)\n",
    "\n",
    "# Results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Training curves\n",
    "axes[0].plot(losses_gd, label='Full-batch GD', linewidth=2)\n",
    "axes[0].plot(losses_sgd, label='Mini-batch SGD', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison\n",
    "metrics = ['Test Loss', 'Sharpness']\n",
    "gd_values = [test_loss_gd, sharpness_gd]\n",
    "sgd_values = [test_loss_sgd, sharpness_sgd]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, gd_values, width, label='Full-batch GD', alpha=0.8)\n",
    "axes[1].bar(x + width/2, sgd_values, width, label='Mini-batch SGD', alpha=0.8)\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('Test Performance vs Sharpness')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Full-batch GD: Test loss = {test_loss_gd:.6f}, Sharpness = {sharpness_gd:.4f}\")\n",
    "print(f\"Mini-batch SGD: Test loss = {test_loss_sgd:.6f}, Sharpness = {sharpness_sgd:.4f}\")\n",
    "print(f\"\\nSGD finds a {sharpness_gd/sharpness_sgd:.2f}x flatter minimum!\")\n",
    "print(f\"Flatter minimum → {(test_loss_gd/test_loss_sgd - 1)*100:.1f}% better generalization\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Implicit Regularization and the Double Descent Phenomenon\n",
    "\n",
    "### 3.1 Implicit Bias of Gradient Descent\n",
    "\n",
    "**Key Question**: When there are multiple solutions that fit the training data perfectly, which one does gradient descent find?\n",
    "\n",
    "**Answer**: Gradient descent has an *implicit bias* toward simpler solutions, even without explicit regularization!\n",
    "\n",
    "**For linear models**: GD converges to the minimum ℓ₂-norm solution:\n",
    "$$\\theta_{\\text{GD}} = \\arg\\min_{\\theta: X\\theta = y} \\|\\theta\\|_2$$\n",
    "\n",
    "**For neural networks**: The implicit bias is more complex, but roughly:\n",
    "- Early stopping → implicit regularization\n",
    "- SGD noise → prefers flat minima → better generalization\n",
    "- Architecture → inductive bias (e.g., CNNs prefer local patterns)\n",
    "\n",
    "### 3.2 The Double Descent Phenomenon\n",
    "\n",
    "**Classical statistical learning**: \n",
    "- Underparameterized (# params < # samples) → underfitting\n",
    "- Perfectly parameterized (# params ≈ # samples) → best performance\n",
    "- Overparameterized (# params > # samples) → overfitting\n",
    "\n",
    "**Modern deep learning**: This is wrong!\n",
    "\n",
    "**Double descent curve**:\n",
    "1. Classical regime: test error decreases as model size increases (underfitting → fitting)\n",
    "2. Interpolation threshold: test error spikes when # params ≈ # samples\n",
    "3. Modern regime: test error *decreases again* as we add more parameters!\n",
    "\n",
    "**Why?**\n",
    "- At interpolation threshold: many solutions fit training data, GD finds a bad one\n",
    "- In overparameterized regime: implicit regularization kicks in, GD finds smoother solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_model_with_width(width: int) -> nn.Module:\n",
    "    \"\"\"Create a 2-layer network with specified width.\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(10, width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(width, 1)\n",
    "    )\n",
    "\n",
    "def train_to_convergence(model: nn.Module, train_loader: DataLoader, \n",
    "                        max_epochs: int = 500, patience: int = 50) -> Tuple[float, float]:\n",
    "    \"\"\"Train until convergence and return train and test losses.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_batch)\n",
    "            loss = F.mse_loss(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= len(train_loader)\n",
    "        \n",
    "        if epoch_loss < best_train_loss - 1e-6:\n",
    "            best_train_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    train_loss = evaluate_model(model, train_loader)\n",
    "    test_loss = evaluate_model(model, test_loader)\n",
    "    \n",
    "    return train_loss, test_loss\n",
    "\n",
    "# Create a small dataset to see double descent clearly\n",
    "n_train = 100\n",
    "n_test = 500\n",
    "input_dim = 10\n",
    "\n",
    "# Training data\n",
    "X_train = torch.randn(n_train, input_dim)\n",
    "# True function with noise\n",
    "true_signal = torch.sin(X_train[:, 0]) + torch.cos(X_train[:, 1])\n",
    "y_train = (true_signal + 0.3 * torch.randn(n_train)).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Test data (clean)\n",
    "X_test = torch.randn(n_test, input_dim)\n",
    "true_signal_test = torch.sin(X_test[:, 0]) + torch.cos(X_test[:, 1])\n",
    "y_test = (true_signal_test + 0.3 * torch.randn(n_test)).unsqueeze(1)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100)\n",
    "\n",
    "# Sweep over model widths\n",
    "widths = [1, 2, 3, 5, 8, 10, 15, 20, 30, 50, 80, 100, 150, 200]\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "n_params_list = []\n",
    "\n",
    "print(\"Training models with different widths...\")\n",
    "for width in tqdm(widths):\n",
    "    model = create_model_with_width(width).to(device)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    n_params_list.append(n_params)\n",
    "    \n",
    "    train_loss, test_loss = train_to_convergence(model, train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(f\"Width {width:3d} ({n_params:5d} params): Train {train_loss:.4f}, Test {test_loss:.4f}\")\n",
    "\n",
    "# Plot double descent\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss vs width\n",
    "axes[0].plot(widths, train_losses, 'o-', label='Train Loss', linewidth=2, markersize=6)\n",
    "axes[0].plot(widths, test_losses, 's-', label='Test Loss', linewidth=2, markersize=6)\n",
    "axes[0].axvline(x=n_train/(input_dim+1), color='r', linestyle='--', \n",
    "                alpha=0.5, label='Interpolation threshold')\n",
    "axes[0].set_xlabel('Model Width (hidden layer size)')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Double Descent: Loss vs Model Width')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "# Loss vs number of parameters\n",
    "axes[1].plot(n_params_list, train_losses, 'o-', label='Train Loss', linewidth=2, markersize=6)\n",
    "axes[1].plot(n_params_list, test_losses, 's-', label='Test Loss', linewidth=2, markersize=6)\n",
    "axes[1].axvline(x=n_train, color='r', linestyle='--', \n",
    "                alpha=0.5, label=f'# training samples ({n_train})')\n",
    "axes[1].set_xlabel('Number of Parameters')\n",
    "axes[1].set_ylabel('Loss (MSE)')\n",
    "axes[1].set_title('Double Descent: Loss vs Number of Parameters')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the interpolation peak\n",
    "peak_idx = np.argmax(test_losses)\n",
    "print(f\"\\nDouble Descent Observations:\")\n",
    "print(f\"Test loss peak at width={widths[peak_idx]} ({n_params_list[peak_idx]} parameters)\")\n",
    "print(f\"Peak test loss: {test_losses[peak_idx]:.4f}\")\n",
    "print(f\"Final test loss (width={widths[-1]}): {test_losses[-1]:.4f}\")\n",
    "print(f\"Overparameterization helps! {(test_losses[peak_idx]/test_losses[-1]):.2f}x improvement.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Why Overparameterization Helps: Lottery Ticket Hypothesis\n",
    "\n",
    "**Lottery Ticket Hypothesis (Frankle & Carbin, 2019)**:\n",
    "> A randomly-initialized, dense neural network contains a subnetwork that, when trained in isolation, can match the test accuracy of the original network.\n",
    "\n",
    "**Implications**:\n",
    "1. Large networks are easier to train because they contain many \"winning lottery tickets\"\n",
    "2. Overparameterization helps optimization, not just representation\n",
    "3. The right initialization matters: the winning ticket must have lucky initial weights\n",
    "\n",
    "**Analogy**: \n",
    "- Small network: buying 1 lottery ticket (might not win)\n",
    "- Large network: buying 1000 lottery tickets (likely to have a winner)\n",
    "\n",
    "Let's demonstrate this with a simple pruning experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prune_weights(model: nn.Module, prune_percent: float) -> nn.Module:\n",
    "    \"\"\"Prune smallest weights by magnitude.\"\"\"\n",
    "    # Collect all weights\n",
    "    all_weights = []\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            all_weights.extend(param.data.abs().flatten().tolist())\n",
    "    \n",
    "    # Find threshold\n",
    "    threshold = np.percentile(all_weights, prune_percent)\n",
    "    \n",
    "    # Create mask\n",
    "    masks = []\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            mask = (param.data.abs() > threshold).float()\n",
    "            masks.append(mask)\n",
    "            param.data.mul_(mask)\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def apply_mask(model: nn.Module, masks: List[torch.Tensor]) -> None:\n",
    "    \"\"\"Apply saved masks to model (for retraining with same structure).\"\"\"\n",
    "    mask_idx = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data.mul_(masks[mask_idx])\n",
    "            mask_idx += 1\n",
    "\n",
    "# Create dataset\n",
    "X_train = torch.randn(500, 10)\n",
    "y_train = (torch.sin(X_train[:, 0]) + 0.2 * torch.randn(500)).unsqueeze(1)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "X_test = torch.randn(200, 10)\n",
    "y_test = (torch.sin(X_test[:, 0]) + 0.2 * torch.randn(200)).unsqueeze(1)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100)\n",
    "\n",
    "# 1. Train full network\n",
    "print(\"1. Training full network...\")\n",
    "full_model = nn.Sequential(\n",
    "    nn.Linear(10, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 1)\n",
    ").to(device)\n",
    "\n",
    "# Save initial weights\n",
    "init_weights = [p.data.clone() for p in full_model.parameters()]\n",
    "\n",
    "# Train\n",
    "optimizer = torch.optim.Adam(full_model.parameters(), lr=0.001)\n",
    "for epoch in range(100):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.mse_loss(full_model(X_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "full_test_loss = evaluate_model(full_model, test_loader)\n",
    "print(f\"Full network test loss: {full_test_loss:.6f}\")\n",
    "\n",
    "# 2. Prune and continue training (\"fine-tuning\")\n",
    "print(\"\\n2. Pruning 80% of weights and fine-tuning...\")\n",
    "pruned_finetuned = nn.Sequential(\n",
    "    nn.Linear(10, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 1)\n",
    ").to(device)\n",
    "\n",
    "# Copy trained weights\n",
    "with torch.no_grad():\n",
    "    for p_pruned, p_full in zip(pruned_finetuned.parameters(), full_model.parameters()):\n",
    "        p_pruned.data.copy_(p_full.data)\n",
    "\n",
    "# Prune\n",
    "masks = prune_weights(pruned_finetuned, 80)\n",
    "\n",
    "# Fine-tune\n",
    "optimizer = torch.optim.Adam(pruned_finetuned.parameters(), lr=0.001)\n",
    "for epoch in range(50):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.mse_loss(pruned_finetuned(X_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        apply_mask(pruned_finetuned, masks)  # Maintain sparsity\n",
    "\n",
    "pruned_finetuned_loss = evaluate_model(pruned_finetuned, test_loader)\n",
    "print(f\"Pruned + fine-tuned test loss: {pruned_finetuned_loss:.6f}\")\n",
    "\n",
    "# 3. The Lottery Ticket: retrain from initialization with the same mask\n",
    "print(\"\\n3. Lottery Ticket: Retraining from initialization with same mask...\")\n",
    "lottery_ticket = nn.Sequential(\n",
    "    nn.Linear(10, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 1)\n",
    ").to(device)\n",
    "\n",
    "# Reset to initial weights\n",
    "with torch.no_grad():\n",
    "    for p_lottery, p_init in zip(lottery_ticket.parameters(), init_weights):\n",
    "        p_lottery.data.copy_(p_init)\n",
    "\n",
    "# Apply mask\n",
    "apply_mask(lottery_ticket, masks)\n",
    "\n",
    "# Retrain from scratch with mask\n",
    "optimizer = torch.optim.Adam(lottery_ticket.parameters(), lr=0.001)\n",
    "for epoch in range(100):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.mse_loss(lottery_ticket(X_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        apply_mask(lottery_ticket, masks)  # Maintain sparsity\n",
    "\n",
    "lottery_ticket_loss = evaluate_model(lottery_ticket, test_loader)\n",
    "print(f\"Lottery ticket test loss: {lottery_ticket_loss:.6f}\")\n",
    "\n",
    "# 4. Random pruning baseline\n",
    "print(\"\\n4. Baseline: Random 80% pruning from initialization...\")\n",
    "random_pruned = nn.Sequential(\n",
    "    nn.Linear(10, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 1)\n",
    ").to(device)\n",
    "\n",
    "# Reset to initial weights\n",
    "with torch.no_grad():\n",
    "    for p_random, p_init in zip(random_pruned.parameters(), init_weights):\n",
    "        p_random.data.copy_(p_init)\n",
    "\n",
    "# Random mask\n",
    "random_masks = []\n",
    "for param in random_pruned.parameters():\n",
    "    if param.requires_grad:\n",
    "        mask = (torch.rand_like(param) > 0.8).float()\n",
    "        random_masks.append(mask)\n",
    "        param.data.mul_(mask)\n",
    "\n",
    "# Train\n",
    "optimizer = torch.optim.Adam(random_pruned.parameters(), lr=0.001)\n",
    "for epoch in range(100):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.mse_loss(random_pruned(X_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        apply_mask(random_pruned, random_masks)\n",
    "\n",
    "random_pruned_loss = evaluate_model(random_pruned, test_loader)\n",
    "print(f\"Random pruning test loss: {random_pruned_loss:.6f}\")\n",
    "\n",
    "# Visualize results\n",
    "models = ['Full\\nNetwork', 'Pruned +\\nFine-tuned', 'Lottery\\nTicket', 'Random\\nPruning']\n",
    "losses = [full_test_loss, pruned_finetuned_loss, lottery_ticket_loss, random_pruned_loss]\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, losses, color=colors, alpha=0.7)\n",
    "plt.ylabel('Test Loss (MSE)')\n",
    "plt.title('Lottery Ticket Hypothesis Demonstration\\n(80% of weights pruned)')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, loss) in enumerate(zip(bars, losses)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{loss:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Key Findings ===\")\n",
    "print(f\"1. Full network: {full_test_loss:.6f}\")\n",
    "print(f\"2. Lottery ticket (20% weights): {lottery_ticket_loss:.6f} \"\n",
    "      f\"({(full_test_loss/lottery_ticket_loss):.2f}x as good as full!)\")\n",
    "print(f\"3. Random pruning: {random_pruned_loss:.6f} \"\n",
    "      f\"({(random_pruned_loss/lottery_ticket_loss):.2f}x worse than lottery ticket)\")\n",
    "print(f\"\\n→ The 'winning ticket' subnetwork performs nearly as well as the full network!\")\n",
    "print(f\"→ This explains why overparameterization helps: more chances to find a winning ticket.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Key Theoretical Insights\n",
    "\n",
    "### 1. Universal Approximation\n",
    "- ✓ Neural networks *can* represent any function (with enough neurons)\n",
    "- ✓ Depth is more efficient than width for compositional functions\n",
    "- ✓ Neural Tangent Kernel connects infinite-width networks to kernel methods\n",
    "- ⚠ Representation ≠ Learnability: finding the weights is a separate challenge\n",
    "\n",
    "### 2. Loss Landscape Geometry\n",
    "- ✓ High-dimensional loss landscapes are easier to optimize than naive intuition suggests\n",
    "- ✓ Most critical points are saddle points (escapable), not local minima (stuck)\n",
    "- ✓ Different minima are connected by low-loss paths (mode connectivity)\n",
    "- ✓ SGD naturally finds flat minima → better generalization\n",
    "\n",
    "### 3. Implicit Regularization\n",
    "- ✓ Gradient descent has implicit bias toward simpler solutions\n",
    "- ✓ Double descent: overparameterization helps generalization (contrary to classical wisdom)\n",
    "- ✓ Lottery Ticket Hypothesis: large networks contain trainable subnetworks\n",
    "- ✓ Overparameterization helps *optimization*, not just *representation*\n",
    "\n",
    "### Why This Matters for Practice\n",
    "\n",
    "**Architecture design**:\n",
    "- Prefer deeper networks over wider ones (when possible)\n",
    "- Don't fear overparameterization\n",
    "- Consider the inductive bias of your architecture\n",
    "\n",
    "**Training**:\n",
    "- Use SGD/mini-batch training (not full-batch)\n",
    "- Early stopping provides implicit regularization\n",
    "- Learning rate matters: affects which basin you reach\n",
    "\n",
    "**Debugging**:\n",
    "- If training fails, try a larger network (more lottery tickets)\n",
    "- Check for sharp minima (high sensitivity to perturbations)\n",
    "- Visualize loss landscapes to understand optimization difficulties\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "**Universal Approximation**:\n",
    "- Cybenko (1989): \"Approximation by superpositions of a sigmoidal function\"\n",
    "- Hornik et al. (1989): \"Multilayer feedforward networks are universal approximators\"\n",
    "- Jacot et al. (2018): \"Neural Tangent Kernel: Convergence and Generalization in Neural Networks\"\n",
    "\n",
    "**Loss Landscapes**:\n",
    "- Goodfellow et al. (2015): \"Qualitatively characterizing neural network optimization problems\"\n",
    "- Garipov et al. (2018): \"Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs\"\n",
    "- Li et al. (2018): \"Visualizing the Loss Landscape of Neural Nets\"\n",
    "\n",
    "**Implicit Regularization**:\n",
    "- Zhang et al. (2017): \"Understanding deep learning requires rethinking generalization\"\n",
    "- Nakkiran et al. (2020): \"Deep Double Descent: Where Bigger Models and More Data Hurt\"\n",
    "- Frankle & Carbin (2019): \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\"\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "These theoretical insights reveal that deep learning's success is not accidental:\n",
    "- The geometry of high-dimensional spaces is actually helpful\n",
    "- Optimization algorithms have beneficial implicit biases\n",
    "- Overparameterization is a feature, not a bug\n",
    "\n",
    "Understanding *why* neural networks work helps us:\n",
    "- Design better architectures\n",
    "- Choose appropriate training procedures\n",
    "- Debug failures more effectively\n",
    "- Push the boundaries of what's possible\n",
    "\n",
    "The gap between theory and practice is closing, but many mysteries remain. Keep experimenting, keep learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
