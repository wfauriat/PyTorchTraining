{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - Recurrent Neural Networks and LSTMs\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand RNN fundamentals** - How recurrent connections process sequences\n",
    "2. **Implement RNNs from scratch** - Build vanilla RNN cells and understand backpropagation through time\n",
    "3. **Master the vanishing gradient problem** - Why it happens and how to diagnose it\n",
    "4. **Work with LSTMs and GRUs** - Gating mechanisms that solve vanishing gradients\n",
    "5. **Build sequence models** - Language modeling, sequence classification, seq2seq\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional\n",
    "import math\n",
    "import string\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. RNN Fundamentals\n",
    "\n",
    "### 1.1 What is a Recurrent Neural Network?\n",
    "\n",
    "An RNN processes sequences by maintaining a **hidden state** that gets updated at each timestep:\n",
    "\n",
    "```\n",
    "h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b)\n",
    "```\n",
    "\n",
    "The same weights are applied at every timestep (weight sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla RNN cell implementation from scratch.\n",
    "    \n",
    "    h_t = tanh(W_ih @ x_t + W_hh @ h_{t-1} + b)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Input-to-hidden weights\n",
    "        self.W_ih = nn.Parameter(torch.randn(hidden_size, input_size) / math.sqrt(input_size))\n",
    "        # Hidden-to-hidden weights\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size) / math.sqrt(hidden_size))\n",
    "        # Bias\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input at current timestep (batch, input_size)\n",
    "            h: Hidden state from previous timestep (batch, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            New hidden state (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        # h_t = tanh(W_ih @ x + W_hh @ h + b)\n",
    "        h_new = torch.tanh(\n",
    "            F.linear(x, self.W_ih) + F.linear(h, self.W_hh) + self.b\n",
    "        )\n",
    "        return h_new\n",
    "\n",
    "\n",
    "# Test the cell\n",
    "cell = VanillaRNNCell(input_size=10, hidden_size=20)\n",
    "x = torch.randn(4, 10)  # batch=4, input_size=10\n",
    "h = torch.zeros(4, 20)  # Initial hidden state\n",
    "\n",
    "h_new = cell(x, h)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Hidden state shape: {h_new.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Full RNN that processes sequences using VanillaRNNCell.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Stack of RNN cells\n",
    "        self.cells = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_dim = input_size if i == 0 else hidden_size\n",
    "            self.cells.append(VanillaRNNCell(input_dim, hidden_size))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, h0: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input sequence (seq_len, batch, input_size)\n",
    "            h0: Initial hidden states (num_layers, batch, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            output: Output at each timestep (seq_len, batch, hidden_size)\n",
    "            h_n: Final hidden states (num_layers, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = x.shape\n",
    "        \n",
    "        # Initialize hidden states\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
    "        \n",
    "        # Current hidden states for each layer\n",
    "        h = [h0[i] for i in range(self.num_layers)]\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        # Process each timestep\n",
    "        for t in range(seq_len):\n",
    "            inp = x[t]  # (batch, input_size)\n",
    "            \n",
    "            # Pass through each layer\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                h[i] = cell(inp, h[i])\n",
    "                inp = h[i]  # Output of this layer is input to next\n",
    "            \n",
    "            outputs.append(h[-1])  # Output from last layer\n",
    "        \n",
    "        # Stack outputs and hidden states\n",
    "        output = torch.stack(outputs, dim=0)  # (seq_len, batch, hidden_size)\n",
    "        h_n = torch.stack(h, dim=0)  # (num_layers, batch, hidden_size)\n",
    "        \n",
    "        return output, h_n\n",
    "\n",
    "\n",
    "# Test\n",
    "rnn = VanillaRNN(input_size=10, hidden_size=20, num_layers=2)\n",
    "x = torch.randn(15, 4, 10)  # seq_len=15, batch=4, input_size=10\n",
    "\n",
    "output, h_n = rnn(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Final hidden shape: {h_n.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PyTorch's nn.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch's built-in RNN\n",
    "pytorch_rnn = nn.RNN(\n",
    "    input_size=10,\n",
    "    hidden_size=20,\n",
    "    num_layers=2,\n",
    "    batch_first=False,  # Input is (seq_len, batch, input_size)\n",
    "    dropout=0.0,\n",
    "    bidirectional=False\n",
    ")\n",
    "\n",
    "x = torch.randn(15, 4, 10)\n",
    "output, h_n = pytorch_rnn(x)\n",
    "\n",
    "print(f\"PyTorch RNN output shape: {output.shape}\")\n",
    "print(f\"PyTorch RNN hidden shape: {h_n.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_first=True changes dimension order\n",
    "rnn_batch_first = nn.RNN(\n",
    "    input_size=10,\n",
    "    hidden_size=20,\n",
    "    num_layers=2,\n",
    "    batch_first=True  # Input is (batch, seq_len, input_size)\n",
    ")\n",
    "\n",
    "x = torch.randn(4, 15, 10)  # (batch, seq_len, input_size)\n",
    "output, h_n = rnn_batch_first(x)\n",
    "\n",
    "print(f\"batch_first=True:\")\n",
    "print(f\"  Input: {x.shape}\")\n",
    "print(f\"  Output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional RNN processes sequence in both directions\n",
    "bi_rnn = nn.RNN(\n",
    "    input_size=10,\n",
    "    hidden_size=20,\n",
    "    num_layers=2,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "x = torch.randn(15, 4, 10)\n",
    "output, h_n = bi_rnn(x)\n",
    "\n",
    "print(f\"Bidirectional RNN:\")\n",
    "print(f\"  Output shape: {output.shape}\")  # hidden_size * 2\n",
    "print(f\"  Hidden shape: {h_n.shape}\")  # num_layers * 2\n",
    "\n",
    "# Output contains [forward; backward] concatenated\n",
    "# Hidden contains alternating [forward_layer_0, backward_layer_0, forward_layer_1, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Vanishing Gradient Problem\n",
    "\n",
    "### 2.1 Understanding the Problem\n",
    "\n",
    "During backpropagation through time (BPTT), gradients are multiplied by W_hh at each timestep. If eigenvalues of W_hh are < 1, gradients vanish exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient_flow(seq_lengths: List[int] = [10, 50, 100, 200]):\n",
    "    \"\"\"\n",
    "    Visualize how gradients vanish with sequence length in vanilla RNN.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        # Create RNN\n",
    "        rnn = nn.RNN(input_size=32, hidden_size=64, num_layers=1)\n",
    "        \n",
    "        # Create input sequence\n",
    "        x = torch.randn(seq_len, 1, 32, requires_grad=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, _ = rnn(x)\n",
    "        \n",
    "        # Backprop from last output\n",
    "        loss = output[-1].sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check gradient at first timestep's input\n",
    "        grad_norm = x.grad[0].norm().item()\n",
    "        results[seq_len] = grad_norm\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar([str(k) for k in results.keys()], results.values())\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Gradient Norm at First Timestep')\n",
    "    plt.title('Vanishing Gradients in Vanilla RNN')\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    for seq_len, grad in results.items():\n",
    "        print(f\"Seq len {seq_len}: gradient norm = {grad:.2e}\")\n",
    "\n",
    "\n",
    "visualize_gradient_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematically: gradient involves product of Jacobians\n",
    "# dL/dh_0 = dL/dh_T * (dh_T/dh_{T-1}) * ... * (dh_1/dh_0)\n",
    "#         = dL/dh_T * W_hh^T * diag(1-h_{T-1}^2) * ... * W_hh^T * diag(1-h_0^2)\n",
    "\n",
    "def analyze_weight_matrix_eigenvalues():\n",
    "    \"\"\"Show how eigenvalues affect gradient flow\"\"\"\n",
    "    hidden_size = 64\n",
    "    \n",
    "    # Random initialization\n",
    "    W_random = torch.randn(hidden_size, hidden_size) / math.sqrt(hidden_size)\n",
    "    \n",
    "    # Orthogonal initialization (eigenvalues on unit circle)\n",
    "    W_ortho = torch.nn.init.orthogonal_(torch.empty(hidden_size, hidden_size))\n",
    "    \n",
    "    # Compute eigenvalues\n",
    "    eig_random = torch.linalg.eigvals(W_random).abs()\n",
    "    eig_ortho = torch.linalg.eigvals(W_ortho).abs()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].hist(eig_random.numpy(), bins=30, alpha=0.7)\n",
    "    axes[0].axvline(x=1.0, color='r', linestyle='--', label='|λ|=1')\n",
    "    axes[0].set_xlabel('|Eigenvalue|')\n",
    "    axes[0].set_title(f'Random Init (mean |λ|={eig_random.mean():.3f})')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].hist(eig_ortho.numpy(), bins=30, alpha=0.7)\n",
    "    axes[1].axvline(x=1.0, color='r', linestyle='--', label='|λ|=1')\n",
    "    axes[1].set_xlabel('|Eigenvalue|')\n",
    "    axes[1].set_title(f'Orthogonal Init (mean |λ|={eig_ortho.mean():.3f})')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"If |λ| < 1: gradients vanish exponentially\")\n",
    "    print(\"If |λ| > 1: gradients explode exponentially\")\n",
    "    print(\"If |λ| ≈ 1: gradients flow well (orthogonal init helps!)\")\n",
    "\n",
    "\n",
    "analyze_weight_matrix_eigenvalues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_gradient_clipping(model, x, y, max_norm: float = 1.0):\n",
    "    \"\"\"\n",
    "    Demonstrate gradient clipping to prevent exploding gradients.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Forward\n",
    "    output, _ = model(x)\n",
    "    loss = criterion(output[-1], y)\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check gradient norm before clipping\n",
    "    total_norm_before = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm_before += p.grad.norm().item() ** 2\n",
    "    total_norm_before = total_norm_before ** 0.5\n",
    "    \n",
    "    # Clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "    \n",
    "    # Check gradient norm after clipping\n",
    "    total_norm_after = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm_after += p.grad.norm().item() ** 2\n",
    "    total_norm_after = total_norm_after ** 0.5\n",
    "    \n",
    "    print(f\"Gradient norm before clipping: {total_norm_before:.4f}\")\n",
    "    print(f\"Gradient norm after clipping: {total_norm_after:.4f}\")\n",
    "    print(f\"Max allowed norm: {max_norm}\")\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Test\n",
    "rnn = nn.RNN(32, 64, num_layers=3)\n",
    "x = torch.randn(50, 8, 32)\n",
    "y = torch.randn(8, 64)\n",
    "\n",
    "train_with_gradient_clipping(rnn, x, y, max_norm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. LSTM: Long Short-Term Memory\n",
    "\n",
    "### 3.1 LSTM Architecture\n",
    "\n",
    "LSTM solves vanishing gradients with:\n",
    "- **Cell state** (c_t): Highway for gradient flow\n",
    "- **Gates**: Control information flow\n",
    "  - Forget gate: What to remove from cell state\n",
    "  - Input gate: What new info to add\n",
    "  - Output gate: What to output from cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM cell implementation from scratch.\n",
    "    \n",
    "    Gates:\n",
    "        f_t = σ(W_f @ [h_{t-1}, x_t] + b_f)  # Forget gate\n",
    "        i_t = σ(W_i @ [h_{t-1}, x_t] + b_i)  # Input gate\n",
    "        o_t = σ(W_o @ [h_{t-1}, x_t] + b_o)  # Output gate\n",
    "        g_t = tanh(W_g @ [h_{t-1}, x_t] + b_g)  # Candidate cell state\n",
    "    \n",
    "    State updates:\n",
    "        c_t = f_t * c_{t-1} + i_t * g_t  # New cell state\n",
    "        h_t = o_t * tanh(c_t)  # New hidden state\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Combined weights for efficiency: [i, f, g, o]\n",
    "        self.W_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size) / math.sqrt(input_size))\n",
    "        self.W_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size) / math.sqrt(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(4 * hidden_size))\n",
    "        \n",
    "        # Initialize forget gate bias to 1 (helps with long-term dependencies)\n",
    "        with torch.no_grad():\n",
    "            self.bias[hidden_size:2*hidden_size].fill_(1.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (batch, input_size)\n",
    "            state: Tuple of (h, c), each (batch, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            h_new: New hidden state\n",
    "            (h_new, c_new): New state tuple\n",
    "        \"\"\"\n",
    "        h, c = state\n",
    "        \n",
    "        # Compute all gates at once\n",
    "        gates = F.linear(x, self.W_ih) + F.linear(h, self.W_hh) + self.bias\n",
    "        \n",
    "        # Split into individual gates\n",
    "        i, f, g, o = gates.chunk(4, dim=1)\n",
    "        \n",
    "        # Apply activations\n",
    "        i = torch.sigmoid(i)  # Input gate\n",
    "        f = torch.sigmoid(f)  # Forget gate\n",
    "        g = torch.tanh(g)     # Candidate cell state\n",
    "        o = torch.sigmoid(o)  # Output gate\n",
    "        \n",
    "        # Update cell state\n",
    "        c_new = f * c + i * g\n",
    "        \n",
    "        # Update hidden state\n",
    "        h_new = o * torch.tanh(c_new)\n",
    "        \n",
    "        return h_new, (h_new, c_new)\n",
    "\n",
    "\n",
    "# Test\n",
    "lstm_cell = LSTMCell(input_size=10, hidden_size=20)\n",
    "x = torch.randn(4, 10)\n",
    "h = torch.zeros(4, 20)\n",
    "c = torch.zeros(4, 20)\n",
    "\n",
    "h_new, (h_out, c_out) = lstm_cell(x, (h, c))\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Hidden: {h_new.shape}\")\n",
    "print(f\"Cell: {c_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gate activations\n",
    "\n",
    "def visualize_lstm_gates():\n",
    "    \"\"\"Show how LSTM gates control information flow\"\"\"\n",
    "    lstm = nn.LSTM(input_size=32, hidden_size=64, batch_first=True)\n",
    "    \n",
    "    # Create input with a pattern: high values at start and end\n",
    "    seq_len = 50\n",
    "    x = torch.zeros(1, seq_len, 32)\n",
    "    x[0, :5, :] = torch.randn(5, 32)  # Information at start\n",
    "    x[0, -5:, :] = torch.randn(5, 32)  # Information at end\n",
    "    \n",
    "    # Hook to capture gate activations\n",
    "    gate_history = {'i': [], 'f': [], 'o': []}\n",
    "    \n",
    "    # Manual forward to capture gates\n",
    "    h = torch.zeros(1, 1, 64)\n",
    "    c = torch.zeros(1, 1, 64)\n",
    "    \n",
    "    for t in range(seq_len):\n",
    "        x_t = x[:, t:t+1, :]\n",
    "        _, (h, c) = lstm(x_t, (h, c))\n",
    "        \n",
    "        # Approximate gate values from weight analysis\n",
    "        # (In practice, you'd use hooks or modify the LSTM)\n",
    "        gate_history['i'].append(h.abs().mean().item())\n",
    "        gate_history['f'].append(c.abs().mean().item())\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].plot(x[0, :, 0].numpy(), label='Input (first dim)')\n",
    "    axes[0].set_xlabel('Timestep')\n",
    "    axes[0].set_ylabel('Input Value')\n",
    "    axes[0].set_title('Input Signal')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(gate_history['i'], label='Hidden state magnitude')\n",
    "    axes[1].plot(gate_history['f'], label='Cell state magnitude')\n",
    "    axes[1].set_xlabel('Timestep')\n",
    "    axes[1].set_ylabel('Magnitude')\n",
    "    axes[1].set_title('LSTM State Evolution')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_lstm_gates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LSTM vs RNN Gradient Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gradient_flow(seq_lengths: List[int] = [10, 50, 100, 200]):\n",
    "    \"\"\"Compare gradient flow in RNN vs LSTM\"\"\"\n",
    "    results = {'RNN': {}, 'LSTM': {}}\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        for model_type in ['RNN', 'LSTM']:\n",
    "            # Create model\n",
    "            if model_type == 'RNN':\n",
    "                model = nn.RNN(32, 64, num_layers=1)\n",
    "            else:\n",
    "                model = nn.LSTM(32, 64, num_layers=1)\n",
    "            \n",
    "            # Input\n",
    "            x = torch.randn(seq_len, 1, 32, requires_grad=True)\n",
    "            \n",
    "            # Forward\n",
    "            output, _ = model(x)\n",
    "            \n",
    "            # Backward from last output\n",
    "            loss = output[-1].sum()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient at first timestep\n",
    "            grad_norm = x.grad[0].norm().item()\n",
    "            results[model_type][seq_len] = grad_norm\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    x_pos = np.arange(len(seq_lengths))\n",
    "    width = 0.35\n",
    "    \n",
    "    rnn_vals = [results['RNN'][s] for s in seq_lengths]\n",
    "    lstm_vals = [results['LSTM'][s] for s in seq_lengths]\n",
    "    \n",
    "    ax.bar(x_pos - width/2, rnn_vals, width, label='RNN', color='red', alpha=0.7)\n",
    "    ax.bar(x_pos + width/2, lstm_vals, width, label='LSTM', color='blue', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Sequence Length')\n",
    "    ax.set_ylabel('Gradient Norm (log scale)')\n",
    "    ax.set_title('Gradient Flow: RNN vs LSTM')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(seq_lengths)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nLSTM maintains much better gradient flow for long sequences!\")\n",
    "\n",
    "\n",
    "compare_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 PyTorch's nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch LSTM\n",
    "lstm = nn.LSTM(\n",
    "    input_size=10,\n",
    "    hidden_size=20,\n",
    "    num_layers=2,\n",
    "    batch_first=True,\n",
    "    dropout=0.1,  # Dropout between layers (not on last layer)\n",
    "    bidirectional=False\n",
    ")\n",
    "\n",
    "x = torch.randn(4, 15, 10)  # (batch, seq_len, input_size)\n",
    "output, (h_n, c_n) = lstm(x)\n",
    "\n",
    "print(f\"LSTM output shape: {output.shape}\")\n",
    "print(f\"Final hidden shape: {h_n.shape}\")\n",
    "print(f\"Final cell shape: {c_n.shape}\")\n",
    "\n",
    "# Note: LSTM returns (output, (h_n, c_n)) unlike RNN which returns (output, h_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GRU: Gated Recurrent Unit\n",
    "\n",
    "GRU is a simpler alternative to LSTM with fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU cell implementation.\n",
    "    \n",
    "    Gates:\n",
    "        r_t = σ(W_r @ [h_{t-1}, x_t])  # Reset gate\n",
    "        z_t = σ(W_z @ [h_{t-1}, x_t])  # Update gate\n",
    "        h̃_t = tanh(W_h @ [r_t * h_{t-1}, x_t])  # Candidate hidden state\n",
    "    \n",
    "    Update:\n",
    "        h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Combined weights: [r, z, h]\n",
    "        self.W_ih = nn.Parameter(torch.randn(3 * hidden_size, input_size) / math.sqrt(input_size))\n",
    "        self.W_hh = nn.Parameter(torch.randn(3 * hidden_size, hidden_size) / math.sqrt(hidden_size))\n",
    "        self.bias_ih = nn.Parameter(torch.zeros(3 * hidden_size))\n",
    "        self.bias_hh = nn.Parameter(torch.zeros(3 * hidden_size))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (batch, input_size)\n",
    "            h: Hidden state (batch, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            h_new: New hidden state\n",
    "        \"\"\"\n",
    "        # Compute gates\n",
    "        gi = F.linear(x, self.W_ih, self.bias_ih)\n",
    "        gh = F.linear(h, self.W_hh, self.bias_hh)\n",
    "        \n",
    "        i_r, i_z, i_n = gi.chunk(3, dim=1)\n",
    "        h_r, h_z, h_n = gh.chunk(3, dim=1)\n",
    "        \n",
    "        r = torch.sigmoid(i_r + h_r)  # Reset gate\n",
    "        z = torch.sigmoid(i_z + h_z)  # Update gate\n",
    "        n = torch.tanh(i_n + r * h_n)  # Candidate hidden state\n",
    "        \n",
    "        # New hidden state\n",
    "        h_new = (1 - z) * h + z * n\n",
    "        \n",
    "        return h_new\n",
    "\n",
    "\n",
    "# Test\n",
    "gru_cell = GRUCell(10, 20)\n",
    "x = torch.randn(4, 10)\n",
    "h = torch.zeros(4, 20)\n",
    "\n",
    "h_new = gru_cell(x, h)\n",
    "print(f\"GRU output shape: {h_new.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare parameter counts\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "input_size, hidden_size = 128, 256\n",
    "\n",
    "rnn = nn.RNN(input_size, hidden_size)\n",
    "lstm = nn.LSTM(input_size, hidden_size)\n",
    "gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "print(f\"Parameter counts (input={input_size}, hidden={hidden_size}):\")\n",
    "print(f\"  RNN:  {count_parameters(rnn):,}\")\n",
    "print(f\"  GRU:  {count_parameters(gru):,} ({count_parameters(gru)/count_parameters(rnn):.1f}x RNN)\")\n",
    "print(f\"  LSTM: {count_parameters(lstm):,} ({count_parameters(lstm)/count_parameters(rnn):.1f}x RNN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Practical Sequence Modeling\n",
    "\n",
    "### 5.1 Handling Variable-Length Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable-length sequences need padding and packing\n",
    "\n",
    "# Example sequences of different lengths\n",
    "sequences = [\n",
    "    torch.randn(10, 32),  # Length 10\n",
    "    torch.randn(7, 32),   # Length 7\n",
    "    torch.randn(15, 32),  # Length 15\n",
    "    torch.randn(5, 32),   # Length 5\n",
    "]\n",
    "\n",
    "lengths = torch.tensor([10, 7, 15, 5])\n",
    "\n",
    "# Pad sequences to same length\n",
    "padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "print(f\"Padded shape: {padded.shape}\")  # (batch, max_len, features)\n",
    "\n",
    "# Sort by length (required for pack_padded_sequence)\n",
    "lengths_sorted, sort_idx = lengths.sort(descending=True)\n",
    "padded_sorted = padded[sort_idx]\n",
    "\n",
    "# Pack for efficient RNN processing\n",
    "packed = pack_padded_sequence(padded_sorted, lengths_sorted.cpu(), batch_first=True)\n",
    "print(f\"\\nPacked data shape: {packed.data.shape}\")\n",
    "print(f\"Packed batch_sizes: {packed.batch_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using packed sequences with LSTM\n",
    "\n",
    "lstm = nn.LSTM(32, 64, batch_first=True)\n",
    "\n",
    "# Forward with packed input\n",
    "packed_output, (h_n, c_n) = lstm(packed)\n",
    "\n",
    "# Unpack output\n",
    "output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "print(f\"Unpacked output shape: {output.shape}\")\n",
    "print(f\"Output lengths: {output_lengths}\")\n",
    "\n",
    "# Unsort to original order\n",
    "_, unsort_idx = sort_idx.sort()\n",
    "output = output[unsort_idx]\n",
    "h_n = h_n[:, unsort_idx]\n",
    "c_n = c_n[:, unsort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class for handling variable-length sequences\n",
    "\n",
    "class SequenceEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM encoder that properly handles variable-length sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1, \n",
    "                 bidirectional: bool = False, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True, bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.bidirectional = bidirectional\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Padded sequences (batch, max_len, input_size)\n",
    "            lengths: Actual lengths (batch,)\n",
    "        \n",
    "        Returns:\n",
    "            output: LSTM output at each timestep\n",
    "            final: Final hidden state for each sequence\n",
    "        \"\"\"\n",
    "        # Sort by length\n",
    "        lengths_sorted, sort_idx = lengths.sort(descending=True)\n",
    "        x_sorted = x[sort_idx]\n",
    "        \n",
    "        # Pack\n",
    "        packed = pack_padded_sequence(x_sorted, lengths_sorted.cpu(), batch_first=True)\n",
    "        \n",
    "        # LSTM\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        # Unpack\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Unsort\n",
    "        _, unsort_idx = sort_idx.sort()\n",
    "        output = output[unsort_idx]\n",
    "        h_n = h_n[:, unsort_idx]\n",
    "        \n",
    "        # Get final hidden state (last layer)\n",
    "        if self.bidirectional:\n",
    "            # Concatenate forward and backward\n",
    "            final = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        else:\n",
    "            final = h_n[-1]\n",
    "        \n",
    "        return output, final\n",
    "\n",
    "\n",
    "# Test\n",
    "encoder = SequenceEncoder(32, 64, num_layers=2, bidirectional=True)\n",
    "x = torch.randn(4, 20, 32)  # (batch, max_len, input_size)\n",
    "lengths = torch.tensor([20, 15, 10, 5])\n",
    "\n",
    "output, final = encoder(x, lengths)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Final hidden shape: {final.shape}\")  # bidirectional: 2*hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based sequence classifier.\n",
    "    \n",
    "    Uses the final hidden state for classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_size: int, \n",
    "                 num_classes: int, num_layers: int = 2, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = SequenceEncoder(\n",
    "            embedding_dim, hidden_size, num_layers,\n",
    "            bidirectional=True, dropout=dropout\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Token indices (batch, max_len)\n",
    "            lengths: Actual lengths (batch,)\n",
    "        \n",
    "        Returns:\n",
    "            Class logits (batch, num_classes)\n",
    "        \"\"\"\n",
    "        # Embed\n",
    "        embedded = self.embedding(x)  # (batch, max_len, embedding_dim)\n",
    "        \n",
    "        # Encode\n",
    "        _, final = self.encoder(embedded, lengths)\n",
    "        \n",
    "        # Classify\n",
    "        out = self.dropout(final)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Test\n",
    "classifier = SequenceClassifier(\n",
    "    vocab_size=10000,\n",
    "    embedding_dim=128,\n",
    "    hidden_size=256,\n",
    "    num_classes=5,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "x = torch.randint(0, 10000, (8, 50))  # (batch, max_len)\n",
    "lengths = torch.randint(10, 50, (8,))\n",
    "\n",
    "logits = classifier(x, lengths)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in classifier.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Language Model.\n",
    "    \n",
    "    Predicts next token given previous tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_size: int,\n",
    "                 num_layers: int = 2, dropout: float = 0.5, tie_weights: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers,\n",
    "            batch_first=True, dropout=dropout\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Weight tying: share embedding and output weights\n",
    "        if tie_weights and embedding_dim == hidden_size:\n",
    "            self.fc.weight = self.embedding.weight\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, hidden: Optional[Tuple] = None) -> Tuple[torch.Tensor, Tuple]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Token indices (batch, seq_len)\n",
    "            hidden: Optional initial hidden state\n",
    "        \n",
    "        Returns:\n",
    "            logits: Next token predictions (batch, seq_len, vocab_size)\n",
    "            hidden: Final hidden state\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.dropout(output)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        h = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        c = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        return (h, c)\n",
    "\n",
    "\n",
    "# Test\n",
    "lm = LanguageModel(\n",
    "    vocab_size=10000,\n",
    "    embedding_dim=256,\n",
    "    hidden_size=256,  # Same as embedding for weight tying\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "x = torch.randint(0, 10000, (4, 35))  # (batch, seq_len)\n",
    "logits, hidden = lm(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"Hidden shapes: h={hidden[0].shape}, c={hidden[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation with language model\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, start_tokens: torch.Tensor, max_length: int = 50,\n",
    "                  temperature: float = 1.0, device: torch.device = device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate text autoregressively.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        start_tokens: Initial tokens (1, start_len)\n",
    "        max_length: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "    \n",
    "    Returns:\n",
    "        Generated token sequence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    generated = start_tokens.to(device)\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    \n",
    "    # Process start tokens\n",
    "    logits, hidden = model(generated, hidden)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get last token's prediction\n",
    "        last_logits = logits[0, -1, :] / temperature\n",
    "        probs = F.softmax(last_logits, dim=-1)\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = torch.multinomial(probs, 1).unsqueeze(0)\n",
    "        \n",
    "        # Append to sequence\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        \n",
    "        # Forward pass for next prediction\n",
    "        logits, hidden = model(next_token, hidden)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "# Test generation (with random model)\n",
    "start = torch.randint(0, 10000, (1, 5))\n",
    "generated = generate_text(lm, start, max_length=20, temperature=0.8)\n",
    "print(f\"Start tokens: {start.shape}\")\n",
    "print(f\"Generated: {generated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Character-Level Language Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character-level dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"Character-level text dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, text: str, seq_length: int = 100):\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.chars = sorted(set(text))\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: c for i, c in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # Encode text\n",
    "        self.data = torch.tensor([self.char_to_idx[c] for c in text], dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + 1:idx + self.seq_length + 1]\n",
    "        return x, y\n",
    "    \n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        return ''.join([self.idx_to_char[i.item()] for i in indices])\n",
    "\n",
    "\n",
    "# Create a simple dataset\n",
    "sample_text = \"\"\"The quick brown fox jumps over the lazy dog. \n",
    "A wizard's job is to vex chumps quickly in fog. \n",
    "Pack my box with five dozen liquor jugs.\n",
    "\"\"\" * 100  # Repeat for more data\n",
    "\n",
    "dataset = CharDataset(sample_text, seq_length=50)\n",
    "print(f\"Vocabulary size: {dataset.vocab_size}\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Characters: {dataset.chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train character-level model\n",
    "\n",
    "char_model = LanguageModel(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    "    tie_weights=False\n",
    ").to(device)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "optimizer = torch.optim.Adam(char_model.parameters(), lr=0.002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "char_model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = char_model(x)\n",
    "        loss = criterion(logits.view(-1, dataset.vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(char_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Perplexity={perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with trained model\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_chars(model, dataset, start_text: str, length: int = 200, temperature: float = 0.8):\n",
    "    \"\"\"Generate characters from a starting string\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode start text\n",
    "    chars = [dataset.char_to_idx.get(c, 0) for c in start_text]\n",
    "    x = torch.tensor(chars, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    hidden = model.init_hidden(1, device)\n",
    "    generated = start_text\n",
    "    \n",
    "    # Process initial context\n",
    "    logits, hidden = model(x, hidden)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # Sample from last position\n",
    "        probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1)\n",
    "        \n",
    "        # Decode and append\n",
    "        next_char = dataset.idx_to_char[next_idx.item()]\n",
    "        generated += next_char\n",
    "        \n",
    "        # Forward pass\n",
    "        x = next_idx.unsqueeze(0)\n",
    "        logits, hidden = model(x, hidden)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "# Generate some text\n",
    "print(\"Generated text:\")\n",
    "print(\"-\" * 50)\n",
    "generated = generate_chars(char_model, dataset, \"The quick \", length=200, temperature=0.7)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Sequence-to-Sequence\n",
    "\n",
    "Build an encoder-decoder model for sequence-to-sequence tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement Seq2Seq model\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence-to-Sequence model with LSTM encoder and decoder.\n",
    "    \n",
    "    Components:\n",
    "    - Encoder: Processes input sequence, produces context\n",
    "    - Decoder: Generates output sequence from context\n",
    "    \n",
    "    Args:\n",
    "        src_vocab_size: Source vocabulary size\n",
    "        tgt_vocab_size: Target vocabulary size\n",
    "        embedding_dim: Embedding dimension\n",
    "        hidden_size: LSTM hidden size\n",
    "        num_layers: Number of LSTM layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, \n",
    "                 embedding_dim: int, hidden_size: int, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: Create encoder embedding, encoder LSTM, decoder embedding, \n",
    "        # decoder LSTM, and output projection\n",
    "        pass\n",
    "    \n",
    "    def encode(self, src: torch.Tensor, src_lengths: torch.Tensor):\n",
    "        \"\"\"Encode source sequence\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def decode_step(self, tgt_token: torch.Tensor, hidden):\n",
    "        \"\"\"Single decoding step\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, src, src_lengths, tgt):\n",
    "        \"\"\"Full forward pass with teacher forcing\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Visualize Hidden State Dynamics\n",
    "\n",
    "Create visualizations showing how hidden states evolve over a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Hidden state visualization\n",
    "\n",
    "def visualize_hidden_dynamics(model, sequence: torch.Tensor, title: str = \"Hidden State Dynamics\"):\n",
    "    \"\"\"\n",
    "    Visualize how LSTM hidden states change over a sequence.\n",
    "    \n",
    "    Create visualizations showing:\n",
    "    1. Hidden state norms over time\n",
    "    2. Cell state norms over time  \n",
    "    3. PCA of hidden states\n",
    "    \n",
    "    Args:\n",
    "        model: LSTM model\n",
    "        sequence: Input sequence (1, seq_len, input_size)\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Run through sequence step by step, collect hidden states,\n",
    "    # then visualize using matplotlib\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Peephole LSTM\n",
    "\n",
    "Add peephole connections that let gates look at the cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Peephole LSTM\n",
    "\n",
    "class PeepholeLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM cell with peephole connections.\n",
    "    \n",
    "    In standard LSTM, gates only see h_{t-1} and x_t.\n",
    "    Peephole connections also let gates see c_{t-1} (or c_t for output gate).\n",
    "    \n",
    "    Modified gate equations:\n",
    "        f_t = σ(W_f @ [h_{t-1}, x_t] + W_cf * c_{t-1} + b_f)\n",
    "        i_t = σ(W_i @ [h_{t-1}, x_t] + W_ci * c_{t-1} + b_i)\n",
    "        o_t = σ(W_o @ [h_{t-1}, x_t] + W_co * c_t + b_o)  # Note: c_t, not c_{t-1}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: Add diagonal weight matrices W_cf, W_ci, W_co for peephole connections\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Seq2Seq model\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int,\n",
    "                 embedding_dim: int, hidden_size: int, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Encoder\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Decoder\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, embedding_dim)\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.output_proj = nn.Linear(hidden_size, tgt_vocab_size)\n",
    "    \n",
    "    def encode(self, src: torch.Tensor, src_lengths: torch.Tensor):\n",
    "        embedded = self.src_embedding(src)\n",
    "        \n",
    "        # Pack for variable lengths\n",
    "        lengths_sorted, sort_idx = src_lengths.sort(descending=True)\n",
    "        embedded_sorted = embedded[sort_idx]\n",
    "        packed = pack_padded_sequence(embedded_sorted, lengths_sorted.cpu(), batch_first=True)\n",
    "        \n",
    "        _, (h, c) = self.encoder(packed)\n",
    "        \n",
    "        # Unsort\n",
    "        _, unsort_idx = sort_idx.sort()\n",
    "        h = h[:, unsort_idx]\n",
    "        c = c[:, unsort_idx]\n",
    "        \n",
    "        return (h, c)\n",
    "    \n",
    "    def decode_step(self, tgt_token: torch.Tensor, hidden):\n",
    "        embedded = self.tgt_embedding(tgt_token)  # (batch, 1, embedding_dim)\n",
    "        output, hidden = self.decoder(embedded, hidden)\n",
    "        logits = self.output_proj(output)  # (batch, 1, vocab_size)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def forward(self, src, src_lengths, tgt):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        \n",
    "        # Encode\n",
    "        hidden = self.encode(src, src_lengths)\n",
    "        \n",
    "        # Decode with teacher forcing\n",
    "        outputs = []\n",
    "        for t in range(tgt_len):\n",
    "            tgt_token = tgt[:, t:t+1]\n",
    "            logits, hidden = self.decode_step(tgt_token, hidden)\n",
    "            outputs.append(logits)\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)  # (batch, tgt_len, vocab_size)\n",
    "\n",
    "\n",
    "# Test\n",
    "seq2seq = Seq2Seq(1000, 1000, 128, 256, num_layers=2)\n",
    "src = torch.randint(0, 1000, (4, 20))\n",
    "src_lengths = torch.tensor([20, 15, 12, 8])\n",
    "tgt = torch.randint(0, 1000, (4, 15))\n",
    "\n",
    "output = seq2seq(src, src_lengths, tgt)\n",
    "print(f\"Seq2Seq output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Hidden state visualization\n",
    "\n",
    "def visualize_hidden_dynamics(model, sequence: torch.Tensor, title: str = \"Hidden State Dynamics\"):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    sequence = sequence.to(device)\n",
    "    \n",
    "    seq_len = sequence.shape[1]\n",
    "    hidden_norms = []\n",
    "    cell_norms = []\n",
    "    hidden_states = []\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    h = torch.zeros(model.num_layers, 1, model.hidden_size, device=device)\n",
    "    c = torch.zeros(model.num_layers, 1, model.hidden_size, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t in range(seq_len):\n",
    "            x_t = sequence[:, t:t+1, :]\n",
    "            _, (h, c) = model.lstm(x_t, (h, c))\n",
    "            \n",
    "            hidden_norms.append(h[-1].norm().item())\n",
    "            cell_norms.append(c[-1].norm().item())\n",
    "            hidden_states.append(h[-1].squeeze().cpu().numpy())\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Plot norms\n",
    "    axes[0].plot(hidden_norms, label='Hidden state')\n",
    "    axes[0].plot(cell_norms, label='Cell state')\n",
    "    axes[0].set_xlabel('Timestep')\n",
    "    axes[0].set_ylabel('Norm')\n",
    "    axes[0].set_title('State Norms Over Time')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # PCA of hidden states\n",
    "    from sklearn.decomposition import PCA\n",
    "    hidden_array = np.array(hidden_states)\n",
    "    if hidden_array.shape[0] > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        hidden_2d = pca.fit_transform(hidden_array)\n",
    "        \n",
    "        scatter = axes[1].scatter(hidden_2d[:, 0], hidden_2d[:, 1], \n",
    "                                  c=range(len(hidden_2d)), cmap='viridis')\n",
    "        axes[1].plot(hidden_2d[:, 0], hidden_2d[:, 1], 'k-', alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=axes[1], label='Timestep')\n",
    "        axes[1].set_xlabel('PC1')\n",
    "        axes[1].set_ylabel('PC2')\n",
    "        axes[1].set_title('Hidden State Trajectory (PCA)')\n",
    "    \n",
    "    # Hidden state heatmap\n",
    "    im = axes[2].imshow(hidden_array.T, aspect='auto', cmap='RdBu')\n",
    "    plt.colorbar(im, ax=axes[2])\n",
    "    axes[2].set_xlabel('Timestep')\n",
    "    axes[2].set_ylabel('Hidden Dimension')\n",
    "    axes[2].set_title('Hidden State Values')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Test\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(32, 64, num_layers=2, batch_first=True)\n",
    "        self.hidden_size = 64\n",
    "        self.num_layers = 2\n",
    "\n",
    "simple_lstm = SimpleLSTM()\n",
    "test_seq = torch.randn(1, 50, 32)\n",
    "visualize_hidden_dynamics(simple_lstm, test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Peephole LSTM\n",
    "\n",
    "class PeepholeLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Standard LSTM weights\n",
    "        self.W_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size) / math.sqrt(input_size))\n",
    "        self.W_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size) / math.sqrt(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(4 * hidden_size))\n",
    "        \n",
    "        # Peephole weights (diagonal matrices, stored as vectors)\n",
    "        self.W_ci = nn.Parameter(torch.randn(hidden_size) * 0.1)  # Input gate peephole\n",
    "        self.W_cf = nn.Parameter(torch.randn(hidden_size) * 0.1)  # Forget gate peephole\n",
    "        self.W_co = nn.Parameter(torch.randn(hidden_size) * 0.1)  # Output gate peephole\n",
    "        \n",
    "        # Initialize forget gate bias\n",
    "        with torch.no_grad():\n",
    "            self.bias[hidden_size:2*hidden_size].fill_(1.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        h, c = state\n",
    "        \n",
    "        # Compute standard gates\n",
    "        gates = F.linear(x, self.W_ih) + F.linear(h, self.W_hh) + self.bias\n",
    "        i_gate, f_gate, g_gate, o_gate = gates.chunk(4, dim=1)\n",
    "        \n",
    "        # Add peephole connections for input and forget gates (using c_{t-1})\n",
    "        i = torch.sigmoid(i_gate + self.W_ci * c)\n",
    "        f = torch.sigmoid(f_gate + self.W_cf * c)\n",
    "        g = torch.tanh(g_gate)\n",
    "        \n",
    "        # Update cell state\n",
    "        c_new = f * c + i * g\n",
    "        \n",
    "        # Output gate peephole uses c_t (not c_{t-1})\n",
    "        o = torch.sigmoid(o_gate + self.W_co * c_new)\n",
    "        \n",
    "        # Hidden state\n",
    "        h_new = o * torch.tanh(c_new)\n",
    "        \n",
    "        return h_new, (h_new, c_new)\n",
    "\n",
    "\n",
    "# Test\n",
    "peephole_cell = PeepholeLSTMCell(32, 64)\n",
    "x = torch.randn(4, 32)\n",
    "h = torch.zeros(4, 64)\n",
    "c = torch.zeros(4, 64)\n",
    "\n",
    "h_new, (h_out, c_out) = peephole_cell(x, (h, c))\n",
    "print(f\"Peephole LSTM output shape: {h_new.shape}\")\n",
    "print(f\"Cell state shape: {c_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **RNN Basics**:\n",
    "   - Process sequences by maintaining hidden state\n",
    "   - Same weights applied at each timestep\n",
    "   - Can be stacked (multi-layer) and bidirectional\n",
    "\n",
    "2. **Vanishing Gradients**:\n",
    "   - Gradients shrink exponentially with sequence length\n",
    "   - Caused by repeated multiplication of weight matrix\n",
    "   - Gradient clipping helps with exploding gradients\n",
    "\n",
    "3. **LSTM**:\n",
    "   - Cell state provides highway for gradient flow\n",
    "   - Gates control information flow (forget, input, output)\n",
    "   - Much better at learning long-term dependencies\n",
    "\n",
    "4. **GRU**:\n",
    "   - Simplified version with fewer parameters\n",
    "   - Reset and update gates instead of LSTM's three gates\n",
    "   - Often comparable performance to LSTM\n",
    "\n",
    "5. **Practical Tips**:\n",
    "   - Use packed sequences for variable-length inputs\n",
    "   - Initialize forget gate bias to 1\n",
    "   - Use gradient clipping (max_norm ≈ 1-5)\n",
    "   - Consider bidirectional for classification tasks\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Model | Use Case | Parameters |\n",
    "|-------|----------|------------|\n",
    "| Vanilla RNN | Short sequences, simple patterns | Fewest |\n",
    "| LSTM | Long sequences, complex dependencies | Most |\n",
    "| GRU | Good balance of performance/efficiency | Middle |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
