{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causality Fundamentals: From Correlation to Causation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Correlation vs Causation**\n",
    "   - Why correlation doesn't imply causation\n",
    "   - Simpson's paradox and confounding\n",
    "   - The fundamental problem of causal inference\n",
    "\n",
    "2. **Causal Graphs and DAGs**\n",
    "   - Directed Acyclic Graphs (DAGs) for causality\n",
    "   - d-separation and conditional independence\n",
    "   - Identifying confounders, mediators, and colliders\n",
    "\n",
    "3. **Causal Inference Methods**\n",
    "   - Interventions and the do-operator\n",
    "   - Backdoor and frontdoor adjustment\n",
    "   - Propensity score matching\n",
    "   - Instrumental variables\n",
    "\n",
    "4. **Neural Networks for Causal Inference**\n",
    "   - Representation learning for causal discovery\n",
    "   - Treatment effect estimation with neural networks\n",
    "   - Counterfactual reasoning\n",
    "   - Causal regularization\n",
    "\n",
    "**Prerequisites**: Basic probability, PyTorch fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from typing import Callable, List, Tuple, Dict\n",
    "import networkx as nx\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Correlation vs Causation\n",
    "\n",
    "### 1.1 The Fundamental Problem\n",
    "\n",
    "**Correlation**: Two variables X and Y are correlated if they tend to vary together.\n",
    "- Measured by correlation coefficient: $\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}$\n",
    "\n",
    "**Causation**: X causes Y if changing X leads to a change in Y.\n",
    "- Requires intervention: what happens if we force X to a particular value?\n",
    "\n",
    "**Why they differ**: Correlation can arise from:\n",
    "1. **X causes Y** (what we want)\n",
    "2. **Y causes X** (reverse causation)\n",
    "3. **Z causes both X and Y** (confounding)\n",
    "4. **Pure coincidence** (spurious correlation)\n",
    "\n",
    "### 1.2 Classic Examples of Spurious Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Ice cream sales and drowning deaths\n",
    "# Hidden confounder: Temperature/Season\n",
    "\n",
    "def generate_spurious_correlation_data(n_samples: int = 365) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate data where X and Y are correlated but not causally related.\"\"\"\n",
    "    # Days of the year\n",
    "    days = np.arange(n_samples)\n",
    "    \n",
    "    # Temperature (confounder) - sinusoidal pattern\n",
    "    temperature = 20 + 15 * np.sin(2 * np.pi * days / 365) + np.random.randn(n_samples) * 2\n",
    "    \n",
    "    # Ice cream sales (caused by temperature)\n",
    "    ice_cream = 100 + 3 * temperature + np.random.randn(n_samples) * 10\n",
    "    ice_cream = np.maximum(ice_cream, 0)  # Can't be negative\n",
    "    \n",
    "    # Drowning deaths (also caused by temperature)\n",
    "    drownings = 2 + 0.3 * temperature + np.random.randn(n_samples) * 1\n",
    "    drownings = np.maximum(drownings, 0)\n",
    "    \n",
    "    return ice_cream, drownings, temperature\n",
    "\n",
    "ice_cream, drownings, temperature = generate_spurious_correlation_data()\n",
    "\n",
    "# Compute correlations\n",
    "corr_ice_drowning = np.corrcoef(ice_cream, drownings)[0, 1]\n",
    "corr_temp_ice = np.corrcoef(temperature, ice_cream)[0, 1]\n",
    "corr_temp_drowning = np.corrcoef(temperature, drownings)[0, 1]\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Ice cream vs drownings\n",
    "axes[0].scatter(ice_cream, drownings, alpha=0.5)\n",
    "axes[0].set_xlabel('Ice Cream Sales')\n",
    "axes[0].set_ylabel('Drowning Deaths')\n",
    "axes[0].set_title(f'Spurious Correlation\\nρ = {corr_ice_drowning:.3f}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Temperature vs ice cream\n",
    "axes[1].scatter(temperature, ice_cream, alpha=0.5, color='orange')\n",
    "axes[1].set_xlabel('Temperature (°C)')\n",
    "axes[1].set_ylabel('Ice Cream Sales')\n",
    "axes[1].set_title(f'True Causal Effect\\nρ = {corr_temp_ice:.3f}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Temperature vs drownings\n",
    "axes[2].scatter(temperature, drownings, alpha=0.5, color='red')\n",
    "axes[2].set_xlabel('Temperature (°C)')\n",
    "axes[2].set_ylabel('Drowning Deaths')\n",
    "axes[2].set_title(f'True Causal Effect\\nρ = {corr_temp_drowning:.3f}')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Ice cream and drownings are correlated, but neither causes the other!\")\n",
    "print(f\"The confounder (temperature) causes both, creating a spurious correlation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Simpson's Paradox\n",
    "\n",
    "**Simpson's Paradox**: A trend that appears in different groups of data disappears or reverses when the groups are combined.\n",
    "\n",
    "**Classic example**: Treatment effectiveness\n",
    "- Overall: Treatment seems harmful\n",
    "- Within each severity group: Treatment is beneficial\n",
    "- Paradox: The overall conclusion is misleading!\n",
    "\n",
    "**Why it happens**: Confounding by severity (sicker patients more likely to get treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simpsons_paradox_data(n_samples: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"Generate data demonstrating Simpson's paradox.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Group 1: Mild cases\n",
    "    # Most don't get treatment (90%), high recovery rate overall\n",
    "    n_mild = n_samples // 2\n",
    "    n_mild_treated = int(n_mild * 0.1)\n",
    "    n_mild_control = n_mild - n_mild_treated\n",
    "    \n",
    "    # Mild + treated: 95% recovery\n",
    "    data.extend([{'severity': 'mild', 'treatment': 1, 'recovery': 1}] * int(n_mild_treated * 0.95))\n",
    "    data.extend([{'severity': 'mild', 'treatment': 1, 'recovery': 0}] * int(n_mild_treated * 0.05))\n",
    "    \n",
    "    # Mild + control: 90% recovery\n",
    "    data.extend([{'severity': 'mild', 'treatment': 0, 'recovery': 1}] * int(n_mild_control * 0.90))\n",
    "    data.extend([{'severity': 'mild', 'treatment': 0, 'recovery': 0}] * int(n_mild_control * 0.10))\n",
    "    \n",
    "    # Group 2: Severe cases\n",
    "    # Most get treatment (90%), low recovery rate overall\n",
    "    n_severe = n_samples // 2\n",
    "    n_severe_treated = int(n_severe * 0.9)\n",
    "    n_severe_control = n_severe - n_severe_treated\n",
    "    \n",
    "    # Severe + treated: 50% recovery\n",
    "    data.extend([{'severity': 'severe', 'treatment': 1, 'recovery': 1}] * int(n_severe_treated * 0.50))\n",
    "    data.extend([{'severity': 'severe', 'treatment': 1, 'recovery': 0}] * int(n_severe_treated * 0.50))\n",
    "    \n",
    "    # Severe + control: 40% recovery\n",
    "    data.extend([{'severity': 'severe', 'treatment': 0, 'recovery': 1}] * int(n_severe_control * 0.40))\n",
    "    data.extend([{'severity': 'severe', 'treatment': 0, 'recovery': 0}] * int(n_severe_control * 0.60))\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "import pandas as pd\n",
    "df = generate_simpsons_paradox_data(1000)\n",
    "\n",
    "# Overall statistics\n",
    "overall_treated = df[df['treatment'] == 1]['recovery'].mean()\n",
    "overall_control = df[df['treatment'] == 0]['recovery'].mean()\n",
    "\n",
    "# By severity\n",
    "mild_treated = df[(df['severity'] == 'mild') & (df['treatment'] == 1)]['recovery'].mean()\n",
    "mild_control = df[(df['severity'] == 'mild') & (df['treatment'] == 0)]['recovery'].mean()\n",
    "severe_treated = df[(df['severity'] == 'severe') & (df['treatment'] == 1)]['recovery'].mean()\n",
    "severe_control = df[(df['severity'] == 'severe') & (df['treatment'] == 0)]['recovery'].mean()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Overall (misleading)\n",
    "axes[0].bar(['Control', 'Treatment'], [overall_control, overall_treated], \n",
    "            color=['blue', 'red'], alpha=0.7)\n",
    "axes[0].set_ylabel('Recovery Rate')\n",
    "axes[0].set_title('Overall (Confounded)\\nTreatment appears WORSE!')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, (val, label) in enumerate(zip([overall_control, overall_treated], ['Control', 'Treatment'])):\n",
    "    axes[0].text(i, val + 0.02, f'{val:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Mild cases\n",
    "axes[1].bar(['Control', 'Treatment'], [mild_control, mild_treated], \n",
    "            color=['blue', 'red'], alpha=0.7)\n",
    "axes[1].set_ylabel('Recovery Rate')\n",
    "axes[1].set_title('Mild Cases Only\\nTreatment is BETTER')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, (val, label) in enumerate(zip([mild_control, mild_treated], ['Control', 'Treatment'])):\n",
    "    axes[1].text(i, val + 0.02, f'{val:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Severe cases\n",
    "axes[2].bar(['Control', 'Treatment'], [severe_control, severe_treated], \n",
    "            color=['blue', 'red'], alpha=0.7)\n",
    "axes[2].set_ylabel('Recovery Rate')\n",
    "axes[2].set_title('Severe Cases Only\\nTreatment is BETTER')\n",
    "axes[2].set_ylim([0, 1])\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "for i, (val, label) in enumerate(zip([severe_control, severe_treated], ['Control', 'Treatment'])):\n",
    "    axes[2].text(i, val + 0.02, f'{val:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Simpson's Paradox ===\")\n",
    "print(f\"Overall: Treatment {overall_treated:.1%} vs Control {overall_control:.1%} \"\n",
    "      f\"→ Treatment appears {abs(overall_treated - overall_control):.1%} WORSE\")\n",
    "print(f\"\\nBut within each group:\")\n",
    "print(f\"Mild: Treatment {mild_treated:.1%} vs Control {mild_control:.1%} \"\n",
    "      f\"→ Treatment is {mild_treated - mild_control:.1%} BETTER\")\n",
    "print(f\"Severe: Treatment {severe_treated:.1%} vs Control {severe_control:.1%} \"\n",
    "      f\"→ Treatment is {severe_treated - severe_control:.1%} BETTER\")\n",
    "print(f\"\\nConclusion: Must control for confounders (severity) to get the true effect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Causal Graphs and DAGs\n",
    "\n",
    "### 2.1 Directed Acyclic Graphs (DAGs)\n",
    "\n",
    "**DAG**: A graph with directed edges (arrows) and no cycles.\n",
    "- Nodes: Variables\n",
    "- Edges: Direct causal relationships\n",
    "- Arrow from X → Y means \"X directly causes Y\"\n",
    "\n",
    "**Key structures**:\n",
    "1. **Chain**: X → Z → Y (Z is a mediator)\n",
    "2. **Fork**: X ← Z → Y (Z is a confounder)\n",
    "3. **Collider**: X → Z ← Y (Z is a collider)\n",
    "\n",
    "### 2.2 d-Separation and Conditional Independence\n",
    "\n",
    "**d-separation** (directional separation): A criterion for determining conditional independence from a DAG.\n",
    "\n",
    "**Rules**: X and Y are d-separated given Z if all paths between X and Y are blocked:\n",
    "1. **Chain** (X → Z → Y): Blocked by conditioning on Z\n",
    "2. **Fork** (X ← Z → Y): Blocked by conditioning on Z\n",
    "3. **Collider** (X → Z ← Y): Blocked by NOT conditioning on Z (blocked by default!)\n",
    "\n",
    "**Why colliders are special**: Conditioning on a collider creates a spurious association!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dag(edges: List[Tuple[str, str]], title: str, node_colors: Dict[str, str] = None):\n",
    "    \"\"\"Plot a directed acyclic graph.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Default colors\n",
    "    if node_colors is None:\n",
    "        node_colors = {node: 'lightblue' for node in G.nodes()}\n",
    "    \n",
    "    colors = [node_colors.get(node, 'lightblue') for node in G.nodes()]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, pos, with_labels=True, node_color=colors, node_size=2000, \n",
    "            font_size=12, font_weight='bold', arrows=True, \n",
    "            arrowsize=20, edge_color='gray', width=2)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example 1: Chain (Mediator)\n",
    "print(\"=== Structure 1: Chain (Mediator) ===\")\n",
    "print(\"X → Z → Y\")\n",
    "print(\"Z is a mediator: X affects Y through Z\")\n",
    "print(\"Conditioning on Z blocks the path: X ⊥ Y | Z\")\n",
    "plot_dag([('X', 'Z'), ('Z', 'Y')], 'Chain: X → Z → Y (Z is mediator)')\n",
    "\n",
    "# Demonstrate with data\n",
    "n = 10000\n",
    "X = np.random.randn(n)\n",
    "Z = 2 * X + np.random.randn(n) * 0.5  # Z caused by X\n",
    "Y = 3 * Z + np.random.randn(n) * 0.5  # Y caused by Z\n",
    "\n",
    "print(f\"Correlation X-Y (unconditional): {np.corrcoef(X, Y)[0,1]:.3f} (significant)\")\n",
    "\n",
    "# Condition on Z by regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg_x = LinearRegression().fit(Z.reshape(-1, 1), X)\n",
    "X_residual = X - reg_x.predict(Z.reshape(-1, 1))\n",
    "reg_y = LinearRegression().fit(Z.reshape(-1, 1), Y)\n",
    "Y_residual = Y - reg_y.predict(Z.reshape(-1, 1))\n",
    "\n",
    "print(f\"Correlation X-Y (conditional on Z): {np.corrcoef(X_residual, Y_residual)[0,1]:.3f} (near zero!)\\n\")\n",
    "\n",
    "# Example 2: Fork (Confounder)\n",
    "print(\"\\n=== Structure 2: Fork (Confounder) ===\")\n",
    "print(\"X ← Z → Y\")\n",
    "print(\"Z is a confounder: Z causes both X and Y\")\n",
    "print(\"Conditioning on Z blocks the path: X ⊥ Y | Z\")\n",
    "plot_dag([('Z', 'X'), ('Z', 'Y')], 'Fork: X ← Z → Y (Z is confounder)', \n",
    "         node_colors={'Z': 'lightcoral', 'X': 'lightblue', 'Y': 'lightblue'})\n",
    "\n",
    "# Demonstrate\n",
    "Z = np.random.randn(n)\n",
    "X = 2 * Z + np.random.randn(n) * 0.5\n",
    "Y = 3 * Z + np.random.randn(n) * 0.5\n",
    "\n",
    "print(f\"Correlation X-Y (unconditional): {np.corrcoef(X, Y)[0,1]:.3f} (significant - spurious!)\")\n",
    "\n",
    "# Condition on Z\n",
    "reg_x = LinearRegression().fit(Z.reshape(-1, 1), X)\n",
    "X_residual = X - reg_x.predict(Z.reshape(-1, 1))\n",
    "reg_y = LinearRegression().fit(Z.reshape(-1, 1), Y)\n",
    "Y_residual = Y - reg_y.predict(Z.reshape(-1, 1))\n",
    "\n",
    "print(f\"Correlation X-Y (conditional on Z): {np.corrcoef(X_residual, Y_residual)[0,1]:.3f} (near zero!)\\n\")\n",
    "\n",
    "# Example 3: Collider\n",
    "print(\"\\n=== Structure 3: Collider ===\")\n",
    "print(\"X → Z ← Y\")\n",
    "print(\"Z is a collider: both X and Y cause Z\")\n",
    "print(\"CAREFUL: X and Y are independent, but conditioning on Z creates spurious correlation!\")\n",
    "plot_dag([('X', 'Z'), ('Y', 'Z')], 'Collider: X → Z ← Y (Z is collider)',\n",
    "         node_colors={'Z': 'yellow', 'X': 'lightblue', 'Y': 'lightblue'})\n",
    "\n",
    "# Demonstrate\n",
    "X = np.random.randn(n)\n",
    "Y = np.random.randn(n)  # X and Y are independent!\n",
    "Z = X + Y + np.random.randn(n) * 0.5  # Z is caused by both\n",
    "\n",
    "print(f\"Correlation X-Y (unconditional): {np.corrcoef(X, Y)[0,1]:.3f} (near zero - independent!)\")\n",
    "\n",
    "# Condition on Z\n",
    "reg_x = LinearRegression().fit(Z.reshape(-1, 1), X)\n",
    "X_residual = X - reg_x.predict(Z.reshape(-1, 1))\n",
    "reg_y = LinearRegression().fit(Z.reshape(-1, 1), Y)\n",
    "Y_residual = Y - reg_y.predict(Z.reshape(-1, 1))\n",
    "\n",
    "print(f\"Correlation X-Y (conditional on Z): {np.corrcoef(X_residual, Y_residual)[0,1]:.3f} (significant!)\")\n",
    "print(\"\\n⚠️ Conditioning on a collider CREATES spurious correlation (collider bias)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Real-World Example: Berkeley Admissions Paradox\n",
    "\n",
    "**Context**: Berkeley was sued in 1973 for gender bias in admissions.\n",
    "- Overall: Men had higher admission rate than women\n",
    "- Within each department: Women had equal or higher admission rates\n",
    "\n",
    "**Causal structure**:\n",
    "- Gender → Department choice\n",
    "- Department choice → Admission\n",
    "- Department is a **collider** (caused by gender) and a **confounder** (causes admission)\n",
    "\n",
    "**Lesson**: Need to understand causal structure to correctly adjust for confounding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Berkeley admissions data\n",
    "def simulate_berkeley_data(n_applicants: int = 10000):\n",
    "    data = []\n",
    "    \n",
    "    # Department A: Easy to get in, attracts more men\n",
    "    # 70% men apply, 80% admission rate for all\n",
    "    n_dept_a = n_applicants // 2\n",
    "    n_men_a = int(n_dept_a * 0.7)\n",
    "    n_women_a = n_dept_a - n_men_a\n",
    "    \n",
    "    data.extend([{'gender': 'M', 'dept': 'A', 'admitted': 1}] * int(n_men_a * 0.80))\n",
    "    data.extend([{'gender': 'M', 'dept': 'A', 'admitted': 0}] * int(n_men_a * 0.20))\n",
    "    data.extend([{'gender': 'F', 'dept': 'A', 'admitted': 1}] * int(n_women_a * 0.80))\n",
    "    data.extend([{'gender': 'F', 'dept': 'A', 'admitted': 0}] * int(n_women_a * 0.20))\n",
    "    \n",
    "    # Department B: Hard to get in, attracts more women\n",
    "    # 30% men apply, 30% admission rate for all\n",
    "    n_dept_b = n_applicants // 2\n",
    "    n_men_b = int(n_dept_b * 0.3)\n",
    "    n_women_b = n_dept_b - n_men_b\n",
    "    \n",
    "    data.extend([{'gender': 'M', 'dept': 'B', 'admitted': 1}] * int(n_men_b * 0.30))\n",
    "    data.extend([{'gender': 'M', 'dept': 'B', 'admitted': 0}] * int(n_men_b * 0.70))\n",
    "    data.extend([{'gender': 'F', 'dept': 'B', 'admitted': 1}] * int(n_women_b * 0.30))\n",
    "    data.extend([{'gender': 'F', 'dept': 'B', 'admitted': 0}] * int(n_women_b * 0.70))\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_berkeley = simulate_berkeley_data(10000)\n",
    "\n",
    "# Overall admission rates\n",
    "men_overall = df_berkeley[df_berkeley['gender'] == 'M']['admitted'].mean()\n",
    "women_overall = df_berkeley[df_berkeley['gender'] == 'F']['admitted'].mean()\n",
    "\n",
    "# By department\n",
    "men_a = df_berkeley[(df_berkeley['gender'] == 'M') & (df_berkeley['dept'] == 'A')]['admitted'].mean()\n",
    "women_a = df_berkeley[(df_berkeley['gender'] == 'F') & (df_berkeley['dept'] == 'A')]['admitted'].mean()\n",
    "men_b = df_berkeley[(df_berkeley['gender'] == 'M') & (df_berkeley['dept'] == 'B')]['admitted'].mean()\n",
    "women_b = df_berkeley[(df_berkeley['gender'] == 'F') & (df_berkeley['dept'] == 'B')]['admitted'].mean()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Overall\n",
    "axes[0].bar(['Men', 'Women'], [men_overall, women_overall], \n",
    "            color=['blue', 'red'], alpha=0.7)\n",
    "axes[0].set_ylabel('Admission Rate')\n",
    "axes[0].set_title('Overall Admission Rates\\nAppears Biased Against Women!')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, (val, label) in enumerate(zip([men_overall, women_overall], ['Men', 'Women'])):\n",
    "    axes[0].text(i, val + 0.02, f'{val:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Department A\n",
    "axes[1].bar(['Men', 'Women'], [men_a, women_a], \n",
    "            color=['blue', 'red'], alpha=0.7)\n",
    "axes[1].set_ylabel('Admission Rate')\n",
    "axes[1].set_title('Department A\\nNo Bias (Equal Rates)')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, (val, label) in enumerate(zip([men_a, women_a], ['Men', 'Women'])):\n",
    "    axes[1].text(i, val + 0.02, f'{val:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Department B\n",
    "axes[2].bar(['Men', 'Women'], [men_b, women_b], \n",
    "            color=['blue', 'red'], alpha=0.7)\n",
    "axes[2].set_ylabel('Admission Rate')\n",
    "axes[2].set_title('Department B\\nNo Bias (Equal Rates)')\n",
    "axes[2].set_ylim([0, 1])\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "for i, (val, label) in enumerate(zip([men_b, women_b], ['Men', 'Women'])):\n",
    "    axes[2].text(i, val + 0.02, f'{val:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Berkeley Admissions Paradox ===\")\n",
    "print(f\"Overall: Men {men_overall:.1%}, Women {women_overall:.1%} → Appears biased!\")\n",
    "print(f\"\\nBut within departments:\")\n",
    "print(f\"Dept A: Men {men_a:.1%}, Women {women_a:.1%} → No bias\")\n",
    "print(f\"Dept B: Men {men_b:.1%}, Women {women_b:.1%} → No bias\")\n",
    "print(f\"\\nExplanation: Men apply more to easy departments (A), women to hard ones (B)\")\n",
    "print(f\"Department is a confounder that must be controlled for!\")\n",
    "\n",
    "# Show causal graph\n",
    "plot_dag([('Gender', 'Department'), ('Department', 'Admission')],\n",
    "         'Berkeley Causal Structure',\n",
    "         node_colors={'Gender': 'lightblue', 'Department': 'lightcoral', 'Admission': 'lightgreen'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Causal Inference Methods\n",
    "\n",
    "### 3.1 The do-Operator and Interventions\n",
    "\n",
    "**Observation** P(Y|X=x): Seeing X=x occur naturally\n",
    "- \"What is Y when we observe X=x?\"\n",
    "- Includes all confounding\n",
    "\n",
    "**Intervention** P(Y|do(X=x)): Forcing X=x by external intervention\n",
    "- \"What is Y when we set X=x?\"\n",
    "- Removes confounding\n",
    "\n",
    "**Example**:\n",
    "- P(recovery|took medicine) includes selection bias (sicker people take medicine)\n",
    "- P(recovery|do(take medicine)) is the true causal effect\n",
    "\n",
    "**Computing do-probabilities**:\n",
    "1. **Randomized Controlled Trial (RCT)**: Force random assignment → gold standard\n",
    "2. **Backdoor adjustment**: Control for confounders\n",
    "3. **Frontdoor adjustment**: Use mediators when confounders unobserved\n",
    "4. **Instrumental variables**: Use natural experiments\n",
    "\n",
    "### 3.2 Backdoor Adjustment\n",
    "\n",
    "**Backdoor criterion**: A set Z satisfies the backdoor criterion if:\n",
    "1. Z blocks all backdoor paths from X to Y\n",
    "2. Z contains no descendants of X\n",
    "\n",
    "**Adjustment formula**:\n",
    "$$P(Y|do(X=x)) = \\sum_z P(Y|X=x, Z=z) P(Z=z)$$\n",
    "\n",
    "**Intuition**: Stratify by confounder Z, compute effect within each stratum, then average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_confounded_treatment_data(n: int = 5000) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simulate treatment effect with confounding.\n",
    "    \n",
    "    Causal structure: Z → X, Z → Y, X → Y\n",
    "    - Z: Confounder (e.g., disease severity)\n",
    "    - X: Treatment\n",
    "    - Y: Outcome\n",
    "    \n",
    "    True causal effect: X causes Y with coefficient 2.0\n",
    "    \"\"\"\n",
    "    # Confounder (e.g., severity)\n",
    "    Z = np.random.randn(n)\n",
    "    \n",
    "    # Treatment (influenced by severity - sicker patients more likely to get treatment)\n",
    "    treatment_prob = 1 / (1 + np.exp(-Z))  # Logistic\n",
    "    X = (np.random.rand(n) < treatment_prob).astype(float)\n",
    "    \n",
    "    # Outcome (influenced by both severity and treatment)\n",
    "    Y = -3 * Z + 2 * X + np.random.randn(n) * 0.5  # True effect of X on Y is +2\n",
    "    \n",
    "    return X, Y, Z\n",
    "\n",
    "X, Y, Z = simulate_confounded_treatment_data(5000)\n",
    "\n",
    "# Naive estimate (confounded)\n",
    "naive_effect = Y[X == 1].mean() - Y[X == 0].mean()\n",
    "\n",
    "# Backdoor adjustment (stratify by Z)\n",
    "# Discretize Z into bins\n",
    "z_bins = pd.cut(Z, bins=10, labels=False)\n",
    "\n",
    "adjusted_effects = []\n",
    "weights = []\n",
    "\n",
    "for bin_idx in range(10):\n",
    "    mask = z_bins == bin_idx\n",
    "    if mask.sum() > 0 and (X[mask] == 1).sum() > 0 and (X[mask] == 0).sum() > 0:\n",
    "        # Effect within this stratum\n",
    "        effect = Y[mask & (X == 1)].mean() - Y[mask & (X == 0)].mean()\n",
    "        adjusted_effects.append(effect)\n",
    "        weights.append(mask.sum())\n",
    "\n",
    "# Weighted average\n",
    "backdoor_effect = np.average(adjusted_effects, weights=weights)\n",
    "\n",
    "print(\"=== Causal Effect Estimation ===\")\n",
    "print(f\"True causal effect (by construction): 2.0\")\n",
    "print(f\"Naive estimate (confounded): {naive_effect:.3f}\")\n",
    "print(f\"Backdoor adjustment estimate: {backdoor_effect:.3f}\")\n",
    "print(f\"\\nBackdoor adjustment recovers the true effect by controlling for Z!\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Confounding visualization\n",
    "axes[0].scatter(Z[X == 0], Y[X == 0], alpha=0.3, label='No treatment', s=10)\n",
    "axes[0].scatter(Z[X == 1], Y[X == 1], alpha=0.3, label='Treatment', s=10)\n",
    "axes[0].set_xlabel('Confounder Z (severity)')\n",
    "axes[0].set_ylabel('Outcome Y')\n",
    "axes[0].set_title('Raw Data (Confounded)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Naive comparison\n",
    "axes[1].bar(['No Treatment', 'Treatment'], \n",
    "            [Y[X == 0].mean(), Y[X == 1].mean()],\n",
    "            color=['blue', 'red'], alpha=0.7)\n",
    "axes[1].set_ylabel('Mean Outcome')\n",
    "axes[1].set_title(f'Naive Comparison\\nEffect = {naive_effect:.2f} (WRONG)')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Adjusted comparison\n",
    "axes[2].bar(['Naive', 'Backdoor Adj', 'True'], \n",
    "            [naive_effect, backdoor_effect, 2.0],\n",
    "            color=['gray', 'green', 'gold'], alpha=0.7)\n",
    "axes[2].axhline(y=2.0, color='gold', linestyle='--', label='True effect')\n",
    "axes[2].set_ylabel('Estimated Treatment Effect')\n",
    "axes[2].set_title('Effect Estimates')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Propensity Score Matching\n",
    "\n",
    "**Problem**: When there are many confounders, stratification becomes infeasible (curse of dimensionality).\n",
    "\n",
    "**Solution**: Propensity score matching\n",
    "- **Propensity score**: e(Z) = P(X=1|Z) (probability of receiving treatment given confounders)\n",
    "- **Key theorem** (Rosenbaum & Rubin, 1983): Conditioning on e(Z) is sufficient for removing confounding\n",
    "\n",
    "**Method**:\n",
    "1. Estimate propensity scores e(Z) using logistic regression\n",
    "2. Match treated units to control units with similar propensity scores\n",
    "3. Estimate effect within matched pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propensity_score_matching(X: np.ndarray, Y: np.ndarray, Z: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Estimate treatment effect using propensity score matching.\n",
    "    \n",
    "    Args:\n",
    "        X: Treatment (0/1)\n",
    "        Y: Outcome\n",
    "        Z: Confounders (can be multidimensional)\n",
    "    \n",
    "    Returns:\n",
    "        Estimated average treatment effect\n",
    "    \"\"\"\n",
    "    # Ensure Z is 2D\n",
    "    if Z.ndim == 1:\n",
    "        Z = Z.reshape(-1, 1)\n",
    "    \n",
    "    # Step 1: Estimate propensity scores\n",
    "    propensity_model = LogisticRegression()\n",
    "    propensity_model.fit(Z, X)\n",
    "    propensity_scores = propensity_model.predict_proba(Z)[:, 1]\n",
    "    \n",
    "    # Step 2: Match each treated unit to nearest control unit\n",
    "    treated_idx = np.where(X == 1)[0]\n",
    "    control_idx = np.where(X == 0)[0]\n",
    "    \n",
    "    matched_effects = []\n",
    "    \n",
    "    for t_idx in treated_idx:\n",
    "        # Find control unit with closest propensity score\n",
    "        ps_t = propensity_scores[t_idx]\n",
    "        ps_controls = propensity_scores[control_idx]\n",
    "        \n",
    "        closest_control = control_idx[np.argmin(np.abs(ps_controls - ps_t))]\n",
    "        \n",
    "        # Compute effect for this matched pair\n",
    "        effect = Y[t_idx] - Y[closest_control]\n",
    "        matched_effects.append(effect)\n",
    "    \n",
    "    # Step 3: Average over matched pairs\n",
    "    return np.mean(matched_effects), propensity_scores\n",
    "\n",
    "# Generate data with multiple confounders\n",
    "n = 3000\n",
    "Z1 = np.random.randn(n)\n",
    "Z2 = np.random.randn(n)\n",
    "Z3 = np.random.randn(n)\n",
    "Z_multi = np.column_stack([Z1, Z2, Z3])\n",
    "\n",
    "# Treatment depends on all confounders\n",
    "treatment_logit = 0.5 * Z1 + 0.3 * Z2 + 0.2 * Z3\n",
    "treatment_prob = 1 / (1 + np.exp(-treatment_logit))\n",
    "X_multi = (np.random.rand(n) < treatment_prob).astype(float)\n",
    "\n",
    "# Outcome depends on all confounders + treatment\n",
    "Y_multi = -2 * Z1 - 1.5 * Z2 - 1 * Z3 + 3 * X_multi + np.random.randn(n) * 0.5\n",
    "\n",
    "# Estimate effects\n",
    "naive_effect_multi = Y_multi[X_multi == 1].mean() - Y_multi[X_multi == 0].mean()\n",
    "psm_effect, propensity_scores = propensity_score_matching(X_multi, Y_multi, Z_multi)\n",
    "\n",
    "print(\"=== Propensity Score Matching (Multiple Confounders) ===\")\n",
    "print(f\"True causal effect: 3.0\")\n",
    "print(f\"Naive estimate: {naive_effect_multi:.3f}\")\n",
    "print(f\"Propensity score matching: {psm_effect:.3f}\")\n",
    "print(f\"\\nPSM successfully adjusts for high-dimensional confounding!\")\n",
    "\n",
    "# Visualize propensity score overlap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Propensity score distributions\n",
    "axes[0].hist(propensity_scores[X_multi == 0], bins=30, alpha=0.5, \n",
    "             label='Control', density=True, color='blue')\n",
    "axes[0].hist(propensity_scores[X_multi == 1], bins=30, alpha=0.5, \n",
    "             label='Treated', density=True, color='red')\n",
    "axes[0].set_xlabel('Propensity Score')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Propensity Score Distributions')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Effect estimates\n",
    "axes[1].bar(['Naive', 'PSM', 'True'], \n",
    "            [naive_effect_multi, psm_effect, 3.0],\n",
    "            color=['gray', 'green', 'gold'], alpha=0.7)\n",
    "axes[1].axhline(y=3.0, color='gold', linestyle='--', label='True effect')\n",
    "axes[1].set_ylabel('Estimated Treatment Effect')\n",
    "axes[1].set_title('Effect Estimates with Multiple Confounders')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Instrumental Variables\n",
    "\n",
    "**Problem**: Unmeasured confounding (confounder U is unobserved)\n",
    "- Structure: U → X, U → Y, X → Y\n",
    "- Cannot use backdoor adjustment (U is unmeasured)\n",
    "\n",
    "**Solution**: Instrumental variable (IV)\n",
    "- Z is an instrument if:\n",
    "  1. Z affects X (relevance)\n",
    "  2. Z affects Y only through X (exclusion restriction)\n",
    "  3. Z is independent of U (no confounding)\n",
    "\n",
    "**Examples**:\n",
    "- Effect of education on earnings: Use geographic proximity to college as IV\n",
    "- Effect of military service on earnings: Use draft lottery number as IV\n",
    "\n",
    "**Estimation**: Two-stage least squares (2SLS)\n",
    "1. Stage 1: Regress X on Z to get predicted X̂\n",
    "2. Stage 2: Regress Y on X̂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_stage_least_squares(Z: np.ndarray, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Estimate causal effect using instrumental variables (2SLS).\n",
    "    \n",
    "    Args:\n",
    "        Z: Instrument\n",
    "        X: Treatment (potentially confounded)\n",
    "        Y: Outcome\n",
    "    \n",
    "    Returns:\n",
    "        Estimated causal effect of X on Y\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Stage 1: Regress X on Z\n",
    "    stage1 = LinearRegression()\n",
    "    stage1.fit(Z.reshape(-1, 1), X)\n",
    "    X_predicted = stage1.predict(Z.reshape(-1, 1))\n",
    "    \n",
    "    # Stage 2: Regress Y on predicted X\n",
    "    stage2 = LinearRegression()\n",
    "    stage2.fit(X_predicted.reshape(-1, 1), Y)\n",
    "    \n",
    "    return stage2.coef_[0]\n",
    "\n",
    "# Simulate IV data\n",
    "n = 5000\n",
    "\n",
    "# Unmeasured confounder\n",
    "U = np.random.randn(n)\n",
    "\n",
    "# Instrument (e.g., randomized encouragement)\n",
    "Z_iv = np.random.randn(n)\n",
    "\n",
    "# Treatment (affected by both instrument and confounder)\n",
    "X_iv = 2 * Z_iv + 3 * U + np.random.randn(n)\n",
    "\n",
    "# Outcome (affected by treatment and confounder)\n",
    "Y_iv = 1.5 * X_iv + 4 * U + np.random.randn(n)\n",
    "# True effect of X on Y is 1.5\n",
    "\n",
    "# Naive OLS (confounded)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_iv.reshape(-1, 1), Y_iv)\n",
    "naive_iv_effect = ols.coef_[0]\n",
    "\n",
    "# 2SLS with instrument\n",
    "iv_effect = two_stage_least_squares(Z_iv, X_iv, Y_iv)\n",
    "\n",
    "print(\"=== Instrumental Variables (Unmeasured Confounding) ===\")\n",
    "print(f\"True causal effect: 1.5\")\n",
    "print(f\"Naive OLS (confounded): {naive_iv_effect:.3f}\")\n",
    "print(f\"2SLS with instrument: {iv_effect:.3f}\")\n",
    "print(f\"\\nIV successfully recovers causal effect even with unmeasured confounding!\")\n",
    "\n",
    "# Visualize causal structure\n",
    "plot_dag([('U', 'X'), ('U', 'Y'), ('X', 'Y'), ('Z', 'X')],\n",
    "         'Instrumental Variable Structure\\n(U is unmeasured)',\n",
    "         node_colors={'Z': 'lightgreen', 'X': 'lightblue', 'Y': 'lightblue', 'U': 'lightcoral'})\n",
    "\n",
    "# Show effect estimates\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['Naive OLS', '2SLS (IV)', 'True'], \n",
    "        [naive_iv_effect, iv_effect, 1.5],\n",
    "        color=['gray', 'green', 'gold'], alpha=0.7)\n",
    "plt.axhline(y=1.5, color='gold', linestyle='--', label='True effect')\n",
    "plt.ylabel('Estimated Treatment Effect')\n",
    "plt.title('IV Estimation (Unmeasured Confounding)')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Neural Networks for Causal Inference\n",
    "\n",
    "### 4.1 Treatment Effect Estimation with Neural Networks\n",
    "\n",
    "**Challenge**: Traditional methods (regression, matching) assume linear effects.\n",
    "- Real treatment effects are often heterogeneous (vary by individual)\n",
    "- Confounding relationships may be non-linear\n",
    "\n",
    "**Solution**: Use neural networks!\n",
    "- Can model complex, non-linear relationships\n",
    "- Can estimate heterogeneous treatment effects\n",
    "\n",
    "**Architecture**: TARNet (Treatment-Agnostic Representation Network)\n",
    "- Shared layers: Learn representation from covariates\n",
    "- Separate heads: One for treated, one for control outcomes\n",
    "- Training: Predict outcome for observed treatment only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TARNet(nn.Module):\n",
    "    \"\"\"Treatment-Agnostic Representation Network for treatment effect estimation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared representation network\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Separate outcome heads\n",
    "        self.head_control = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.head_treated = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Covariates [batch, input_dim]\n",
    "            t: Treatment indicator [batch, 1]\n",
    "        \n",
    "        Returns:\n",
    "            Predicted outcome [batch, 1]\n",
    "        \"\"\"\n",
    "        # Shared representation\n",
    "        repr = self.shared(x)\n",
    "        \n",
    "        # Predict outcomes for both treatment conditions\n",
    "        y0 = self.head_control(repr)\n",
    "        y1 = self.head_treated(repr)\n",
    "        \n",
    "        # Return outcome for observed treatment\n",
    "        y = t * y1 + (1 - t) * y0\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def predict_ite(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict individual treatment effect: E[Y|X,T=1] - E[Y|X,T=0]\"\"\"\n",
    "        with torch.no_grad():\n",
    "            repr = self.shared(x)\n",
    "            y0 = self.head_control(repr)\n",
    "            y1 = self.head_treated(repr)\n",
    "            return y1 - y0\n",
    "\n",
    "# Generate complex non-linear treatment effect data\n",
    "def generate_nonlinear_treatment_data(n: int = 5000):\n",
    "    # Covariates\n",
    "    X1 = np.random.randn(n)\n",
    "    X2 = np.random.randn(n)\n",
    "    X3 = np.random.randn(n)\n",
    "    X = np.column_stack([X1, X2, X3])\n",
    "    \n",
    "    # Treatment (depends on covariates)\n",
    "    propensity_logit = 0.5 * X1 + 0.3 * np.sin(X2) + 0.2 * X3**2\n",
    "    propensity = 1 / (1 + np.exp(-propensity_logit))\n",
    "    T = (np.random.rand(n) < propensity).astype(float)\n",
    "    \n",
    "    # Heterogeneous treatment effect (depends on X1)\n",
    "    treatment_effect = 2 + X1  # Effect is larger for higher X1\n",
    "    \n",
    "    # Outcome (non-linear in covariates)\n",
    "    Y_control = 3 * np.sin(X1) + 2 * X2**2 + X3 + np.random.randn(n) * 0.5\n",
    "    Y_treated = Y_control + treatment_effect + np.random.randn(n) * 0.5\n",
    "    \n",
    "    # Observe outcome only for assigned treatment\n",
    "    Y = T * Y_treated + (1 - T) * Y_control\n",
    "    \n",
    "    return X, T.reshape(-1, 1), Y.reshape(-1, 1), treatment_effect\n",
    "\n",
    "X_nl, T_nl, Y_nl, true_ite = generate_nonlinear_treatment_data(5000)\n",
    "\n",
    "# Convert to tensors\n",
    "X_nl_t = torch.FloatTensor(X_nl).to(device)\n",
    "T_nl_t = torch.FloatTensor(T_nl).to(device)\n",
    "Y_nl_t = torch.FloatTensor(Y_nl).to(device)\n",
    "\n",
    "# Train TARNet\n",
    "model = TARNet(input_dim=3, hidden_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "dataset = TensorDataset(X_nl_t, T_nl_t, Y_nl_t)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(\"Training TARNet...\")\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for X_batch, T_batch, Y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model(X_batch, T_batch)\n",
    "        loss = F.mse_loss(Y_pred, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(loader):.4f}\")\n",
    "\n",
    "# Predict individual treatment effects\n",
    "model.eval()\n",
    "predicted_ite = model.predict_ite(X_nl_t).cpu().numpy().flatten()\n",
    "\n",
    "# Compare to naive estimate\n",
    "naive_ate = Y_nl[T_nl == 1].mean() - Y_nl[T_nl == 0].mean()\n",
    "tarnet_ate = predicted_ite.mean()\n",
    "true_ate = true_ite.mean()\n",
    "\n",
    "print(f\"\\n=== Average Treatment Effect Estimation ===\")\n",
    "print(f\"True ATE: {true_ate:.3f}\")\n",
    "print(f\"Naive estimate: {naive_ate:.3f}\")\n",
    "print(f\"TARNet estimate: {tarnet_ate:.3f}\")\n",
    "\n",
    "# Visualize heterogeneous effects\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# True vs predicted ITE\n",
    "axes[0].scatter(true_ite, predicted_ite, alpha=0.3, s=10)\n",
    "axes[0].plot([true_ite.min(), true_ite.max()], [true_ite.min(), true_ite.max()], \n",
    "             'r--', label='Perfect prediction')\n",
    "axes[0].set_xlabel('True Individual Treatment Effect')\n",
    "axes[0].set_ylabel('Predicted Individual Treatment Effect')\n",
    "axes[0].set_title(f'ITE Prediction (R² = {np.corrcoef(true_ite, predicted_ite)[0,1]**2:.3f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Heterogeneity by X1\n",
    "sort_idx = np.argsort(X_nl[:, 0])\n",
    "axes[1].plot(X_nl[sort_idx, 0], true_ite[sort_idx], label='True effect', linewidth=2, alpha=0.7)\n",
    "axes[1].plot(X_nl[sort_idx, 0], predicted_ite[sort_idx], label='Predicted effect', \n",
    "             linewidth=2, alpha=0.7)\n",
    "axes[1].axhline(y=naive_ate, color='gray', linestyle='--', label=f'Naive ATE = {naive_ate:.2f}')\n",
    "axes[1].set_xlabel('Covariate X₁')\n",
    "axes[1].set_ylabel('Treatment Effect')\n",
    "axes[1].set_title('Heterogeneous Treatment Effects')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTARNet successfully captures heterogeneous treatment effects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Causal Regularization: Learning Invariant Representations\n",
    "\n",
    "**Idea**: If we learn representations that are invariant across different environments/domains, they're more likely to capture causal (not spurious) features.\n",
    "\n",
    "**Invariant Risk Minimization (IRM)**:\n",
    "- Learn a representation Φ(X) that works well across all environments\n",
    "- Penalize representations whose optimal predictor varies across environments\n",
    "\n",
    "**Why it helps**:\n",
    "- Spurious correlations vary across environments\n",
    "- Causal relationships are stable\n",
    "- Forces model to learn causal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantPredictor(nn.Module):\n",
    "    \"\"\"Neural network with causal regularization (simplified IRM).\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extractor (representation)\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.phi(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "def train_with_irm(model: nn.Module, loaders: List[DataLoader], \n",
    "                   epochs: int = 50, irm_lambda: float = 1000.0):\n",
    "    \"\"\"\n",
    "    Train with Invariant Risk Minimization penalty.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network\n",
    "        loaders: List of DataLoaders (one per environment)\n",
    "        epochs: Number of training epochs\n",
    "        irm_lambda: Weight of IRM penalty\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Iterate over environments\n",
    "        for env_idx, loader in enumerate(loaders):\n",
    "            for X_batch, Y_batch in loader:\n",
    "                X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Standard prediction loss\n",
    "                pred = model(X_batch)\n",
    "                loss_pred = F.binary_cross_entropy_with_logits(pred, Y_batch)\n",
    "                \n",
    "                # IRM penalty: variance of gradients across environments\n",
    "                # (Simplified version)\n",
    "                loss = loss_pred\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Generate data with spurious correlation\n",
    "def generate_spurious_data(n: int, spurious_corr: float = 0.9):\n",
    "    \"\"\"\n",
    "    Generate data where:\n",
    "    - Y depends causally on X_causal\n",
    "    - Y is spuriously correlated with X_spurious (correlation varies by environment)\n",
    "    \"\"\"\n",
    "    # Causal feature (stable across environments)\n",
    "    X_causal = np.random.randn(n, 1)\n",
    "    \n",
    "    # Label (depends only on causal feature)\n",
    "    Y = (X_causal[:, 0] > 0).astype(float)\n",
    "    \n",
    "    # Spurious feature (correlated with Y, but not causal)\n",
    "    X_spurious = np.random.randn(n, 1)\n",
    "    # Create spurious correlation\n",
    "    flip_mask = np.random.rand(n) > spurious_corr\n",
    "    X_spurious[Y == 0] *= -1\n",
    "    X_spurious[flip_mask] *= -1\n",
    "    \n",
    "    X = np.hstack([X_causal, X_spurious])\n",
    "    \n",
    "    return X, Y.reshape(-1, 1)\n",
    "\n",
    "# Create multiple environments with different spurious correlations\n",
    "print(\"Generating data from multiple environments...\")\n",
    "X_env1, Y_env1 = generate_spurious_data(1000, spurious_corr=0.9)\n",
    "X_env2, Y_env2 = generate_spurious_data(1000, spurious_corr=0.8)\n",
    "X_env3, Y_env3 = generate_spurious_data(1000, spurious_corr=0.7)\n",
    "X_test, Y_test = generate_spurious_data(500, spurious_corr=0.5)  # Test env with lower correlation\n",
    "\n",
    "# Create dataloaders\n",
    "train_loaders = [\n",
    "    DataLoader(TensorDataset(torch.FloatTensor(X_env1), torch.FloatTensor(Y_env1)), \n",
    "               batch_size=64, shuffle=True),\n",
    "    DataLoader(TensorDataset(torch.FloatTensor(X_env2), torch.FloatTensor(Y_env2)), \n",
    "               batch_size=64, shuffle=True),\n",
    "    DataLoader(TensorDataset(torch.FloatTensor(X_env3), torch.FloatTensor(Y_env3)), \n",
    "               batch_size=64, shuffle=True),\n",
    "]\n",
    "\n",
    "test_loader = DataLoader(TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(Y_test)),\n",
    "                         batch_size=64)\n",
    "\n",
    "# Train standard ERM model\n",
    "print(\"\\nTraining standard model (ERM)...\")\n",
    "model_erm = InvariantPredictor(input_dim=2, hidden_dim=32).to(device)\n",
    "train_with_irm(model_erm, train_loaders, epochs=30, irm_lambda=0.0)\n",
    "\n",
    "# Evaluate\n",
    "model_erm.eval()\n",
    "correct_erm = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "        pred = model_erm(X_batch)\n",
    "        pred_labels = (torch.sigmoid(pred) > 0.5).float()\n",
    "        correct_erm += (pred_labels == Y_batch).sum().item()\n",
    "        total += Y_batch.size(0)\n",
    "\n",
    "acc_erm = correct_erm / total\n",
    "\n",
    "# Analyze feature importance\n",
    "X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "with torch.no_grad():\n",
    "    features_erm = model_erm.phi(X_test_t).cpu().numpy()\n",
    "    weights_erm = model_erm.classifier.weight.data.cpu().numpy()\n",
    "\n",
    "print(f\"\\n=== Results ===\")\n",
    "print(f\"ERM model test accuracy: {acc_erm:.1%}\")\n",
    "print(f\"\\nFeature importance (average absolute gradient):\")\n",
    "X_test_t.requires_grad = True\n",
    "pred = model_erm(X_test_t)\n",
    "pred.sum().backward()\n",
    "grad_importance = X_test_t.grad.abs().mean(dim=0).cpu().numpy()\n",
    "print(f\"  Causal feature (X₁): {grad_importance[0]:.4f}\")\n",
    "print(f\"  Spurious feature (X₂): {grad_importance[1]:.4f}\")\n",
    "\n",
    "if grad_importance[1] > grad_importance[0]:\n",
    "    print(\"\\n⚠️ Model relies more on spurious feature! This hurts generalization.\")\n",
    "else:\n",
    "    print(\"\\n✓ Model learned to use causal feature!\")\n",
    "\n",
    "# Visualize decision boundary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Test data\n",
    "axes[0].scatter(X_test[Y_test[:, 0] == 0, 0], X_test[Y_test[:, 0] == 0, 1], \n",
    "                alpha=0.5, label='Class 0', s=20)\n",
    "axes[0].scatter(X_test[Y_test[:, 0] == 1, 0], X_test[Y_test[:, 0] == 1, 1], \n",
    "                alpha=0.5, label='Class 1', s=20)\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='True boundary (X₁=0)')\n",
    "axes[0].set_xlabel('X₁ (Causal Feature)')\n",
    "axes[0].set_ylabel('X₂ (Spurious Feature)')\n",
    "axes[0].set_title('Test Environment Data')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance\n",
    "axes[1].bar(['Causal (X₁)', 'Spurious (X₂)'], grad_importance, \n",
    "            color=['green', 'red'], alpha=0.7)\n",
    "axes[1].set_ylabel('Average Absolute Gradient')\n",
    "axes[1].set_title('Feature Importance')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Causal regularization (like IRM) helps models learn robust, causal features\")\n",
    "print(\"that generalize better to new environments with different spurious correlations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Key Concepts\n",
    "\n",
    "### 1. Correlation vs Causation\n",
    "- ✓ Correlation can arise from: X→Y, Y→X, Z→X,Y, or coincidence\n",
    "- ✓ Simpson's paradox shows confounding can reverse conclusions\n",
    "- ✓ Must understand causal structure to avoid misleading conclusions\n",
    "\n",
    "### 2. Causal Graphs (DAGs)\n",
    "- ✓ DAGs encode causal assumptions\n",
    "- ✓ Three key structures: chains, forks, colliders\n",
    "- ✓ d-separation determines conditional independence\n",
    "- ⚠ **Collider bias**: Conditioning on colliders creates spurious associations!\n",
    "\n",
    "### 3. Causal Inference Methods\n",
    "- ✓ **do-operator**: Distinguishes observation from intervention\n",
    "- ✓ **Backdoor adjustment**: Control for confounders\n",
    "- ✓ **Propensity score matching**: Handles high-dimensional confounding\n",
    "- ✓ **Instrumental variables**: Handle unmeasured confounding\n",
    "\n",
    "### 4. Neural Networks for Causality\n",
    "- ✓ **TARNet**: Estimate heterogeneous treatment effects\n",
    "- ✓ **Causal regularization**: Learn invariant representations\n",
    "- ✓ Deep learning enables flexible modeling of complex causal relationships\n",
    "\n",
    "### Practical Takeaways\n",
    "\n",
    "**Before training any model, ask**:\n",
    "1. What is the causal question I'm trying to answer?\n",
    "2. What is the causal structure of my problem (draw a DAG)?\n",
    "3. What are the potential confounders?\n",
    "4. Am I conditioning on colliders (creating spurious associations)?\n",
    "5. Do I have the right data to answer my causal question?\n",
    "\n",
    "**When evaluating models**:\n",
    "- High predictive accuracy ≠ correct causal effect\n",
    "- Test robustness across different environments/distributions\n",
    "- Be skeptical of correlations without causal mechanisms\n",
    "\n",
    "**When deploying models**:\n",
    "- Models trained on observational data encode spurious correlations\n",
    "- Interventions (changing X) have different effects than observations (seeing X)\n",
    "- Consider out-of-distribution generalization\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "**Foundational texts**:\n",
    "- Pearl, J. (2009). *Causality: Models, Reasoning, and Inference*\n",
    "- Pearl, J., Glymour, M., & Jewell, N. (2016). *Causal Inference in Statistics: A Primer*\n",
    "- Hernán, M. & Robins, J. (2020). *Causal Inference: What If*\n",
    "\n",
    "**Neural networks for causality**:\n",
    "- Shalit et al. (2017). \"Estimating individual treatment effect: generalization bounds and algorithms\"\n",
    "- Arjovsky et al. (2019). \"Invariant Risk Minimization\"\n",
    "- Yao et al. (2021). \"A Survey on Causal Inference\"\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "Causality is fundamental to:\n",
    "- **Science**: Understanding mechanisms\n",
    "- **Decision-making**: Predicting intervention effects\n",
    "- **Fairness**: Identifying and removing discrimination\n",
    "- **Robustness**: Building models that generalize\n",
    "\n",
    "Machine learning is moving beyond prediction toward causal reasoning. Understanding causality makes you a better ML practitioner and helps you build systems that are more robust, fair, and trustworthy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
